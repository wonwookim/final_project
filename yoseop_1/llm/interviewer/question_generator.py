#!/usr/bin/env python3
"""
ìˆœìˆ˜ ì§ˆë¬¸ ìƒì„±ê¸° - InterviewerServiceì—ì„œ ë¶„ë¦¬
í„´ì œ ê´€ë¦¬ ë¡œì§ ì—†ì´ ì˜¤ì§ ì§ˆë¬¸ ìƒì„±ë§Œ ë‹´ë‹¹
"""

import os
import sys
import json
import random
import time
import openai
import logging
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from backend.services.supabase_client import get_supabase_client
from llm.shared.constants import GPT_MODEL, MAX_TOKENS, TEMPERATURE
from llm.candidate.model import CandidatePersona
from .prompt import InterviewerPromptBuilder


class QuestionGenerator:
    """ìˆœìˆ˜ ì§ˆë¬¸ ìƒì„±ê¸° - í„´ì œ ê´€ë¦¬ ì—†ì´ ì§ˆë¬¸ ìƒì„±ë§Œ ë‹´ë‹¹"""
    
    def __init__(self):
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = get_supabase_client()
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.openai_client = openai.OpenAI(api_key=api_key)
        
        # í”„ë¡¬í”„íŠ¸ ë¹Œë” ì´ˆê¸°í™”
        self.prompt_builder = InterviewerPromptBuilder()
        
        # DB ë°ì´í„° ë¡œë”© ë° ìºì‹±
        self.companies_data = self._load_companies_data()
        self.fixed_questions = self._load_fixed_questions()
        
        # DB ì§ˆë¬¸ íƒ€ì… ID ë§¤í•‘ (ë©´ì ‘ê´€ ì—­í•  â†’ DB ID)
        self.interviewer_role_to_db_id = {
            'HR': 1,
            'TECH': 2, 
            'COLLABORATION': 3
        }
        
        # ë©´ì ‘ê´€ë³„ ì£¼ì œ í’€ ì •ì˜
        self.topic_pools = {
            'HR': ['ì¸ì„±_ê°€ì¹˜ê´€', 'ì„±ì¥_ë™ê¸°', 'ê°ˆë“±_í•´ê²°', 'ìŠ¤íŠ¸ë ˆìŠ¤_ê´€ë¦¬', 'íŒ€ì›Œí¬_ë¦¬ë”ì‹­'],
            'TECH': ['ê¸°ìˆ _ì—­ëŸ‰', 'ë¬¸ì œ_í•´ê²°', 'ì„±ëŠ¥_ìµœì í™”', 'ì½”ë“œ_í’ˆì§ˆ', 'ìƒˆë¡œìš´_ê¸°ìˆ _í•™ìŠµ'],
            'COLLABORATION': ['ì†Œí†µ_ëŠ¥ë ¥', 'í”„ë¡œì íŠ¸_í˜‘ì—…', 'ì˜ê²¬_ì¡°ìœ¨', 'í¬ë¡œìŠ¤_íŒ€_í˜‘ì—…', 'ì¡°ì§_ë¬¸í™”_ì ì‘']
        }
    
    def _load_companies_data(self) -> Dict[str, Any]:
        """íšŒì‚¬ ë°ì´í„° ë¡œë”© ë° ìºì‹±"""
        try:
            result = self.client.table('company').select('*').execute()
            companies_dict = {}
            
            # CompanyDataLoaderì™€ ë™ì¼í•œ ë§¤í•‘ í…Œì´ë¸” ì‚¬ìš©
            company_id_mapping = {
                'ë„¤ì´ë²„': 'naver',
                'ì¹´ì¹´ì˜¤': 'kakao', 
                'ë¼ì¸': 'line',
                'ë¼ì¸í”ŒëŸ¬ìŠ¤': 'ë¼ì¸í”ŒëŸ¬ìŠ¤',
                'ì¿ íŒ¡': 'coupang',
                'ë°°ë‹¬ì˜ë¯¼ì¡±': 'baemin',
                'ë‹¹ê·¼ë§ˆì¼“': 'daangn',
                'í† ìŠ¤': 'toss'
            }
            
            if result.data:
                for company in result.data:
                    company_name = company.get('name', '')
                    english_id = company_id_mapping.get(company_name, company_name.lower())
                    companies_dict[english_id] = company
                    # í•œê¸€ëª…ìœ¼ë¡œë„ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ì¶”ê°€
                    companies_dict[company_name] = company
                    
            print(f"[SUCCESS] íšŒì‚¬ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(companies_dict)}ê°œ")
            print(f"[DEBUG] ë¡œë”©ëœ íšŒì‚¬ í‚¤ë“¤: {list(companies_dict.keys())}")
            return companies_dict
        except Exception as e:
            print(f"[ERROR] íšŒì‚¬ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def _load_fixed_questions(self) -> List[Dict[str, Any]]:
        """ê³ ì • ì§ˆë¬¸ ë°ì´í„° ë¡œë”© ë° ìºì‹±"""
        try:
            result = self.client.table('fix_question').select('*').execute()
            questions = result.data if result.data else []
            print(f"[SUCCESS] ê³ ì • ì§ˆë¬¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(questions)}ê°œ")
            return questions
        except Exception as e:
            print(f"[ERROR] ê³ ì • ì§ˆë¬¸ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return []
    
    def generate_fixed_question(self, question_index: int, company_id: str, user_resume: Dict = None) -> Dict:
        """ê³ ì • ì§ˆë¬¸ ìƒì„± (ìê¸°ì†Œê°œ, ì§€ì›ë™ê¸°)"""
        if question_index == 0:
            # ì²« ë²ˆì§¸ ì§ˆë¬¸: ìê¸°ì†Œê°œ
            candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
            base_question = 'ê°„ë‹¨í•˜ê²Œ ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.'
            question_with_name = self._add_candidate_name_to_question(base_question, candidate_name)
            return {
                'question': question_with_name,
                'intent': 'ì§€ì›ìì˜ ê¸°ë³¸ ì •ë³´ì™€ ì„±ê²©, ì—­ëŸ‰ì„ íŒŒì•…',
                'interviewer_type': 'INTRO'
            }
        
        elif question_index == 1:
            # ë‘ ë²ˆì§¸ ì§ˆë¬¸: ì§€ì›ë™ê¸°
            company_info = self.companies_data.get(company_id, {})
            company_name = company_info.get('name', 'ì €í¬ íšŒì‚¬')
            
            base_question = f'ì €í¬ {company_name}ì— ì§€ì›í•˜ì‹  ë™ê¸°ë¥¼ ë§ì”€í•´ ì£¼ì„¸ìš”.'
            candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
            question_with_name = self._add_candidate_name_to_question(base_question, candidate_name)
            
            return {
                'question': question_with_name,
                'intent': 'íšŒì‚¬ì— ëŒ€í•œ ê´€ì‹¬ë„ì™€ ì§€ì› ë™ê¸° íŒŒì•…',
                'interviewer_type': 'INTRO'
            }
        
        else:
            raise ValueError(f"ê³ ì • ì§ˆë¬¸ì€ 0, 1ë²ˆë§Œ ì§€ì›ë©ë‹ˆë‹¤. ì…ë ¥: {question_index}")
    
    def generate_intro_message(self, company_id: str, user_resume: Dict = None) -> Dict:
        """ì¸íŠ¸ë¡œ ë©”ì‹œì§€ ìƒì„± (í„´ 0ìš©)"""
        company_info = self.companies_data.get(company_id, {})
        company_name = company_info.get('name', 'ì €í¬ íšŒì‚¬')
        user_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
        user_position = user_resume.get('position', 'ê°œë°œì') if user_resume else 'ê°œë°œì'
        
        intro_message = f"""
ì•ˆë…•í•˜ì„¸ìš”.
{company_name}ì— ì§€ì›í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.
{user_name}ë‹˜ ê·¸ë¦¬ê³  ì¶˜ì‹ì´ë‹˜ {user_position} ì „í˜• ë©´ì ‘ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.
"""
        
        return {
            'question': intro_message,
            'intent': 'ë©´ì ‘ ì‹œì‘ ì¸ì‚¬ ë° ë©´ì ‘ê´€ ì†Œê°œ',
            'interviewer_type': 'INTRO'
        }
    
    def generate_question_by_role(self, interviewer_role: str, company_id: str, 
                                 user_resume: Dict, user_answer: str = None, 
                                 chun_sik_answer: str = None, previous_qa_pairs: List[Dict] = None) -> Dict:
        """ë©´ì ‘ê´€ ì—­í• ë³„ ê°œë³„ ì§ˆë¬¸ ìƒì„± (ì‚¬ìš©ì/AI ë™ì‹œ ìƒì„±)"""
        print(f"[DEBUG] ê°œë³„ ì§ˆë¬¸ ìƒì„± ìš”ì²­: company_id='{company_id}', role='{interviewer_role}'")
        print(f"[DEBUG] ì‚¬ìš© ê°€ëŠ¥í•œ íšŒì‚¬ í‚¤ë“¤: {list(self.companies_data.keys())}")
        
        company_info = self.companies_data.get(company_id, {})
        if not company_info:
            print(f"[WARNING] íšŒì‚¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {company_id}, ê¸°ë³¸ íšŒì‚¬ ì •ë³´ë¡œ ëŒ€ì²´")
            # ì—ëŸ¬ ëŒ€ì‹  ê¸°ë³¸ íšŒì‚¬ ì •ë³´ ìƒì„±
            company_info = {
                "name": company_id.capitalize(),
                "id": company_id,
                "core_competencies": [],
                "tech_focus": [],
                "talent_profile": "í˜ì‹ ì ì¸ ê¸°ìˆ  íšŒì‚¬"
            }
        
        # ì£¼ì œ ì„ íƒ
        topic_pool = self.topic_pools.get(interviewer_role, [])
        if not topic_pool:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë©´ì ‘ê´€ ì—­í• : {interviewer_role}")
        
        selected_topic = random.choice(topic_pool)
        
        # ì‚¬ìš©ì ì§ˆë¬¸ ìƒì„±
        user_main_question = self._try_generate_main_question_for_user(
            user_resume, company_info, interviewer_role, selected_topic
        )
        
        # AI ì§ˆë¬¸ ìƒì„±
        ai_main_question = self._try_generate_main_question_for_ai(
            user_resume, company_info, interviewer_role, selected_topic
        )
        
        print(f"[DEBUG] ê°œë³„ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ - ì£¼ì œ: {selected_topic}")
        print(f"[DEBUG] ì‚¬ìš©ì ì§ˆë¬¸: {user_main_question.get('question', 'N/A')[:50]}...")
        print(f"[DEBUG] AI ì§ˆë¬¸: {ai_main_question.get('question', 'N/A')[:50]}...")
        
        return {
            'user_question': user_main_question,
            'ai_question': ai_main_question,
            'interviewer_type': interviewer_role,
            'question_type': 'main',
            'is_individual_questions': True
        }
    
    def generate_question_with_orchestrator_state(self, state: Dict[str, Any]) -> Dict:
        """
        Orchestratorì˜ state ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ì•„ì„œ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ
        
        Args:
            state: Orchestratorì˜ state ê°ì²´
        """
        try:
            # Orchestratorì˜ stateì—ì„œ ì§ì ‘ ì •ë³´ ì¶”ì¶œ
            turn_count = state.get('turn_count', 0)
            current_interviewer = state.get('current_interviewer')
            turn_state = state.get('interviewer_turn_state', {})
            
            # í„´ 0: ì¸íŠ¸ë¡œ ë©”ì‹œì§€ ìƒì„±
            if turn_count == 0:
                company_id = state.get('company_id')
                user_resume = {
                    'name': state.get('user_name', 'ì§€ì›ì'),
                    'position': state.get('position', 'ê°œë°œì')
                }
                return self.generate_intro_message(company_id, user_resume)
            
            # í„´ 1: ìê¸°ì†Œê°œ (fixed)
            elif turn_count == 1:
                question_index = 0
                question = self.generate_fixed_question(question_index, state.get('company_id'), 
                                                      {"name": state.get('user_name', 'ì§€ì›ì')})
                return question
            
            # í„´ 2: ì§€ì›ë™ê¸° (fixed)
            elif turn_count == 2:
                question_index = 1
                question = self.generate_fixed_question(question_index, state.get('company_id'), 
                                                      {"name": state.get('user_name', 'ì§€ì›ì')})
                return question
            
            # í„´ 3ë¶€í„°: ë©´ì ‘ê´€ë³„ ì§ˆë¬¸ (ë©”ì¸ ì§ˆë¬¸ + ê¼¬ë¦¬ ì§ˆë¬¸)
            else:
                # ğŸ†• ìƒíƒœ ê¸°ë°˜ ë©´ì ‘ê´€ ê²°ì • ë¡œì§
                if not current_interviewer:
                    # ì²« ë²ˆì§¸ ë©´ì ‘ê´€ì€ HRë¶€í„° ì‹œì‘
                    current_interviewer = 'HR'
                
                # ğŸ†• ê²°ì •í•œ ë©´ì ‘ê´€ì„ stateì— ì„¤ì •
                state['current_interviewer'] = current_interviewer
                
                # ğŸ†• ë©´ì ‘ê´€ ìƒíƒœ ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
                if current_interviewer not in turn_state:
                    turn_state[current_interviewer] = {
                        'main_question_asked': False,
                        'follow_up_count': 0
                    }
                
                current_turn_state = turn_state.get(current_interviewer, {})
                
                # ê¸°ë³¸ user_resume êµ¬ì„±
                user_resume = {
                    'name': state.get('user_name', 'ì§€ì›ì'),
                    'position': state.get('position', 'ê°œë°œì')
                }
                
                # ë©”ì¸ ì§ˆë¬¸ ì•ˆí–ˆìœ¼ë©´ ë©”ì¸ ì§ˆë¬¸ ìƒì„±
                if not current_turn_state.get('main_question_asked', False):
                    question = self.generate_question_by_role(
                        interviewer_role=current_interviewer,
                        company_id=state.get('company_id'),
                        user_resume=user_resume,
                        previous_qa_pairs=state.get('qa_history', [])
                    )
                    return question
                
                # ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± (ìµœëŒ€ 2ê°œ)
                elif current_turn_state.get('follow_up_count', 0) < 2:
                    # ğŸ†• qa_historyì—ì„œ ìµœì‹  ë°ì´í„° ì¶”ì¶œ
                    qa_history = state.get('qa_history', [])
                    if len(qa_history) >= 2:
                        # ê°€ì¥ ìµœê·¼ ì§ˆë¬¸ê³¼ ë‹µë³€ë“¤ ì¶”ì¶œ
                        latest_qa_pairs = qa_history[-2:]  # ë§ˆì§€ë§‰ 2ê°œ (ì‚¬ìš©ì + AI ë‹µë³€)
                        previous_question = latest_qa_pairs[0]['question'] if latest_qa_pairs else ''
                        
                        # ì‚¬ìš©ìì™€ AI ë‹µë³€ ë¶„ë¦¬
                        user_answer = ""
                        ai_answer = ""
                        for qa in latest_qa_pairs:
                            if qa['answerer'] == 'user':
                                user_answer = qa['answer']
                            elif qa['answerer'] == 'ai':
                                ai_answer = qa['answer']
                    else:
                        previous_question = ""
                        user_answer = ""
                        ai_answer = ""
                    
                    company_info = self.companies_data.get(state.get('company_id'), {})
                    
                    # ğŸ†• ê°œë³„ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±ìœ¼ë¡œ ë³€ê²½
                    if user_answer and ai_answer:
                        print(f"[DEBUG] ê°œë³„ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± í˜¸ì¶œ - {current_interviewer}")
                        individual_questions = self.generate_follow_up_questions_for_both(
                            previous_question=previous_question,
                            user_answer=user_answer,
                            ai_answer=ai_answer,
                            company_info=company_info,
                            interviewer_role=current_interviewer,
                            user_resume=user_resume
                        )
                        return individual_questions
                    else:
                        # í´ë°±: ê¸°ì¡´ ë‹¨ì¼ ì§ˆë¬¸ ë°©ì‹
                        question = self.generate_follow_up_question(
                            previous_question=previous_question,
                            user_answer=user_answer,
                            chun_sik_answer=ai_answer,
                            company_info=company_info,
                            interviewer_role=current_interviewer,
                            user_resume=user_resume
                        )
                        return {
                            'user_question': question,
                            'ai_question': question,
                            'interviewer_type': current_interviewer,
                            'question_type': 'follow_up',
                            'is_individual_questions': False,
                            'fallback_reason': 'missing_answers'
                        }
                
                # í„´ ì „í™˜ í•„ìš” (ê¼¬ë¦¬ ì§ˆë¬¸ 2ê°œ ì™„ë£Œ)
                else:
                    # ë‹¤ìŒ ë©´ì ‘ê´€ ê²°ì •
                    roles = ['HR', 'TECH', 'COLLABORATION']
                    current_index = roles.index(current_interviewer)
                    next_index = (current_index + 1) % len(roles)
                    next_interviewer = roles[next_index]
                    
                    # ğŸ†• í„´ ì „í™˜ ì‹œ ìƒˆë¡œìš´ ë©´ì ‘ê´€ì˜ ìƒíƒœ ì´ˆê¸°í™”
                    turn_state[next_interviewer] = {
                        'main_question_asked': False,
                        'follow_up_count': 0
                    }
                    
                    # ğŸ†• stateì˜ current_interviewerë„ ì—…ë°ì´íŠ¸
                    state['current_interviewer'] = next_interviewer
                    
                    return {
                        'turn_switch': True,
                        'next_interviewer': next_interviewer,
                        'message': f'{current_interviewer} ë©´ì ‘ê´€ í„´ ì™„ë£Œ, {next_interviewer} ë©´ì ‘ê´€ìœ¼ë¡œ ì „í™˜'
                    }
            
        except Exception as e:
            print(f"[ERROR] state ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            user_name = state.get('user_name', 'ì§€ì›ì')
            return {
                'question': f'{user_name}ë‹˜, ììœ ë¡­ê²Œ ë³¸ì¸ì— ëŒ€í•´ ë§ì”€í•´ ì£¼ì„¸ìš”.',
                'intent': 'ì¼ë°˜ì ì¸ ë©´ì ‘ ì§ˆë¬¸',
                'interviewer_type': 'HR'
            }

   
    def generate_follow_up_question(self, previous_question: str, user_answer: str, 
                                   chun_sik_answer: str, company_info: Dict, 
                                   interviewer_role: str, user_resume: Dict = None) -> Dict:
        """ë™ì  ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± - ë‹µë³€ ê¸°ë°˜ ì‹¤ì‹œê°„ ì‹¬ì¸µ íƒêµ¬"""
        
        # í”„ë¡¬í”„íŠ¸ ë¹Œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
        position = user_resume.get('position', 'ê°œë°œì') if user_resume else 'ê°œë°œì'
        prompt = self.prompt_builder.build_follow_up_question_prompt(
            previous_question, user_answer, chun_sik_answer, company_info, interviewer_role, position
        )
        system_prompt = self.prompt_builder.build_system_prompt_for_follow_up()
        
        # LLM í˜¸ì¶œ
        try:
            response = self.openai_client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=MAX_TOKENS,
                temperature=0.7  # ì°½ì˜ì ì¸ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìœ„í•´ ì¡°ê¸ˆ ë†’ì„
            )
            
            # JSON íŒŒì‹± ê°œì„  (ê¼¬ë¦¬ ì§ˆë¬¸ìš©)
            result_text = response.choices[0].message.content.strip()
            
            if not result_text:
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
            
            # JSON ë¸”ë¡ ì¶”ì¶œ
            if '```json' in result_text:
                json_start = result_text.find('```json') + 7
                json_end = result_text.find('```', json_start)
                result_text = result_text[json_start:json_end].strip()
            elif '{' in result_text and '}' in result_text:
                json_start = result_text.find('{')
                json_end = result_text.rfind('}') + 1
                result_text = result_text[json_start:json_end]
            
            result = json.loads(result_text)
            
            # ê²°ê³¼ ê²€ì¦ ë° ë³´ì •
            if not result.get('question'):
                raise ValueError("question í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # ì´ë¦„ í˜¸ëª… ì¶”ê°€
            candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
            result['question'] = self._add_candidate_name_to_question(result['question'], candidate_name)
            
            result['interviewer_type'] = interviewer_role
            result['question_flow_type'] = 'follow_up'
            result['question_source'] = 'llm_follow_up'
            return result
            
        except Exception as e:
            print(f"[ERROR] ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ê¼¬ë¦¬ ì§ˆë¬¸
            return self._get_fallback_follow_up_question(interviewer_role, previous_question, user_resume)
    
    def _try_generate_from_db_template(self, user_resume: Dict, company_info: Dict, 
                                     interviewer_role: str, topic: str) -> Optional[Dict]:
        """DB í…œí”Œë¦¿ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ì‹œë„"""
        try:
            return self._generate_from_db_template_with_topic(
                user_resume, company_info, interviewer_role, topic
            )
        except Exception as e:
            print(f"[ERROR] DB í…œí”Œë¦¿ ìƒì„± ì¤‘ ì˜ˆì™¸: {e}")
            return None
    
    def _try_generate_from_llm(self, user_resume: Dict, company_info: Dict, 
                             interviewer_role: str, topic: str) -> Optional[Dict]:
        """LLM ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ì‹œë„"""
        try:
            return self._generate_from_llm_with_topic(
                user_resume, company_info, interviewer_role, topic
            )
        except Exception as e:
            print(f"[ERROR] LLM ìƒì„± ì¤‘ ì˜ˆì™¸: {e}")
            return None
    
    def _generate_from_db_template_with_topic(self, user_resume: Dict, company_info: Dict, 
                                            interviewer_role: str, topic: str) -> Dict:
        """ì£¼ì œ íŠ¹í™” DB í…œí”Œë¦¿ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± (LLM íŠœë‹ í¬í•¨)"""
        question_type = self.interviewer_role_to_db_id.get(interviewer_role)
        if not question_type:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë©´ì ‘ê´€ ì—­í• : {interviewer_role}")
        
        # í•´ë‹¹ ë©´ì ‘ê´€ ìœ í˜•ì˜ ì§ˆë¬¸ë“¤ í•„í„°ë§
        role_questions = [
            q for q in self.fixed_questions 
            if q.get('question_type') == question_type
        ]
        
        if not role_questions:
            raise ValueError(f"{interviewer_role} ìœ í˜•ì˜ ì§ˆë¬¸ì´ DBì— ì—†ìŠµë‹ˆë‹¤")
        
        # ëœë¤ ì„ íƒ
        selected_template = random.choice(role_questions)
        question_content = selected_template.get('question_content', 'ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        
        # í…œí”Œë¦¿ì— ë°ì´í„° ì£¼ì… (ì°¸ì¡°ì§ˆë¬¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•)
        question_content = self._inject_data_to_template(question_content, user_resume, company_info)
        
        # â­ ìƒˆë¡œìš´ LLM íŠœë‹ ë‹¨ê³„ ì¶”ê°€ â­
        enhanced_question = self._enhance_db_template_with_llm(
            db_template=question_content,
            user_resume=user_resume,
            company_info=company_info,
            interviewer_role=interviewer_role
        )
        
        # ì´ë¦„ í˜¸ëª… ì¶”ê°€
        candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
        final_question = self._add_candidate_name_to_question(enhanced_question, candidate_name)
        
        return {
            'question': final_question,
            'intent': f"{topic} ê´€ë ¨ {selected_template.get('question_intent', f'{interviewer_role} ì—­ëŸ‰ í‰ê°€')}",
            'interviewer_type': interviewer_role,
            'topic': topic,
            'question_source': 'db_template_enhanced'  # ì†ŒìŠ¤ í‘œì‹œ ë³€ê²½
        }
    
    def _generate_from_llm_with_topic(self, user_resume: Dict, company_info: Dict, 
                                     interviewer_role: str, topic: str) -> Dict:
        """ì£¼ì œ íŠ¹í™” LLM ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±"""
        
        # í”„ë¡¬í”„íŠ¸ ë¹Œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.prompt_builder.build_main_question_prompt(
            user_resume, company_info, interviewer_role, topic
        )
        system_prompt = self.prompt_builder.build_system_prompt_for_question_generation()
        
        # LLM í˜¸ì¶œ
        try:
            response = self.openai_client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE
            )
            
            # JSON íŒŒì‹±
            result_text = response.choices[0].message.content.strip()
            
            if not result_text:
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
            
            result = json.loads(result_text)
            
            # ê²°ê³¼ ê²€ì¦ ë° ë³´ì •
            if not result.get('question'):
                raise ValueError("question í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # ì´ë¦„ í˜¸ëª… ì¶”ê°€
            candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
            result['question'] = self._add_candidate_name_to_question(result['question'], candidate_name)
            
            result['interviewer_type'] = interviewer_role
            result['topic'] = topic
            result['question_source'] = 'llm_generated'
            return result
            
        except Exception as e:
            print(f"[ERROR] LLM ë©”ì¸ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _get_generic_question(self, interviewer_role: str, topic: str, candidate_name: str = None) -> Dict:
        """ìµœì¢… í´ë°±: ì¼ë°˜ì ì¸ ì§ˆë¬¸"""
        generic_questions = {
            'HR': {
                'question': f'{topic} ê´€ë ¨í•´ì„œ ë³¸ì¸ì˜ ê²½í—˜ì„ ììœ ë¡­ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”.',
                'intent': 'ì§€ì›ìì˜ ê²½í—˜ê³¼ ì—­ëŸ‰ íŒŒì•…'
            },
            'TECH': {
                'question': f'{topic}ì™€ ê´€ë ¨ëœ ê¸°ìˆ ì  ê²½í—˜ì´ë‚˜ í•™ìŠµí•œ ë‚´ìš©ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.',
                'intent': 'ê¸°ìˆ ì  ì—­ëŸ‰ê³¼ í•™ìŠµ ëŠ¥ë ¥ í‰ê°€'
            },
            'COLLABORATION': {
                'question': f'{topic} ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ëŒ€ì²˜í•˜ì…¨ëŠ”ì§€ êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ë§ì”€í•´ ì£¼ì„¸ìš”.',
                'intent': 'í˜‘ì—… ëŠ¥ë ¥ê³¼ ë¬¸ì œ í•´ê²° ì—­ëŸ‰ í‰ê°€'
            }
        }
        
        template = generic_questions.get(interviewer_role, generic_questions['HR'])
        question_text = template['question']
        
        if candidate_name:
            question_text = self._add_candidate_name_to_question(question_text, candidate_name)
        
        return {
            'question': question_text,
            'intent': template['intent'],
            'interviewer_type': interviewer_role,
            'topic': topic,
            'question_source': 'generic_fallback'
        }
    
    def _get_fallback_follow_up_question(self, interviewer_role: str, previous_question: str, user_resume: Dict = None) -> Dict:
        """ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ ì‹œ í´ë°± ì§ˆë¬¸"""
        
        fallback_follow_ups = {
            'HR': {
                'question': 'ê·¸ëŸ° ê²½í—˜ì„ í†µí•´ ì–´ë–¤ ì ì„ ë°°ìš°ì…¨ë‚˜ìš”?',
                'intent': 'ê²½í—˜ì„ í†µí•œ í•™ìŠµê³¼ ì„±ì¥ í™•ì¸'
            },
            'TECH': {
                'question': 'ê·¸ ê³¼ì •ì—ì„œ ê°€ì¥ ì–´ë ¤ì› ë˜ ê¸°ìˆ ì  ë¬¸ì œëŠ” ë¬´ì—‡ì´ì—ˆë‚˜ìš”?',
                'intent': 'ê¸°ìˆ ì  ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ í‰ê°€'
            },
            'COLLABORATION': {
                'question': 'ê·¸ ìƒí™©ì—ì„œ íŒ€ì›ë“¤ê³¼ëŠ” ì–´ë–»ê²Œ ì†Œí†µí•˜ì…¨ë‚˜ìš”?',
                'intent': 'íŒ€ ë‚´ ì†Œí†µ ë° í˜‘ì—… ëŠ¥ë ¥ í‰ê°€'
            }
        }
        
        template = fallback_follow_ups.get(interviewer_role, fallback_follow_ups['HR'])
        question_text = template['question']
        
        # ì´ë¦„ í˜¸ëª… ì¶”ê°€
        candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
        question_text = self._add_candidate_name_to_question(question_text, candidate_name)
        
        return {
            'question': question_text,
            'intent': template['intent'],
            'interviewer_type': interviewer_role,
            'question_flow_type': 'follow_up',
            'question_source': 'fallback_follow_up'
        }
    
    def generate_follow_up_questions_for_both(self, previous_question: str, user_answer: str,
                                             ai_answer: str, company_info: Dict,
                                             interviewer_role: str, user_resume: Dict = None) -> Dict:
        """ì‚¬ìš©ìì™€ AI ê°ê°ì˜ ë‹µë³€ì— ê¸°ë°˜í•œ ê°œë³„ ê¼¬ë¦¬ì§ˆë¬¸ 2ê°œ ìƒì„±"""
        
        try:
            print(f"[DEBUG] ê°œë³„ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì‹œì‘ - ë©´ì ‘ê´€: {interviewer_role}")
            
            # ì‚¬ìš©ììš© ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±
            user_follow_up = self.generate_follow_up_question(
                previous_question=previous_question,
                user_answer=user_answer,
                chun_sik_answer=ai_answer,  # AI ë‹µë³€ë„ ì „ë‹¬ (ë¹„êµ ì°¸ê³ ìš©)
                company_info=company_info,
                interviewer_role=interviewer_role,
                user_resume=user_resume
            )
            
            # AIìš© ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± (AI ë‹µë³€ì— ë” ì§‘ì¤‘)
            ai_follow_up = self._generate_ai_focused_follow_up(
                previous_question=previous_question,
                user_answer=user_answer,
                ai_answer=ai_answer,
                company_info=company_info,
                interviewer_role=interviewer_role,
                user_resume=user_resume
            )
            
            result = {
                'user_question': user_follow_up,
                'ai_question': ai_follow_up,
                'interviewer_type': interviewer_role,
                'question_type': 'follow_up',
                'is_individual_questions': True
            }
            
            print(f"[DEBUG] ê°œë³„ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì™„ë£Œ")
            print(f"[DEBUG] ì‚¬ìš©ì ì§ˆë¬¸: {user_follow_up.get('question', 'N/A')[:50]}...")
            print(f"[DEBUG] AI ì§ˆë¬¸: {ai_follow_up.get('question', 'N/A')[:50]}...")
            
            return result
            
        except Exception as e:
            print(f"[ERROR] ê°œë³„ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê³µí†µ ê¼¬ë¦¬ì§ˆë¬¸ ì‚¬ìš©
            common_follow_up = self.generate_follow_up_question(
                previous_question, user_answer, ai_answer, 
                company_info, interviewer_role, user_resume
            )
            
            return {
                'user_question': common_follow_up,
                'ai_question': common_follow_up,
                'interviewer_type': interviewer_role,
                'question_type': 'follow_up',
                'is_individual_questions': False,
                'fallback_reason': 'individual_generation_failed'
            }
    
    def _generate_ai_focused_follow_up(self, previous_question: str, user_answer: str,
                                     ai_answer: str, company_info: Dict,
                                     interviewer_role: str, user_resume: Dict = None) -> Dict:
        """AI ë‹µë³€ì— ë” ì§‘ì¤‘í•œ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±"""
        
        # AIì—ê²Œ ë” ì í•©í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        position = user_resume.get('position', 'ê°œë°œì') if user_resume else 'ê°œë°œì'
        
        # AI ì¤‘ì‹¬ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ (user_answerì™€ ai_answer ìˆœì„œ ë°”ê¿ˆ)
        ai_focused_prompt = self.prompt_builder.build_follow_up_question_prompt(
            previous_question, ai_answer, user_answer, company_info, interviewer_role, position
        )
        
        # AI ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë” ê¸°ìˆ ì /ì´ë¡ ì  ê´€ì  ê°•ì¡°)
        ai_system_prompt = f"""
ë‹¹ì‹ ì€ {interviewer_role} ë©´ì ‘ê´€ì…ë‹ˆë‹¤. AI ì§€ì›ìì˜ ë‹µë³€ì— ê¸°ë°˜í•˜ì—¬ ì‹¬ì¸µì ì¸ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

AI ì§€ì›ì íŠ¹ì„±ì„ ê³ ë ¤í•œ ì§ˆë¬¸ ìƒì„± ê°€ì´ë“œë¼ì¸:
- ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì´ë‚˜ ì´ë¡ ì  ë°°ê²½ì„ ë” ê¹Šì´ íƒêµ¬
- êµ¬í˜„ ë°©ë²•ë¡ ì´ë‚˜ ì•„í‚¤í…ì²˜ì  ê´€ì ì—ì„œ ì ‘ê·¼
- ë¹„êµ ë¶„ì„ì´ë‚˜ ëŒ€ì•ˆì  ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•œ ì§ˆë¬¸
- í™•ì¥ì„±ì´ë‚˜ ìµœì í™” ê´€ì ì—ì„œì˜ ì‹¬í™” ì§ˆë¬¸

ì‘ë‹µ í˜•ì‹:
{{
    "question": "ì§ˆë¬¸ ë‚´ìš©",
    "intent": "ì§ˆë¬¸ ì˜ë„",
    "focus": "ê¸°ìˆ ì  ì‹¬í™”"
}}
        """
        
        try:
            response = self.openai_client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": ai_system_prompt},
                    {"role": "user", "content": ai_focused_prompt}
                ],
                max_tokens=MAX_TOKENS,
                temperature=0.7
            )
            
            result_text = response.choices[0].message.content.strip()
            
            if not result_text:
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
            
            # JSON íŒŒì‹±
            if '```json' in result_text:
                json_start = result_text.find('```json') + 7
                json_end = result_text.find('```', json_start)
                result_text = result_text[json_start:json_end].strip()
            elif '{' in result_text and '}' in result_text:
                json_start = result_text.find('{')
                json_end = result_text.rfind('}') + 1
                result_text = result_text[json_start:json_end]
            
            result = json.loads(result_text)
            
            if not result.get('question'):
                raise ValueError("question í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # AIìš© ì§ˆë¬¸ì´ë¯€ë¡œ "ì¶˜ì‹ì´ë‹˜" í˜¸ëª… ì¶”ê°€
            result['question'] = f"ì¶˜ì‹ì´ë‹˜, {result['question']}"
            result['interviewer_type'] = interviewer_role
            result['question_flow_type'] = 'ai_follow_up'
            result['question_source'] = 'ai_focused_llm'
            
            return result
            
        except Exception as e:
            print(f"[ERROR] AI ì¤‘ì‹¬ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: AIìš© ê¸°ë³¸ ê¼¬ë¦¬ì§ˆë¬¸
            return self._get_fallback_follow_up_question(interviewer_role, previous_question, 
                                                       {"name": "ì¶˜ì‹ì´"})

    def _inject_data_to_template(self, template: str, user_resume: Dict, company_info: Dict) -> str:
        """í…œí”Œë¦¿ì— ì‹¤ì œ ë°ì´í„° ë™ì  ì£¼ì…"""
        result = template
        
        # íšŒì‚¬ ì •ë³´ ì¹˜í™˜
        result = result.replace('{company_name}', company_info.get('name', 'íšŒì‚¬'))
        result = result.replace('{talent_profile}', company_info.get('talent_profile', ''))
        result = result.replace('{tech_focus}', ', '.join(company_info.get('tech_focus', [])))
        
        # ì§€ì›ì ì •ë³´ ì¹˜í™˜ 
        if user_resume:
            result = result.replace('{candidate_name}', user_resume.get('name', 'ì§€ì›ì'))
            result = result.replace('{experience_years}', str(user_resume.get('career_years', '0')))
            result = result.replace('{main_skills}', ', '.join(user_resume.get('technical_skills', [])[:3]))
        
        # AI ì§€ì›ì ì´ë¦„ í†µì¼
        result = result.replace('{persona_name}', 'ì¶˜ì‹ì´')
        
        return result



    def _add_candidate_name_to_question(self, question: str, candidate_name: str) -> str:
        """ì§ˆë¬¸ì— ì§€ì›ì ì´ë¦„ í˜¸ëª… ì¶”ê°€"""
        if not candidate_name or candidate_name == 'ì§€ì›ì':
            return question
        
        # ì´ë¯¸ ì´ë¦„ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if candidate_name in question:
            return question
        
        # ì§ˆë¬¸ ëì— ì´ë¦„ ì¶”ê°€
        if question.endswith('.') or question.endswith('?'):
            return f"{candidate_name}ë‹˜, {question}"
        else:
            return f"{candidate_name}ë‹˜, {question}."

    def _enhance_db_template_with_llm(self, db_template: str, user_resume: Dict, 
                                    company_info: Dict, interviewer_role: str) -> str:
        """DB í…œí”Œë¦¿ì„ LLMìœ¼ë¡œ íŠœë‹/ê°œì„ í•˜ëŠ” ë©”ì„œë“œ"""
        try:
            # í”„ë¡¬í”„íŠ¸ ë¹Œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ë„í™”ëœ íŠœë‹ í”„ë¡¬í”„íŠ¸ ìƒì„±
            enhancement_prompt = self.prompt_builder.build_db_template_enhancement_prompt(
                db_template=db_template,
                user_resume=user_resume,
                company_info=company_info,
                interviewer_role=interviewer_role
            )
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_prompt = self.prompt_builder.build_system_prompt_for_question_generation()
            
            # OpenAI API í˜¸ì¶œ
            response = self.openai_client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": enhancement_prompt}
                ],
                max_tokens=300,
                temperature=0.7
            )
            
            # ì‘ë‹µ íŒŒì‹±
            response_text = response.choices[0].message.content.strip()
            
            try:
                # JSON íŒŒì‹± ì‹œë„
                import json
                result = json.loads(response_text)
                
                if 'question' in result and result['question'].strip():
                    return result['question'].strip()
                else:
                    logger.warning(f"LLM ì‘ë‹µì— question í•„ë“œê°€ ì—†ìŒ: {result}")
                    return db_template
                    
            except json.JSONDecodeError as e:
                logger.warning(f"LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {response_text}, ì—ëŸ¬: {e}")
                return db_template
                
        except Exception as e:
            logger.error(f"DB í…œí”Œë¦¿ LLM íŠœë‹ ì‹¤íŒ¨: {e}")
            return db_template

    def _try_generate_main_question_for_user(self, user_resume: Dict, company_info: Dict, 
                                            interviewer_role: str, topic: str) -> Dict:
        """ì‚¬ìš©ìì—ê²Œ ì í•©í•œ ë©”ì¸ ì§ˆë¬¸ ìƒì„± ì‹œë„ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)"""
        use_db_first = random.choice([True, False])
        
        question_result = None
        
        if use_db_first:
            # 1ì°¨: DB í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„± ì‹œë„
            question_result = self._try_generate_from_db_template(
                user_resume, company_info, interviewer_role, topic
            )
            if question_result:
                return question_result
            
            # 2ì°¨: DB ì‹¤íŒ¨ ì‹œ LLM ìƒì„± ì‹œë„ 
            question_result = self._try_generate_from_llm(
                user_resume, company_info, interviewer_role, topic
            )
            if question_result:
                return question_result
        else:
            # 1ì°¨: LLM ê¸°ë°˜ ìƒì„± ì‹œë„
            question_result = self._try_generate_from_llm(
                user_resume, company_info, interviewer_role, topic
            )
            if question_result:
                return question_result
            
            # 2ì°¨: LLM ì‹¤íŒ¨ ì‹œ DB í…œí”Œë¦¿ ì‹œë„
            question_result = self._try_generate_from_db_template(
                user_resume, company_info, interviewer_role, topic
            )
            if question_result:
                return question_result
        
        # ìµœì¢… í´ë°±: ì¼ë°˜ì ì¸ ì§ˆë¬¸
        return self._get_generic_question(interviewer_role, topic, 
                                        user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì')

    def _try_generate_main_question_for_ai(self, user_resume: Dict, company_info: Dict, 
                                          interviewer_role: str, topic: str) -> Dict:
        """AI ì§€ì›ìì—ê²Œ ì í•©í•œ ë©”ì¸ ì§ˆë¬¸ ìƒì„± ì‹œë„"""
        use_db_first = random.choice([True, False])
        
        question_result = None
        
        if use_db_first:
            # 1ì°¨: AIìš© DB í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„± ì‹œë„
            try:
                question_result = self._generate_from_db_template_for_ai_with_topic(
                    user_resume, company_info, interviewer_role, topic
                )
                if question_result:
                    return question_result
            except Exception as e:
                print(f"[ERROR] AIìš© DB í…œí”Œë¦¿ ìƒì„± ì¤‘ ì˜ˆì™¸: {e}")
            
            # 2ì°¨: DB ì‹¤íŒ¨ ì‹œ AIìš© LLM ìƒì„± ì‹œë„
            try:
                question_result = self._generate_from_llm_for_ai_with_topic(
                    user_resume, company_info, interviewer_role, topic
                )
                if question_result:
                    return question_result
            except Exception as e:
                print(f"[ERROR] AIìš© LLM ìƒì„± ì¤‘ ì˜ˆì™¸: {e}")
        else:
            # 1ì°¨: AIìš© LLM ê¸°ë°˜ ìƒì„± ì‹œë„
            try:
                question_result = self._generate_from_llm_for_ai_with_topic(
                    user_resume, company_info, interviewer_role, topic
                )
                if question_result:
                    return question_result
            except Exception as e:
                print(f"[ERROR] AIìš© LLM ìƒì„± ì¤‘ ì˜ˆì™¸: {e}")
            
            # 2ì°¨: LLM ì‹¤íŒ¨ ì‹œ AIìš© DB í…œí”Œë¦¿ ì‹œë„
            try:
                question_result = self._generate_from_db_template_for_ai_with_topic(
                    user_resume, company_info, interviewer_role, topic
                )
                if question_result:
                    return question_result
            except Exception as e:
                print(f"[ERROR] AIìš© DB í…œí”Œë¦¿ ìƒì„± ì¤‘ ì˜ˆì™¸: {e}")
        
        # ìµœì¢… í´ë°±: AIìš© ì¼ë°˜ì ì¸ ì§ˆë¬¸
        return self._get_generic_question(interviewer_role, topic, 'ì¶˜ì‹ì´')

    def _generate_from_llm_for_ai_with_topic(self, user_resume: Dict, company_info: Dict, 
                                     interviewer_role: str, topic: str) -> Dict:
        """AI ì§€ì›ìì—ê²Œ ì í•©í•œ LLM ê¸°ë°˜ ë©”ì¸ ì§ˆë¬¸ ìƒì„±"""
        
        # í”„ë¡¬í”„íŠ¸ ë¹Œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
        base_prompt = self.prompt_builder.build_main_question_prompt(
            user_resume, company_info, interviewer_role, topic
        )
        
        # AI ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        ai_system_prompt = f"""
ë‹¹ì‹ ì€ {interviewer_role} ë©´ì ‘ê´€ì…ë‹ˆë‹¤. AI ì§€ì›ìì—ê²Œ ì í•©í•œ ë©”ì¸ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

AI ì§€ì›ì íŠ¹ì„±ì„ ê³ ë ¤í•œ ì§ˆë¬¸ ìƒì„± ê°€ì´ë“œë¼ì¸:
- ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì´ë‚˜ ì´ë¡ ì  ë°°ê²½ì„ ë” ê¹Šì´ íƒêµ¬í•˜ëŠ” ì§ˆë¬¸
- êµ¬í˜„ ë°©ë²•ë¡ ì´ë‚˜ ì•„í‚¤í…ì²˜ì  ê´€ì ì—ì„œ ì ‘ê·¼í•˜ëŠ” ì§ˆë¬¸
- ë¹„êµ ë¶„ì„ì´ë‚˜ ëŒ€ì•ˆì  ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•œ ì§ˆë¬¸
- í™•ì¥ì„±ì´ë‚˜ ìµœì í™” ê´€ì ì—ì„œì˜ ì‹¬í™” ì§ˆë¬¸
- AIì˜ í•™ìŠµ ëŠ¥ë ¥, ë°ì´í„° ì²˜ë¦¬, ëª¨ë¸ ìµœì í™” ë“±ì— ì´ˆì ì„ ë§ì¶˜ ì§ˆë¬¸

ì‘ë‹µ í˜•ì‹:
{{
    "question": "ì§ˆë¬¸ ë‚´ìš©",
    "intent": "ì§ˆë¬¸ ì˜ë„",
    "focus": "ê¸°ìˆ ì  ì‹¬í™”"
}}
        """
        
        # LLM í˜¸ì¶œ
        try:
            response = self.openai_client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": ai_system_prompt},
                    {"role": "user", "content": base_prompt}
                ],
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE
            )
            
            # JSON íŒŒì‹±
            result_text = response.choices[0].message.content.strip()
            
            if not result_text:
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
            
            result = json.loads(result_text)
            
            # ê²°ê³¼ ê²€ì¦ ë° ë³´ì •
            if not result.get('question'):
                raise ValueError("question í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # AIìš© ì§ˆë¬¸ì´ë¯€ë¡œ "ì¶˜ì‹ì´ë‹˜" í˜¸ëª… ì¶”ê°€
            result['question'] = f"ì¶˜ì‹ì´ë‹˜, {result['question']}"
            result['interviewer_type'] = interviewer_role
            result['topic'] = topic
            result['question_source'] = 'llm_generated_for_ai'
            return result
            
        except Exception as e:
            print(f"[ERROR] AI ì¤‘ì‹¬ ë©”ì¸ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _generate_from_db_template_for_ai_with_topic(self, user_resume: Dict, company_info: Dict, 
                                            interviewer_role: str, topic: str) -> Dict:
        """AI ì§€ì›ìì—ê²Œ ì í•©í•œ DB í…œí”Œë¦¿ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±"""
        question_type = self.interviewer_role_to_db_id.get(interviewer_role)
        if not question_type:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë©´ì ‘ê´€ ì—­í• : {interviewer_role}")
        
        # í•´ë‹¹ ë©´ì ‘ê´€ ìœ í˜•ì˜ ì§ˆë¬¸ë“¤ í•„í„°ë§
        role_questions = [
            q for q in self.fixed_questions 
            if q.get('question_type') == question_type
        ]
        
        if not role_questions:
            raise ValueError(f"{interviewer_role} ìœ í˜•ì˜ ì§ˆë¬¸ì´ DBì— ì—†ìŠµë‹ˆë‹¤")
        
        # ëœë¤ ì„ íƒ
        selected_template = random.choice(role_questions)
        question_content = selected_template.get('question_content', 'ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        
        # AIìš© ì§ˆë¬¸ì´ë¯€ë¡œ "ì¶˜ì‹ì´ë‹˜" í˜¸ëª… ì¶”ê°€
        question_with_name = f"ì¶˜ì‹ì´ë‹˜, {question_content}"
        
        return {
            'question': question_with_name,
            'intent': f"{topic} ê´€ë ¨ {selected_template.get('question_intent', f'{interviewer_role} ì—­ëŸ‰ í‰ê°€')}",
            'interviewer_type': interviewer_role,
            'topic': topic,
            'question_source': 'db_template_for_ai'
        }


# ë¡œê±° ì´ˆê¸°í™”
logger = logging.getLogger(__name__)
