#!/usr/bin/env python3
"""
ìˆœìˆ˜ ì§ˆë¬¸ ìƒì„±ê¸° - InterviewerServiceì—ì„œ ë¶„ë¦¬
í„´ì œ ê´€ë¦¬ ë¡œì§ ì—†ì´ ì˜¤ì§ ì§ˆë¬¸ ìƒì„±ë§Œ ë‹´ë‹¹
"""

import os
import sys
import json
import random
import time
import openai
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from backend.services.supabase_client import get_supabase_client
from llm.shared.constants import GPT_MODEL, MAX_TOKENS, TEMPERATURE
from llm.candidate.model import CandidatePersona
from .prompt import InterviewerPromptBuilder


class QuestionGenerator:
    """ìˆœìˆ˜ ì§ˆë¬¸ ìƒì„±ê¸° - í„´ì œ ê´€ë¦¬ ì—†ì´ ì§ˆë¬¸ ìƒì„±ë§Œ ë‹´ë‹¹"""
    
    def __init__(self):
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = get_supabase_client()
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.openai_client = openai.OpenAI(api_key=api_key)
        
        # í”„ë¡¬í”„íŠ¸ ë¹Œë” ì´ˆê¸°í™”
        self.prompt_builder = InterviewerPromptBuilder()
        
        # DB ë°ì´í„° ë¡œë”© ë° ìºì‹±
        self.companies_data = self._load_companies_data()
        self.fixed_questions = self._load_fixed_questions()
        
        # DB ì§ˆë¬¸ íƒ€ì… ID ë§¤í•‘ (ë©´ì ‘ê´€ ì—­í•  â†’ DB ID)
        self.interviewer_role_to_db_id = {
            'HR': 1,
            'TECH': 2, 
            'COLLABORATION': 3
        }
        
        # ë©´ì ‘ê´€ë³„ ì£¼ì œ í’€ ì •ì˜
        self.topic_pools = {
            'HR': ['ì¸ì„±_ê°€ì¹˜ê´€', 'ì„±ì¥_ë™ê¸°', 'ê°ˆë“±_í•´ê²°', 'ìŠ¤íŠ¸ë ˆìŠ¤_ê´€ë¦¬', 'íŒ€ì›Œí¬_ë¦¬ë”ì‹­'],
            'TECH': ['ê¸°ìˆ _ì—­ëŸ‰', 'ë¬¸ì œ_í•´ê²°', 'ì„±ëŠ¥_ìµœì í™”', 'ì½”ë“œ_í’ˆì§ˆ', 'ìƒˆë¡œìš´_ê¸°ìˆ _í•™ìŠµ'],
            'COLLABORATION': ['ì†Œí†µ_ëŠ¥ë ¥', 'í”„ë¡œì íŠ¸_í˜‘ì—…', 'ì˜ê²¬_ì¡°ìœ¨', 'í¬ë¡œìŠ¤_íŒ€_í˜‘ì—…', 'ì¡°ì§_ë¬¸í™”_ì ì‘']
        }
    
    def _load_companies_data(self) -> Dict[str, Any]:
        """íšŒì‚¬ ë°ì´í„° ë¡œë”© ë° ìºì‹±"""
        try:
            result = self.client.table('company').select('*').execute()
            companies_dict = {}
            
            # CompanyDataLoaderì™€ ë™ì¼í•œ ë§¤í•‘ í…Œì´ë¸” ì‚¬ìš©
            company_id_mapping = {
                'ë„¤ì´ë²„': 'naver',
                'ì¹´ì¹´ì˜¤': 'kakao', 
                'ë¼ì¸': 'line',
                'ë¼ì¸í”ŒëŸ¬ìŠ¤': 'ë¼ì¸í”ŒëŸ¬ìŠ¤',
                'ì¿ íŒ¡': 'coupang',
                'ë°°ë‹¬ì˜ë¯¼ì¡±': 'baemin',
                'ë‹¹ê·¼ë§ˆì¼“': 'daangn',
                'í† ìŠ¤': 'toss'
            }
            
            if result.data:
                for company in result.data:
                    company_name = company.get('name', '')
                    english_id = company_id_mapping.get(company_name, company_name.lower())
                    companies_dict[english_id] = company
                    
            print(f"[SUCCESS] íšŒì‚¬ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(companies_dict)}ê°œ")
            return companies_dict
        except Exception as e:
            print(f"[ERROR] íšŒì‚¬ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def _load_fixed_questions(self) -> List[Dict[str, Any]]:
        """ê³ ì • ì§ˆë¬¸ ë°ì´í„° ë¡œë”© ë° ìºì‹±"""
        try:
            result = self.client.table('fix_question').select('*').execute()
            questions = result.data if result.data else []
            print(f"[SUCCESS] ê³ ì • ì§ˆë¬¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(questions)}ê°œ")
            return questions
        except Exception as e:
            print(f"[ERROR] ê³ ì • ì§ˆë¬¸ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return []
    
    def generate_fixed_question(self, question_index: int, company_id: str, user_resume: Dict = None) -> Dict:
        """ê³ ì • ì§ˆë¬¸ ìƒì„± (ìê¸°ì†Œê°œ, ì§€ì›ë™ê¸°)"""
        if question_index == 0:
            # ì²« ë²ˆì§¸ ì§ˆë¬¸: ìê¸°ì†Œê°œ
            candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
            base_question = 'ê°„ë‹¨í•˜ê²Œ ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.'
            question_with_name = self._add_candidate_name_to_question(base_question, candidate_name)
            return {
                'question': question_with_name,
                'intent': 'ì§€ì›ìì˜ ê¸°ë³¸ ì •ë³´ì™€ ì„±ê²©, ì—­ëŸ‰ì„ íŒŒì•…',
                'interviewer_type': 'INTRO'
            }
        
        elif question_index == 1:
            # ë‘ ë²ˆì§¸ ì§ˆë¬¸: ì§€ì›ë™ê¸°
            company_info = self.companies_data.get(company_id, {})
            company_name = company_info.get('name', 'ì €í¬ íšŒì‚¬')
            
            base_question = f'ì €í¬ {company_name}ì— ì§€ì›í•˜ì‹  ë™ê¸°ë¥¼ ë§ì”€í•´ ì£¼ì„¸ìš”.'
            candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
            question_with_name = self._add_candidate_name_to_question(base_question, candidate_name)
            
            return {
                'question': question_with_name,
                'intent': 'íšŒì‚¬ì— ëŒ€í•œ ê´€ì‹¬ë„ì™€ ì§€ì› ë™ê¸° íŒŒì•…',
                'interviewer_type': 'INTRO'
            }
        
        else:
            raise ValueError(f"ê³ ì • ì§ˆë¬¸ì€ 0, 1ë²ˆë§Œ ì§€ì›ë©ë‹ˆë‹¤. ì…ë ¥: {question_index}")
    
    def generate_intro_message(self, company_id: str, user_resume: Dict = None) -> Dict:
        """ì¸íŠ¸ë¡œ ë©”ì‹œì§€ ìƒì„± (í„´ 0ìš©)"""
        company_info = self.companies_data.get(company_id, {})
        company_name = company_info.get('name', 'ì €í¬ íšŒì‚¬')
        user_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
        
        intro_message = f"""
{company_name}ì— ì§€ì›í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.

ë©´ì ‘ê´€ ì†Œê°œ:
- HR ë©´ì ‘ê´€: ì¸ì‚¬ ë° ì§€ì› ë™ê¸° íŒŒì•…
- ê¸°ìˆ  ë©´ì ‘ê´€: ê¸°ìˆ  ì—­ëŸ‰ ë° í”„ë¡œì íŠ¸ ê²½í—˜ ê²€ì¦  
- í˜‘ì—… ë©´ì ‘ê´€: íŒ€ì›Œí¬ ë° ì†Œí”„íŠ¸ ìŠ¤í‚¬ í‰ê°€

{user_name}ë‹˜, ë©´ì ‘ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.
"""
        
        return {
            'question': intro_message,
            'intent': 'ë©´ì ‘ ì‹œì‘ ì¸ì‚¬ ë° ë©´ì ‘ê´€ ì†Œê°œ',
            'interviewer_type': 'INTRO'
        }
    
    def generate_question_by_role(self, interviewer_role: str, company_id: str, 
                                 user_resume: Dict, user_answer: str = None, 
                                 chun_sik_answer: str = None, previous_qa_pairs: List[Dict] = None) -> Dict:
        """ë©´ì ‘ê´€ ì—­í• ë³„ ì§ˆë¬¸ ìƒì„±"""
        company_info = self.companies_data.get(company_id, {})
        if not company_info:
            raise ValueError(f"íšŒì‚¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {company_id}")
        
        # ì£¼ì œ ì„ íƒ
        topic_pool = self.topic_pools.get(interviewer_role, [])
        if not topic_pool:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë©´ì ‘ê´€ ì—­í• : {interviewer_role}")
        
        selected_topic = random.choice(topic_pool)
        
        # 50% í™•ë¥ ë¡œ DB í…œí”Œë¦¿ ë˜ëŠ” LLM ìƒì„± ë°©ì‹ ì„ íƒ
        use_db_first = random.choice([True, False])
        
        question_result = None
        
        if use_db_first:
            # 1ì°¨: DB í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„± ì‹œë„
            question_result = self._try_generate_from_db_template(
                user_resume, company_info, interviewer_role, selected_topic
            )
            
            if question_result:
                return question_result
            
            # 2ì°¨: DB ì‹¤íŒ¨ ì‹œ LLM ìƒì„± ì‹œë„ 
            question_result = self._try_generate_from_llm(
                user_resume, company_info, interviewer_role, selected_topic
            )
            
            if question_result:
                return question_result
        else:
            # 1ì°¨: LLM ê¸°ë°˜ ìƒì„± ì‹œë„
            question_result = self._try_generate_from_llm(
                user_resume, company_info, interviewer_role, selected_topic
            )
            
            if question_result:
                return question_result
            
            # 2ì°¨: LLM ì‹¤íŒ¨ ì‹œ DB í…œí”Œë¦¿ ì‹œë„
            question_result = self._try_generate_from_db_template(
                user_resume, company_info, interviewer_role, selected_topic
            )
            
            if question_result:
                return question_result
        
        # ìµœì¢… í´ë°±: ì¼ë°˜ì ì¸ ì§ˆë¬¸
        return self._get_generic_question(interviewer_role, selected_topic, 
                                        user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì')
    
    def generate_question_with_orchestrator_state(self, state: Dict[str, Any]) -> Dict:
        """
        Orchestratorì˜ state ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ì•„ì„œ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ
        
        Args:
            state: Orchestratorì˜ state ê°ì²´
        """
        try:
            # Orchestratorì˜ stateì—ì„œ ì§ì ‘ ì •ë³´ ì¶”ì¶œ
            turn_count = state.get('turn_count', 0)
            current_interviewer = state.get('current_interviewer')
            turn_state = state.get('interviewer_turn_state', {})
            
            # í„´ 0: ì¸íŠ¸ë¡œ ë©”ì‹œì§€ ìƒì„±
            if turn_count == 0:
                company_id = state.get('company_id')
                user_resume = {
                    'name': state.get('user_name', 'ì§€ì›ì'),
                    'position': state.get('position', 'ê°œë°œì')
                }
                return self.generate_intro_message(company_id, user_resume)
            
            # í„´ 1: ìê¸°ì†Œê°œ (fixed)
            elif turn_count == 1:
                question_index = 0
                question = self.generate_fixed_question(question_index, state.get('company_id'), 
                                                      {"name": state.get('user_name', 'ì§€ì›ì')})
                return question
            
            # í„´ 2: ì§€ì›ë™ê¸° (fixed)
            elif turn_count == 2:
                question_index = 1
                question = self.generate_fixed_question(question_index, state.get('company_id'), 
                                                      {"name": state.get('user_name', 'ì§€ì›ì')})
                return question
            
            # í„´ 3ë¶€í„°: ë©´ì ‘ê´€ë³„ ì§ˆë¬¸ (ë©”ì¸ ì§ˆë¬¸ + ê¼¬ë¦¬ ì§ˆë¬¸)
            else:
                # ğŸ†• ìƒíƒœ ê¸°ë°˜ ë©´ì ‘ê´€ ê²°ì • ë¡œì§
                if not current_interviewer:
                    # ì²« ë²ˆì§¸ ë©´ì ‘ê´€ì€ HRë¶€í„° ì‹œì‘
                    current_interviewer = 'HR'
                
                # ğŸ†• ê²°ì •í•œ ë©´ì ‘ê´€ì„ stateì— ì„¤ì •
                state['current_interviewer'] = current_interviewer
                
                # ğŸ†• ë©´ì ‘ê´€ ìƒíƒœ ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
                if current_interviewer not in turn_state:
                    turn_state[current_interviewer] = {
                        'main_question_asked': False,
                        'follow_up_count': 0
                    }
                
                current_turn_state = turn_state.get(current_interviewer, {})
                
                # ê¸°ë³¸ user_resume êµ¬ì„±
                user_resume = {
                    'name': state.get('user_name', 'ì§€ì›ì'),
                    'position': state.get('position', 'ê°œë°œì')
                }
                
                # ë©”ì¸ ì§ˆë¬¸ ì•ˆí–ˆìœ¼ë©´ ë©”ì¸ ì§ˆë¬¸ ìƒì„±
                if not current_turn_state.get('main_question_asked', False):
                    question = self.generate_question_by_role(
                        interviewer_role=current_interviewer,
                        company_id=state.get('company_id'),
                        user_resume=user_resume,
                        previous_qa_pairs=state.get('qa_history', [])
                    )
                    return question
                
                # ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± (ìµœëŒ€ 2ê°œ)
                elif current_turn_state.get('follow_up_count', 0) < 2:
                    # ğŸ†• qa_historyì—ì„œ ìµœì‹  ë°ì´í„° ì¶”ì¶œ
                    qa_history = state.get('qa_history', [])
                    if len(qa_history) >= 2:
                        # ê°€ì¥ ìµœê·¼ ì§ˆë¬¸ê³¼ ë‹µë³€ë“¤ ì¶”ì¶œ
                        latest_qa_pairs = qa_history[-2:]  # ë§ˆì§€ë§‰ 2ê°œ (ì‚¬ìš©ì + AI ë‹µë³€)
                        previous_question = latest_qa_pairs[0]['question'] if latest_qa_pairs else ''
                        
                        # ì‚¬ìš©ìì™€ AI ë‹µë³€ ë¶„ë¦¬
                        user_answer = ""
                        ai_answer = ""
                        for qa in latest_qa_pairs:
                            if qa['answerer'] == 'user':
                                user_answer = qa['answer']
                            elif qa['answerer'] == 'ai':
                                ai_answer = qa['answer']
                    else:
                        previous_question = ""
                        user_answer = ""
                        ai_answer = ""
                    
                    company_info = self.companies_data.get(state.get('company_id'), {})
                    
                    question = self.generate_follow_up_question(
                        previous_question=previous_question,
                        user_answer=user_answer,
                        chun_sik_answer=ai_answer,
                        company_info=company_info,
                        interviewer_role=current_interviewer,
                        user_resume=user_resume
                    )
                    return question
                
                # í„´ ì „í™˜ í•„ìš” (ê¼¬ë¦¬ ì§ˆë¬¸ 2ê°œ ì™„ë£Œ)
                else:
                    # ë‹¤ìŒ ë©´ì ‘ê´€ ê²°ì •
                    roles = ['HR', 'TECH', 'COLLABORATION']
                    current_index = roles.index(current_interviewer)
                    next_index = (current_index + 1) % len(roles)
                    next_interviewer = roles[next_index]
                    
                    # ğŸ†• í„´ ì „í™˜ ì‹œ ìƒˆë¡œìš´ ë©´ì ‘ê´€ì˜ ìƒíƒœ ì´ˆê¸°í™”
                    turn_state[next_interviewer] = {
                        'main_question_asked': False,
                        'follow_up_count': 0
                    }
                    
                    # ğŸ†• stateì˜ current_interviewerë„ ì—…ë°ì´íŠ¸
                    state['current_interviewer'] = next_interviewer
                    
                    return {
                        'turn_switch': True,
                        'next_interviewer': next_interviewer,
                        'message': f'{current_interviewer} ë©´ì ‘ê´€ í„´ ì™„ë£Œ, {next_interviewer} ë©´ì ‘ê´€ìœ¼ë¡œ ì „í™˜'
                    }
            
        except Exception as e:
            print(f"[ERROR] state ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            user_name = state.get('user_name', 'ì§€ì›ì')
            return {
                'question': f'{user_name}ë‹˜, ììœ ë¡­ê²Œ ë³¸ì¸ì— ëŒ€í•´ ë§ì”€í•´ ì£¼ì„¸ìš”.',
                'intent': 'ì¼ë°˜ì ì¸ ë©´ì ‘ ì§ˆë¬¸',
                'interviewer_type': 'HR'
            }

   
    def generate_follow_up_question(self, previous_question: str, user_answer: str, 
                                   chun_sik_answer: str, company_info: Dict, 
                                   interviewer_role: str, user_resume: Dict = None) -> Dict:
        """ë™ì  ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± - ë‹µë³€ ê¸°ë°˜ ì‹¤ì‹œê°„ ì‹¬ì¸µ íƒêµ¬"""
        
        # í”„ë¡¬í”„íŠ¸ ë¹Œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
        position = user_resume.get('position', 'ê°œë°œì') if user_resume else 'ê°œë°œì'
        prompt = self.prompt_builder.build_follow_up_question_prompt(
            previous_question, user_answer, chun_sik_answer, company_info, interviewer_role, position
        )
        system_prompt = self.prompt_builder.build_system_prompt_for_follow_up()
        
        # LLM í˜¸ì¶œ
        try:
            response = self.openai_client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=MAX_TOKENS,
                temperature=0.7  # ì°½ì˜ì ì¸ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìœ„í•´ ì¡°ê¸ˆ ë†’ì„
            )
            
            # JSON íŒŒì‹± ê°œì„  (ê¼¬ë¦¬ ì§ˆë¬¸ìš©)
            result_text = response.choices[0].message.content.strip()
            
            if not result_text:
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
            
            # JSON ë¸”ë¡ ì¶”ì¶œ
            if '```json' in result_text:
                json_start = result_text.find('```json') + 7
                json_end = result_text.find('```', json_start)
                result_text = result_text[json_start:json_end].strip()
            elif '{' in result_text and '}' in result_text:
                json_start = result_text.find('{')
                json_end = result_text.rfind('}') + 1
                result_text = result_text[json_start:json_end]
            
            result = json.loads(result_text)
            
            # ê²°ê³¼ ê²€ì¦ ë° ë³´ì •
            if not result.get('question'):
                raise ValueError("question í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # ì´ë¦„ í˜¸ëª… ì¶”ê°€
            candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
            result['question'] = self._add_candidate_name_to_question(result['question'], candidate_name)
            
            result['interviewer_type'] = interviewer_role
            result['question_flow_type'] = 'follow_up'
            result['question_source'] = 'llm_follow_up'
            return result
            
        except Exception as e:
            print(f"[ERROR] ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ê¼¬ë¦¬ ì§ˆë¬¸
            return self._get_fallback_follow_up_question(interviewer_role, previous_question, user_resume)
    
    def _try_generate_from_db_template(self, user_resume: Dict, company_info: Dict, 
                                     interviewer_role: str, topic: str) -> Optional[Dict]:
        """DB í…œí”Œë¦¿ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ì‹œë„"""
        try:
            return self._generate_from_db_template_with_topic(
                user_resume, company_info, interviewer_role, topic
            )
        except Exception as e:
            print(f"[ERROR] DB í…œí”Œë¦¿ ìƒì„± ì¤‘ ì˜ˆì™¸: {e}")
            return None
    
    def _try_generate_from_llm(self, user_resume: Dict, company_info: Dict, 
                             interviewer_role: str, topic: str) -> Optional[Dict]:
        """LLM ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ì‹œë„"""
        try:
            return self._generate_from_llm_with_topic(
                user_resume, company_info, interviewer_role, topic
            )
        except Exception as e:
            print(f"[ERROR] LLM ìƒì„± ì¤‘ ì˜ˆì™¸: {e}")
            return None
    
    def _generate_from_db_template_with_topic(self, user_resume: Dict, company_info: Dict, 
                                            interviewer_role: str, topic: str) -> Dict:
        """ì£¼ì œ íŠ¹í™” DB í…œí”Œë¦¿ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±"""
        question_type_id = self.interviewer_role_to_db_id.get(interviewer_role)
        if not question_type_id:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë©´ì ‘ê´€ ì—­í• : {interviewer_role}")
        
        # í•´ë‹¹ ë©´ì ‘ê´€ ìœ í˜•ì˜ ì§ˆë¬¸ë“¤ í•„í„°ë§
        role_questions = [
            q for q in self.fixed_questions 
            if q.get('question_type_id') == question_type_id
        ]
        
        if not role_questions:
            raise ValueError(f"{interviewer_role} ìœ í˜•ì˜ ì§ˆë¬¸ì´ DBì— ì—†ìŠµë‹ˆë‹¤")
        
        # ëœë¤ ì„ íƒ
        selected_template = random.choice(role_questions)
        question_content = selected_template.get('question_content', 'ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        
        # ì´ë¦„ í˜¸ëª… ì¶”ê°€
        candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
        question_with_name = self._add_candidate_name_to_question(question_content, candidate_name)
        
        return {
            'question': question_with_name,
            'intent': f"{topic} ê´€ë ¨ {selected_template.get('question_intent', f'{interviewer_role} ì—­ëŸ‰ í‰ê°€')}",
            'interviewer_type': interviewer_role,
            'topic': topic,
            'question_source': 'db_template'
        }
    
    def _generate_from_llm_with_topic(self, user_resume: Dict, company_info: Dict, 
                                     interviewer_role: str, topic: str) -> Dict:
        """ì£¼ì œ íŠ¹í™” LLM ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±"""
        
        # í”„ë¡¬í”„íŠ¸ ë¹Œë”ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.prompt_builder.build_main_question_prompt(
            user_resume, company_info, interviewer_role, topic
        )
        system_prompt = self.prompt_builder.build_system_prompt_for_question_generation()
        
        # LLM í˜¸ì¶œ
        try:
            response = self.openai_client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE
            )
            
            # JSON íŒŒì‹±
            result_text = response.choices[0].message.content.strip()
            
            if not result_text:
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
            
            result = json.loads(result_text)
            
            # ê²°ê³¼ ê²€ì¦ ë° ë³´ì •
            if not result.get('question'):
                raise ValueError("question í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # ì´ë¦„ í˜¸ëª… ì¶”ê°€
            candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
            result['question'] = self._add_candidate_name_to_question(result['question'], candidate_name)
            
            result['interviewer_type'] = interviewer_role
            result['topic'] = topic
            result['question_source'] = 'llm_generated'
            return result
            
        except Exception as e:
            print(f"[ERROR] LLM ë©”ì¸ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _get_generic_question(self, interviewer_role: str, topic: str, candidate_name: str = None) -> Dict:
        """ìµœì¢… í´ë°±: ì¼ë°˜ì ì¸ ì§ˆë¬¸"""
        generic_questions = {
            'HR': {
                'question': f'{topic} ê´€ë ¨í•´ì„œ ë³¸ì¸ì˜ ê²½í—˜ì„ ììœ ë¡­ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”.',
                'intent': 'ì§€ì›ìì˜ ê²½í—˜ê³¼ ì—­ëŸ‰ íŒŒì•…'
            },
            'TECH': {
                'question': f'{topic}ì™€ ê´€ë ¨ëœ ê¸°ìˆ ì  ê²½í—˜ì´ë‚˜ í•™ìŠµí•œ ë‚´ìš©ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.',
                'intent': 'ê¸°ìˆ ì  ì—­ëŸ‰ê³¼ í•™ìŠµ ëŠ¥ë ¥ í‰ê°€'
            },
            'COLLABORATION': {
                'question': f'{topic} ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ëŒ€ì²˜í•˜ì…¨ëŠ”ì§€ êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ë§ì”€í•´ ì£¼ì„¸ìš”.',
                'intent': 'í˜‘ì—… ëŠ¥ë ¥ê³¼ ë¬¸ì œ í•´ê²° ì—­ëŸ‰ í‰ê°€'
            }
        }
        
        template = generic_questions.get(interviewer_role, generic_questions['HR'])
        question_text = template['question']
        
        if candidate_name:
            question_text = self._add_candidate_name_to_question(question_text, candidate_name)
        
        return {
            'question': question_text,
            'intent': template['intent'],
            'interviewer_type': interviewer_role,
            'topic': topic,
            'question_source': 'generic_fallback'
        }
    
    def _get_fallback_follow_up_question(self, interviewer_role: str, previous_question: str, user_resume: Dict = None) -> Dict:
        """ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ ì‹œ í´ë°± ì§ˆë¬¸"""
        
        fallback_follow_ups = {
            'HR': {
                'question': 'ê·¸ëŸ° ê²½í—˜ì„ í†µí•´ ì–´ë–¤ ì ì„ ë°°ìš°ì…¨ë‚˜ìš”?',
                'intent': 'ê²½í—˜ì„ í†µí•œ í•™ìŠµê³¼ ì„±ì¥ í™•ì¸'
            },
            'TECH': {
                'question': 'ê·¸ ê³¼ì •ì—ì„œ ê°€ì¥ ì–´ë ¤ì› ë˜ ê¸°ìˆ ì  ë¬¸ì œëŠ” ë¬´ì—‡ì´ì—ˆë‚˜ìš”?',
                'intent': 'ê¸°ìˆ ì  ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ í‰ê°€'
            },
            'COLLABORATION': {
                'question': 'ê·¸ ìƒí™©ì—ì„œ íŒ€ì›ë“¤ê³¼ëŠ” ì–´ë–»ê²Œ ì†Œí†µí•˜ì…¨ë‚˜ìš”?',
                'intent': 'íŒ€ ë‚´ ì†Œí†µ ë° í˜‘ì—… ëŠ¥ë ¥ í‰ê°€'
            }
        }
        
        template = fallback_follow_ups.get(interviewer_role, fallback_follow_ups['HR'])
        question_text = template['question']
        
        # ì´ë¦„ í˜¸ëª… ì¶”ê°€
        candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
        question_text = self._add_candidate_name_to_question(question_text, candidate_name)
        
        return {
            'question': question_text,
            'intent': template['intent'],
            'interviewer_type': interviewer_role,
            'question_flow_type': 'follow_up',
            'question_source': 'fallback_follow_up'
        }
    
    def _add_candidate_name_to_question(self, question: str, candidate_name: str) -> str:
        """ì§ˆë¬¸ì— ì§€ì›ì ì´ë¦„ í˜¸ëª… ì¶”ê°€"""
        if not candidate_name or candidate_name == 'ì§€ì›ì':
            return question
        
        # ì´ë¯¸ ì´ë¦„ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if candidate_name in question:
            return question
        
        # ì§ˆë¬¸ ëì— ì´ë¦„ ì¶”ê°€
        if question.endswith('.') or question.endswith('?'):
            return f"{candidate_name}ë‹˜, {question}"
        else:
            return f"{candidate_name}ë‹˜, {question}."