#!/usr/bin/env python3
"""
ì§€ëŠ¥í˜• í„´ì œ ë©´ì ‘ê´€ íŒ¨ë„ ì‹œìŠ¤í…œ

í•µì‹¬ íŠ¹ì§•:
- 3ëª…ì˜ ë©´ì ‘ê´€(HR, TECH, COLLABORATION)ì´ í„´ì œë¡œ ì§ˆë¬¸ ì§„í–‰
- ê° ë©´ì ‘ê´€ì€ ë©”ì¸ ì§ˆë¬¸ 1ê°œ + ë™ì  ê¼¬ë¦¬ ì§ˆë¬¸ 1~2ê°œë¡œ ì£¼ì œ ì‹¬ì¸µ íƒêµ¬
- 15ê°œ ì£¼ì œ í’€ì—ì„œ ë‹¤ì–‘í•œ ë©”ì¸ ì§ˆë¬¸ ì„ íƒ
- ì§€ì›ì ë‹µë³€ì„ ì‹¤ì‹œê°„ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„±
- DB ê¸°ë°˜ ì°¸ì¡°ì§ˆë¬¸ê³¼ LLM ê¸°ë°˜ ìƒì„±ì§ˆë¬¸ì˜ ì „ëµì  í˜¼í•©
"""

import json
import random
import os
import sys
from typing import Dict, List, Any, Optional
import openai
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from backend.services.supabase_client import get_supabase_client
from llm.shared.constants import GPT_MODEL, MAX_TOKENS, TEMPERATURE
from llm.candidate.model import CandidatePersona

class InterviewerService:
    """ì§€ëŠ¥í˜• í„´ì œ ê¸°ë°˜ ë©´ì ‘ê´€ íŒ¨ë„ ì‹œìŠ¤í…œ"""
    
    def __init__(self, total_question_limit: int = 15):
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = get_supabase_client()
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        self.openai_client = openai.OpenAI(api_key=api_key)
        
        # DB ë°ì´í„° ë¡œë”© ë° ìºì‹±
        self.companies_data = self._load_companies_data()
        self.fixed_questions = self._load_fixed_questions()
        
        # ë©´ì ‘ê´€ ì—­í•  ì •ì˜
        self.interviewer_roles = ['HR', 'TECH', 'COLLABORATION']
        self.question_type_mapping = {
            'HR': 1,
            'TECH': 2, 
            'COLLABORATION': 3
        }
        
        # í„´ì œ ì‹œìŠ¤í…œ ê´€ë¦¬ ë³€ìˆ˜
        self.total_question_limit = total_question_limit
        self.questions_asked_count = 0
        self.current_interviewer_index = 0
        self.interviewer_turn_state = {
            'HR': {'main_question_asked': False, 'follow_up_count': 0},
            'TECH': {'main_question_asked': False, 'follow_up_count': 0}, 
            'COLLABORATION': {'main_question_asked': False, 'follow_up_count': 0}
        }
        
        # ë©´ì ‘ê´€ë³„ ì£¼ì œ í’€ ì •ì˜
        self.topic_pools = {
            'HR': ['ì¸ì„±_ê°€ì¹˜ê´€', 'ì„±ì¥_ë™ê¸°', 'ê°ˆë“±_í•´ê²°', 'ìŠ¤íŠ¸ë ˆìŠ¤_ê´€ë¦¬', 'íŒ€ì›Œí¬_ë¦¬ë”ì‹­'],
            'TECH': ['ê¸°ìˆ _ì—­ëŸ‰', 'ë¬¸ì œ_í•´ê²°', 'ì„±ëŠ¥_ìµœì í™”', 'ì½”ë“œ_í’ˆì§ˆ', 'ìƒˆë¡œìš´_ê¸°ìˆ _í•™ìŠµ'],
            'COLLABORATION': ['ì†Œí†µ_ëŠ¥ë ¥', 'í”„ë¡œì íŠ¸_í˜‘ì—…', 'ì˜ê²¬_ì¡°ìœ¨', 'í¬ë¡œìŠ¤_íŒ€_í˜‘ì—…', 'ì¡°ì§_ë¬¸í™”_ì ì‘']
        }
    
    def _load_companies_data(self) -> Dict[str, Any]:
        """íšŒì‚¬ ë°ì´í„° ë¡œë”© ë° ìºì‹± (CompanyDataLoaderì™€ ë™ì¼í•œ ë§¤í•‘ ë°©ì‹ ì‚¬ìš©)"""
        try:
            result = self.client.table('company').select('*').execute()
            companies_dict = {}
            
            # CompanyDataLoaderì™€ ë™ì¼í•œ ë§¤í•‘ í…Œì´ë¸” ì‚¬ìš©
            company_id_mapping = {
                'ë„¤ì´ë²„': 'naver',
                'ì¹´ì¹´ì˜¤': 'kakao', 
                'ë¼ì¸': 'line',
                'ë¼ì¸í”ŒëŸ¬ìŠ¤': 'ë¼ì¸í”ŒëŸ¬ìŠ¤',  # Supabase DB í˜¸í™˜ì„±
                'ì¿ íŒ¡': 'coupang',
                'ë°°ë‹¬ì˜ë¯¼ì¡±': 'baemin',
                'ë‹¹ê·¼ë§ˆì¼“': 'daangn',
                'í† ìŠ¤': 'toss'
            }
            
            if result.data:
                for company in result.data:
                    # í•œê¸€ ì´ë¦„ì„ ì˜ë¬¸ IDë¡œ ë§¤í•‘
                    company_name = company.get('name', '')
                    english_id = company_id_mapping.get(company_name, company_name.lower())
                    companies_dict[english_id] = company
                    
            print(f"âœ… íšŒì‚¬ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(companies_dict)}ê°œ, í‚¤: {list(companies_dict.keys())}")
            return companies_dict
        except Exception as e:
            print(f"âŒ íšŒì‚¬ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def _load_fixed_questions(self) -> List[Dict[str, Any]]:
        """ê³ ì • ì§ˆë¬¸ ë°ì´í„° ë¡œë”© ë° ìºì‹±"""
        try:
            result = self.client.table('fix_question').select('*').execute()
            questions = result.data if result.data else []
            print(f"âœ… ê³ ì • ì§ˆë¬¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(questions)}ê°œ")
            return questions
        except Exception as e:
            print(f"âŒ ê³ ì • ì§ˆë¬¸ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return []
    
    def generate_next_question(self, user_resume: Dict, chun_sik_persona: CandidatePersona, 
                              company_id: str, previous_qa_pairs: List[Dict] = None,
                              user_answer: str = None, chun_sik_answer: str = None) -> Dict:
        """í„´ì œ ê¸°ë°˜ ë©´ì ‘ ì»¨íŠ¸ë¡¤ íƒ€ì›Œ - ì§ˆë¬¸ ìˆ˜ í•œë„ ê´€ë¦¬ ë° ë©´ì ‘ê´€ í„´ ì œì–´"""
        
        print(f"ğŸ¯ [InterviewerService] generate_next_question í˜¸ì¶œ: questions_asked_count={self.questions_asked_count}, total_limit={self.total_question_limit}")
        
        # ì§ˆë¬¸ ìˆ˜ í•œë„ í™•ì¸
        if self.questions_asked_count >= self.total_question_limit:
            print(f"ğŸ [InterviewerService] ì§ˆë¬¸ í•œë„ ë„ë‹¬, ë©´ì ‘ ì¢…ë£Œ: {self.questions_asked_count}/{self.total_question_limit}")
            return {
                'question': 'ë©´ì ‘ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.',
                'intent': 'ë©´ì ‘ ì¢…ë£Œ',
                'interviewer_type': 'SYSTEM',
                'is_final': True
            }
        
        # ì²« 2ê°œ ì§ˆë¬¸ì€ ê³ ì • (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if self.questions_asked_count == 0:
            self.questions_asked_count += 1
            print(f"ğŸ“ [InterviewerService] 1ë²ˆì§¸ ì§ˆë¬¸ ìƒì„±: ìê¸°ì†Œê°œ")
            # ìê¸°ì†Œê°œëŠ” ì´ë¦„ì„ ëª¨ë¥´ëŠ” ìƒí™©ì´ë¯€ë¡œ ì´ë¦„ í˜¸ëª… ì—†ì´ ì§„í–‰
            return {
                'question': 'ìê¸°ì†Œê°œë¥¼ ë¶€íƒë“œë¦½ë‹ˆë‹¤.',
                'intent': 'ì§€ì›ìì˜ ê¸°ë³¸ ì •ë³´ì™€ ì„±ê²©, ì—­ëŸ‰ì„ íŒŒì•…',
                'interviewer_type': 'HR'
            }
        
        elif self.questions_asked_count == 1:
            company_info = self.companies_data.get(company_id, {})
            company_name = company_info.get('name', 'ì €í¬ íšŒì‚¬')
            self.questions_asked_count += 1
            print(f"ğŸ“ [InterviewerService] 2ë²ˆì§¸ ì§ˆë¬¸ ìƒì„±: ì§€ì›ë™ê¸° ({company_name})")
            
            # ì§€ì›ë™ê¸° ì§ˆë¬¸ì— ì´ë¦„ í˜¸ëª… ì¶”ê°€ (ìê¸°ì†Œê°œ í›„ì´ë¯€ë¡œ ì´ë¦„ì„ ì•Œê³  ìˆìŒ)
            base_question = f'ì €í¬ {company_name}ì— ì§€ì›í•˜ì‹  ë™ê¸°ë¥¼ ë§ì”€í•´ ì£¼ì„¸ìš”.'
            candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
            question_with_name = self._add_candidate_name_to_question(base_question, candidate_name)
            
            return {
                'question': question_with_name,
                'intent': 'íšŒì‚¬ì— ëŒ€í•œ ê´€ì‹¬ë„ì™€ ì§€ì› ë™ê¸° íŒŒì•…',
                'interviewer_type': 'HR'
            }
        
        # í„´ì œ ì‹œìŠ¤í…œ ì‹œì‘ (question_index >= 2)
        else:
            print(f"ğŸ­ [InterviewerService] {self.questions_asked_count + 1}ë²ˆì§¸ ì§ˆë¬¸ ìƒì„± (í„´ì œ ì‹œìŠ¤í…œ)")
            company_info = self.companies_data.get(company_id, {})
            if not company_info:
                raise ValueError(f"íšŒì‚¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {company_id}")
            
            # í˜„ì¬ ë©´ì ‘ê´€ ê²°ì •
            current_interviewer = self._get_current_interviewer()
            print(f"ğŸ‘” [InterviewerService] í˜„ì¬ ë©´ì ‘ê´€: {current_interviewer}")
            
            # ë©´ì ‘ê´€ì˜ í„´ ìˆ˜í–‰
            question_result = self._conduct_interview_turn(
                user_resume, chun_sik_persona, company_info, current_interviewer,
                user_answer, chun_sik_answer, previous_qa_pairs
            )
            
            # ì§ˆë¬¸ ìˆ˜ ì¦ê°€
            self.questions_asked_count += 1
            print(f"ğŸ“ˆ [InterviewerService] ì§ˆë¬¸ ìˆ˜ ì¦ê°€: {self.questions_asked_count}/{self.total_question_limit}")
            
            # ë©´ì ‘ê´€ í„´ ìƒíƒœ ì—…ë°ì´íŠ¸
            self._update_turn_state(current_interviewer, question_result)
            
            return question_result
    
    def _get_current_interviewer(self) -> str:
        """í˜„ì¬ í„´ì„ ì§„í–‰í•  ë©´ì ‘ê´€ ê²°ì •"""
        return self.interviewer_roles[self.current_interviewer_index]
    
    def _update_turn_state(self, interviewer_role: str, question_result: Dict):
        """ë©´ì ‘ê´€ í„´ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ë‹¤ìŒ ë©´ì ‘ê´€ìœ¼ë¡œ ì „í™˜ ì—¬ë¶€ ê²°ì •"""
        turn_state = self.interviewer_turn_state[interviewer_role]
        
        # ê°•ì œ í„´ ì „í™˜ì¸ ê²½ìš° (ë¹ˆ ì§ˆë¬¸) ìƒíƒœ ì—…ë°ì´íŠ¸ ì—†ì´ ë°”ë¡œ ì „í™˜
        if question_result.get('force_turn_switch', False):
            self._switch_to_next_interviewer()
            return
        
        # ë©”ì¸ ì§ˆë¬¸ì´ì—ˆëŠ”ì§€ ê¼¬ë¦¬ ì§ˆë¬¸ì´ì—ˆëŠ”ì§€ í™•ì¸
        if question_result.get('question_type') == 'follow_up':
            turn_state['follow_up_count'] += 1
        else:
            # ë©”ì¸ ì§ˆë¬¸ì¸ ê²½ìš°
            turn_state['main_question_asked'] = True
        
        # í„´ ì „í™˜ ì¡°ê±´ í™•ì¸ (ë©”ì¸ ì§ˆë¬¸ + ìµœëŒ€ 2ê°œ ê¼¬ë¦¬ ì§ˆë¬¸ ë˜ëŠ” ë‚¨ì€ ì§ˆë¬¸ ìˆ˜ ë¶€ì¡±)
        remaining_questions = self.total_question_limit - self.questions_asked_count
        should_switch_turn = (
            turn_state['follow_up_count'] >= 2 or  # ìµœëŒ€ ê¼¬ë¦¬ ì§ˆë¬¸ ìˆ˜ ë„ë‹¬
            remaining_questions <= 3  # ë‚¨ì€ ì§ˆë¬¸ì´ ì ì–´ ë‹¤ë¥¸ ë©´ì ‘ê´€ì—ê²Œ ê¸°íšŒ ì œê³µ
        )
        
        if should_switch_turn:
            self._switch_to_next_interviewer()
    
    def _switch_to_next_interviewer(self):
        """ë‹¤ìŒ ë©´ì ‘ê´€ìœ¼ë¡œ í„´ ì „í™˜"""
        # í˜„ì¬ ë©´ì ‘ê´€ì˜ í„´ ìƒíƒœ ì´ˆê¸°í™”
        current_interviewer = self.interviewer_roles[self.current_interviewer_index]
        self.interviewer_turn_state[current_interviewer] = {
            'main_question_asked': False, 
            'follow_up_count': 0
        }
        
        # ë‹¤ìŒ ë©´ì ‘ê´€ìœ¼ë¡œ ì „í™˜
        self.current_interviewer_index = (self.current_interviewer_index + 1) % 3
    
    def _conduct_interview_turn(self, user_resume: Dict, chun_sik_persona: CandidatePersona,
                               company_info: Dict, interviewer_role: str, 
                               user_answer: str = None, chun_sik_answer: str = None,
                               previous_qa_pairs: List[Dict] = None) -> Dict:
        """ë©´ì ‘ê´€ì˜ í„´ ìˆ˜í–‰ - ë©”ì¸ ì§ˆë¬¸ ë˜ëŠ” ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„±"""
        
        turn_state = self.interviewer_turn_state[interviewer_role]
        remaining_budget = self.total_question_limit - self.questions_asked_count
        
        # ë©”ì¸ ì§ˆë¬¸ì´ ì•„ì§ ì•ˆ ë‚˜ì™”ë‹¤ë©´ ë©”ì¸ ì§ˆë¬¸ ìƒì„±
        if not turn_state['main_question_asked']:
            return self._generate_main_question(
                user_resume, chun_sik_persona, company_info, interviewer_role
            )
        
        # ë©”ì¸ ì§ˆë¬¸ì´ ë‚˜ì™”ë‹¤ë©´ ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì¡°ê±´ í™•ì¸
        else:
            # ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì¡°ê±´ ì²´í¬
            should_generate_follow_up = self._should_generate_follow_up_question(
                turn_state, remaining_budget, user_answer, chun_sik_answer
            )
            
            if should_generate_follow_up and user_answer and chun_sik_answer:
                # ì´ì „ ì§ˆë¬¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                previous_question = self._get_last_question_from_history(previous_qa_pairs)
                
                return self._generate_follow_up_question(
                    previous_question, user_answer, chun_sik_answer, 
                    company_info, interviewer_role, user_resume
                )
            else:
                # ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ì§€ ì•Šê³  í„´ ì¢…ë£Œ, ë‹¤ìŒ ë©´ì ‘ê´€ìœ¼ë¡œ ë„˜ê¹€
                return {
                    'question': '',
                    'intent': '',
                    'interviewer_type': interviewer_role,
                    'force_turn_switch': True
                }
    
    def _should_generate_follow_up_question(self, turn_state: Dict, remaining_budget: int,
                                          user_answer: str, chun_sik_answer: str) -> bool:
        """ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì—¬ë¶€ ê²°ì •"""
        
        # ê¸°ë³¸ ì¡°ê±´ ì²´í¬
        if turn_state['follow_up_count'] >= 2:  # ìµœëŒ€ ê¼¬ë¦¬ ì§ˆë¬¸ ìˆ˜ ë„ë‹¬
            return False
        
        if remaining_budget <= 3:  # ë‚¨ì€ ì§ˆë¬¸ ìˆ˜ê°€ ì ì–´ ë‹¤ë¥¸ ë©´ì ‘ê´€ì—ê²Œ ê¸°íšŒ ì œê³µ
            return False
        
        if not user_answer or not chun_sik_answer:  # ì´ì „ ë‹µë³€ì´ ì—†ìœ¼ë©´ ê¼¬ë¦¬ ì§ˆë¬¸ ë¶ˆê°€
            return False
        
        # ë™ì  ê²°ì • (ë‹µë³€ ê¸¸ì´ ê¸°ë°˜)
        answer_quality_score = len(user_answer.split()) + len(chun_sik_answer.split())
        
        # ë‹µë³€ì´ ì¶©ë¶„íˆ ê¸¸ê±°ë‚˜ ë‚´ìš©ì´ ìˆìœ¼ë©´ ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„±
        return answer_quality_score >= 20
    
    def _get_last_question_from_history(self, previous_qa_pairs: List[Dict]) -> str:
        """ì´ì „ ì§ˆë¬¸ ê¸°ë¡ì—ì„œ ë§ˆì§€ë§‰ ì§ˆë¬¸ ì¶”ì¶œ"""
        if not previous_qa_pairs:
            return "ì´ì „ ì§ˆë¬¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        last_qa = previous_qa_pairs[-1] if previous_qa_pairs else {}
        return last_qa.get('question', 'ì´ì „ ì§ˆë¬¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
    
    def _generate_main_question(self, user_resume: Dict, chun_sik_persona: CandidatePersona,
                               company_info: Dict, interviewer_role: str) -> Dict:
        """ë©”ì¸ ì§ˆë¬¸ ìƒì„± - ë‹¤ì–‘í•œ ì£¼ì œ í’€ì—ì„œ ì„ íƒí•˜ì—¬ ì°¸ì¡°/ìƒì„± í˜¼í•© (í´ë°± ë°©ì§€)"""
        
        # ë©´ì ‘ê´€ ì—­í• ì— ë§ëŠ” ì£¼ì œ ëª©ë¡ ì„ íƒ
        topic_pool = self.topic_pools.get(interviewer_role, [])
        if not topic_pool:
            print(f"âš ï¸ [InterviewerService] {interviewer_role} ì£¼ì œ í’€ì´ ë¹„ì–´ìˆìŒ. ì¼ë°˜ ì£¼ì œë¡œ ì‹œë„")
            topic_pool = ['ì¼ë°˜']
        
        # ëœë¤í•˜ê²Œ ì£¼ì œ ì„ íƒ
        selected_topic = random.choice(topic_pool)
        
        # 50% í™•ë¥ ë¡œ DB í…œí”Œë¦¿ ë˜ëŠ” LLM ìƒì„± ë°©ì‹ ì„ íƒ
        use_db_first = random.choice([True, False])
        
        question_result = None
        
        if use_db_first:
            print(f"ğŸ¯ [InterviewerService] DB í…œí”Œë¦¿ ìš°ì„  ì‹œë„: {selected_topic}")
            # 1ì°¨: DB í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„± ì‹œë„
            question_result = self._try_generate_from_db_template(
                user_resume, company_info, interviewer_role, selected_topic
            )
            
            if question_result:
                return question_result
            
            print(f"âŒ [InterviewerService] DB í…œí”Œë¦¿ ì‹¤íŒ¨. LLM ìƒì„±ìœ¼ë¡œ ì „í™˜")
            # 2ì°¨: DB ì‹¤íŒ¨ ì‹œ LLM ìƒì„± ì‹œë„ 
            question_result = self._try_generate_from_llm(
                user_resume, company_info, interviewer_role, selected_topic
            )
            
            if question_result:
                return question_result
                
        else:
            print(f"ğŸ¤– [InterviewerService] LLM ìƒì„± ìš°ì„  ì‹œë„: {selected_topic}")
            # 1ì°¨: LLM ê¸°ë°˜ ìƒì„± ì‹œë„
            question_result = self._try_generate_from_llm(
                user_resume, company_info, interviewer_role, selected_topic
            )
            
            if question_result:
                return question_result
            
            print(f"âŒ [InterviewerService] LLM ìƒì„± ì‹¤íŒ¨. DB í…œí”Œë¦¿ìœ¼ë¡œ ì „í™˜")
            # 2ì°¨: LLM ì‹¤íŒ¨ ì‹œ DB í…œí”Œë¦¿ ì‹œë„
            question_result = self._try_generate_from_db_template(
                user_resume, company_info, interviewer_role, selected_topic
            )
            
            if question_result:
                return question_result
        
        # ìµœì¢… í´ë°±: ë‘˜ ë‹¤ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ì ì¸ ì§ˆë¬¸ (ì¥ì /ë‹¨ì  ì•„ë‹˜)
        print(f"ğŸš¨ [InterviewerService] ëª¨ë“  ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨. ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ í´ë°±")
        candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
        return self._get_generic_question(interviewer_role, selected_topic, candidate_name)
    
    def _try_generate_from_db_template(self, user_resume: Dict, company_info: Dict, 
                                     interviewer_role: str, topic: str) -> Optional[Dict]:
        """DB í…œí”Œë¦¿ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ì‹œë„ (ì‹¤íŒ¨ ì‹œ None ë°˜í™˜)"""
        try:
            return self._generate_from_db_template_with_topic(
                user_resume, company_info, interviewer_role, topic
            )
        except Exception as e:
            print(f"âŒ [InterviewerService] DB í…œí”Œë¦¿ ìƒì„± ì¤‘ ì˜ˆì™¸: {e}")
            return None
    
    def _try_generate_from_llm(self, user_resume: Dict, company_info: Dict, 
                             interviewer_role: str, topic: str) -> Optional[Dict]:
        """LLM ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ì‹œë„ (ì‹¤íŒ¨ ì‹œ None ë°˜í™˜)"""
        try:
            return self._generate_from_llm_with_topic(
                user_resume, company_info, interviewer_role, topic
            )
        except Exception as e:
            print(f"âŒ [InterviewerService] LLM ìƒì„± ì¤‘ ì˜ˆì™¸: {e}")
            return None
    
    def _get_generic_question(self, interviewer_role: str, topic: str, candidate_name: str = None) -> Dict:
        """ìµœì¢… í´ë°±: ì¼ë°˜ì ì¸ ì§ˆë¬¸ (ì¥ì /ë‹¨ì  ì•„ë‹˜)"""
        generic_questions = {
            'HR': {
                'question': f'{topic} ê´€ë ¨í•´ì„œ ë³¸ì¸ì˜ ê²½í—˜ì„ ììœ ë¡­ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”.',
                'intent': 'ì§€ì›ìì˜ ê²½í—˜ê³¼ ì—­ëŸ‰ íŒŒì•…'
            },
            'TECH': {
                'question': f'{topic} ë¶„ì•¼ì—ì„œ ë³¸ì¸ì´ í•´ê²°í•œ ë¬¸ì œë‚˜ ê²½í—˜ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.',
                'intent': 'ê¸°ìˆ ì  ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ í‰ê°€'
            },
            'COLLABORATION': {
                'question': f'{topic}ê³¼ ê´€ë ¨ëœ íŒ€ í˜‘ì—… ê²½í—˜ì„ ë§ì”€í•´ ì£¼ì„¸ìš”.',
                'intent': 'í˜‘ì—… ëŠ¥ë ¥ê³¼ ì†Œí†µ ì—­ëŸ‰ í‰ê°€'
            }
        }
        
        fallback = generic_questions.get(interviewer_role, generic_questions['HR'])
        
        # ì´ë¦„ í˜¸ëª… ì¶”ê°€
        question_with_name = self._add_candidate_name_to_question(
            fallback['question'], candidate_name
        )
        
        return {
            'question': question_with_name,
            'intent': fallback['intent'],
            'interviewer_type': interviewer_role,
            'topic': topic,
            'question_source': 'generic_fallback'
        }
    
    def _generate_from_db_template_with_topic(self, user_resume: Dict, company_info: Dict, 
                                            interviewer_role: str, topic: str) -> Dict:
        """ì£¼ì œ íŠ¹í™” DB í…œí”Œë¦¿ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±"""
        
        # ë©´ì ‘ê´€ ì—­í• ì— í•´ë‹¹í•˜ëŠ” question_type ID ê°€ì ¸ì˜¤ê¸°
        question_type_id = self.question_type_mapping.get(interviewer_role, 1)
        
        # í•´ë‹¹ íƒ€ì…ì˜ ì§ˆë¬¸ í…œí”Œë¦¿ í•„í„°ë§
        filtered_questions = [
            q for q in self.fixed_questions 
            if q.get('question_type') == question_type_id
        ]
        
        if not filtered_questions:
            raise ValueError(f"DBì— {interviewer_role} ì—­í• (question_type={question_type_id})ì˜ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëœë¤ í…œí”Œë¦¿ ì„ íƒ
        selected_template = random.choice(filtered_questions)
        
        # í…œí”Œë¦¿ì— ë°ì´í„° ì£¼ì…
        question_content = self._inject_data_to_template(
            selected_template.get('question_content', ''),
            user_resume, 
            company_info
        )
        
        # ì´ë¦„ í˜¸ëª… ì¶”ê°€
        candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
        question_with_name = self._add_candidate_name_to_question(question_content, candidate_name)
        
        return {
            'question': question_with_name,
            'intent': f"{topic} ê´€ë ¨ {selected_template.get('question_intent', f'{interviewer_role} ì—­ëŸ‰ í‰ê°€')}",
            'interviewer_type': interviewer_role,
            'topic': topic,
            'question_source': 'db_template'
        }
    
    def _generate_from_llm_with_topic(self, user_resume: Dict, company_info: Dict, 
                                     interviewer_role: str, topic: str) -> Dict:
        """ì£¼ì œ íŠ¹í™” LLM ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±"""
        
        # ì£¼ì œë³„ í”„ë¡¬í”„íŠ¸ ë¹Œë” í˜¸ì¶œ
        prompt = self._build_topic_specific_prompt(
            user_resume, company_info, interviewer_role, topic
        )
        
        # LLM í˜¸ì¶œ
        try:
            response = self.openai_client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": """ë‹¹ì‹ ì€ ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. 

ğŸš¨ **ì ˆëŒ€ ì¤€ìˆ˜ ì‚¬í•­** ğŸš¨
- ì˜¤ì§ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”
- ë‹¤ë¥¸ ì–´ë–¤ í…ìŠ¤íŠ¸, ì„¤ëª…, ì£¼ì„ë„ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- JSON ì•ë’¤ì— ```jsonì´ë‚˜ ê¸°íƒ€ í…ìŠ¤íŠ¸ ê¸ˆì§€

**í•„ìˆ˜ ì‘ë‹µ í˜•ì‹:**
{"question": "ì§ˆë¬¸ ë‚´ìš©", "intent": "ì§ˆë¬¸ ì˜ë„"}

**ì˜ˆì‹œ:**
{"question": "í”„ë¡œì íŠ¸ì—ì„œ ê°€ì¥ ì–´ë ¤ì› ë˜ ê¸°ìˆ ì  ë„ì „ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?", "intent": "ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ê³¼ ê¸°ìˆ ì  ì—­ëŸ‰ í‰ê°€"}

ìœ„ í˜•ì‹ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ë‹¤ë¥¸ í˜•íƒœì˜ ì‘ë‹µì€ ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤."""},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE
            )
            
            # JSON íŒŒì‹±
            result_text = response.choices[0].message.content.strip()
            
            if not result_text:
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
            
            result = json.loads(result_text)
            
            # ê²°ê³¼ ê²€ì¦ ë° ë³´ì •
            if not result.get('question'):
                raise ValueError("question í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # ì´ë¦„ í˜¸ëª… ì¶”ê°€
            candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
            result['question'] = self._add_candidate_name_to_question(result['question'], candidate_name)
            
            result['interviewer_type'] = interviewer_role
            result['topic'] = topic
            result['question_source'] = 'llm_generated'
            return result
            
        except Exception as e:
            print(f"âŒ LLM ë©”ì¸ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise  # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ìƒìœ„ í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
    
    def _build_topic_specific_prompt(self, user_resume: Dict, company_info: Dict, 
                                   interviewer_role: str, topic: str) -> str:
        """ì „ì‚¬ì  DNA ë§¤íŠ¸ë¦­ìŠ¤ í”„ë¡¬í”„íŠ¸ ë¹Œë” - íšŒì‚¬ì˜ ëª¨ë“  íŠ¹ì„±ê³¼ ì§ë¬´ ì „ë¬¸ì„±ì„ ìœµí•©í•œ ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„±"""
        
        # 1. [íšŒì‚¬ DNA] ì •ë³´ ì²´ê³„ì  ì¶”ì¶œ
        company_name = company_info.get('name', 'íšŒì‚¬')
        talent_profile = company_info.get('talent_profile', 'ì •ì˜ë˜ì§€ ì•ŠìŒ')
        core_competencies = ', '.join(company_info.get('core_competencies', ['ì •ì˜ë˜ì§€ ì•ŠìŒ']))
        tech_focus = ', '.join(company_info.get('tech_focus', ['ì •ì˜ë˜ì§€ ì•ŠìŒ']))
        question_direction = company_info.get('question_direction', 'ì •ì˜ë˜ì§€ ì•ŠìŒ')
        
        # ê¸°ì—…ë¬¸í™” ì •ë³´ ì¶”ì¶œ
        company_culture = company_info.get('company_culture', {})
        core_values = []
        work_style = ''
        if isinstance(company_culture, dict):
            core_values = company_culture.get('core_values', [])
            work_style = company_culture.get('work_style', '')
        
        # 2. [ì§€ì›ì ì§ë¬´ ì»¨í…ìŠ¤íŠ¸] ì •ì˜
        position = user_resume.get('position', 'ê°œë°œì')
        position_contexts = {
            "ë°±ì—”ë“œ": "ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬, ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„, MSA ì•„í‚¤í…ì²˜, API ì„±ëŠ¥ ìµœì í™”, ì‹œìŠ¤í…œ ì•ˆì •ì„±, ë¶„ì‚° íŠ¸ëœì­ì…˜",
            "í”„ë¡ íŠ¸ì—”ë“œ": "ì›¹ ì„±ëŠ¥ ìµœì í™”(ë¡œë”© ì†ë„), ìƒíƒœ ê´€ë¦¬, UI/UX ê°œì„ , í¬ë¡œìŠ¤ ë¸Œë¼ìš°ì§•, ì›¹ ì ‘ê·¼ì„±, Critical Rendering Path",
            "AI": "ëª¨ë¸ ì„±ëŠ¥ ë° ì •í™•ë„, ë°ì´í„° ì „ì²˜ë¦¬, ê³¼ì í•© ë°©ì§€, ìµœì‹  ë…¼ë¬¸ êµ¬í˜„, MLOps, Transformer ì•„í‚¤í…ì²˜",
            "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤": "A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„, í†µê³„ì  ê°€ì„¤ ê²€ì¦, Feature Engineering, ë°ì´í„° ì‹œê°í™”, ì˜ˆì¸¡ ëª¨ë¸ë§, ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬",
            "ê¸°íš": "ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ë¶„ì„, ì œí’ˆ ë¡œë“œë§µ ì„¤ì •, KPI ì •ì˜, ì‹œì¥ ì¡°ì‚¬, ê¸°ëŠ¥ ìš°ì„ ìˆœìœ„ ê²°ì •, RICE/ICE í”„ë ˆì„ì›Œí¬"
        }
        
        # ì§êµ° ë§¤ì¹­ (ë¶€ë¶„ ë¬¸ìì—´ í¬í•¨ ê²€ì‚¬)
        position_context = "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ì¼ë°˜"
        for key, context in position_contexts.items():
            if key in position or key.lower() in position.lower():
                position_context = context
                break
        
        # ì£¼ì œë³„ ê¸°ë³¸ ê°€ì´ë“œë¼ì¸
        topic_guidelines = {
            'ì¸ì„±_ê°€ì¹˜ê´€': 'ì§€ì›ìì˜ í•µì‹¬ ê°€ì¹˜ê´€ê³¼ ì¸ìƒ ì² í•™ì„ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸',
            'ì„±ì¥_ë™ê¸°': 'í•™ìŠµ ì˜ì§€ì™€ ìê¸°ê³„ë°œì— ëŒ€í•œ íƒœë„ë¥¼ í™•ì¸í•˜ëŠ” ì§ˆë¬¸',
            'ê°ˆë“±_í•´ê²°': 'ëŒ€ì¸ê´€ê³„ë‚˜ ì—…ë¬´ìƒ ê°ˆë“± ìƒí™©ì—ì„œì˜ í•´ê²° ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ì§ˆë¬¸',
            'ìŠ¤íŠ¸ë ˆìŠ¤_ê´€ë¦¬': 'ì••ë°• ìƒí™©ì—ì„œì˜ ëŒ€ì²˜ ëŠ¥ë ¥ê³¼ íšŒë³µë ¥ì„ ì¸¡ì •í•˜ëŠ” ì§ˆë¬¸',
            'íŒ€ì›Œí¬_ë¦¬ë”ì‹­': 'íŒ€ ë‚´ì—ì„œì˜ ì—­í• ê³¼ ë¦¬ë”ì‹­ ì—­ëŸ‰ì„ í™•ì¸í•˜ëŠ” ì§ˆë¬¸',
            'ê¸°ìˆ _ì—­ëŸ‰': 'ì „ë¬¸ ê¸°ìˆ  ì§€ì‹ê³¼ ì‹¤ë¬´ ì ìš© ëŠ¥ë ¥ì„ ê²€ì¦í•˜ëŠ” ì§ˆë¬¸',
            'ë¬¸ì œ_í•´ê²°': 'ë³µì¡í•œ ê¸°ìˆ ì  ë¬¸ì œì— ëŒ€í•œ ì ‘ê·¼ ë°©ì‹ê³¼ í•´ê²° ê³¼ì •ì„ í‰ê°€í•˜ëŠ” ì§ˆë¬¸',
            'ì„±ëŠ¥_ìµœì í™”': 'ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„ ê³¼ ìµœì í™” ê²½í—˜ì„ í™•ì¸í•˜ëŠ” ì§ˆë¬¸',
            'ì½”ë“œ_í’ˆì§ˆ': 'ì½”ë“œ ë¦¬ë·°, í…ŒìŠ¤íŠ¸, ë¬¸ì„œí™” ë“± í’ˆì§ˆ ê´€ë¦¬ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ì§ˆë¬¸',
            'ìƒˆë¡œìš´_ê¸°ìˆ _í•™ìŠµ': 'ê¸°ìˆ  íŠ¸ë Œë“œ íŒŒì•…ê³¼ ìƒˆë¡œìš´ ê¸°ìˆ  ìŠµë“ ëŠ¥ë ¥ì„ í™•ì¸í•˜ëŠ” ì§ˆë¬¸',
            'ì†Œí†µ_ëŠ¥ë ¥': 'ì˜ì‚¬ì†Œí†µê³¼ ì •ë³´ ì „ë‹¬ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ì§ˆë¬¸',
            'í”„ë¡œì íŠ¸_í˜‘ì—…': 'ë‹¤ì–‘í•œ ì—­í• ì˜ íŒ€ì›ë“¤ê³¼ì˜ í˜‘ì—… ê²½í—˜ì„ í™•ì¸í•˜ëŠ” ì§ˆë¬¸',
            'ì˜ê²¬_ì¡°ìœ¨': 'ì„œë¡œ ë‹¤ë¥¸ ì˜ê²¬ì„ ì¡°ìœ¨í•˜ê³  í•©ì˜ì ì„ ì°¾ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ì§ˆë¬¸',
            'í¬ë¡œìŠ¤_íŒ€_í˜‘ì—…': 'ë¶€ì„œê°„ í˜‘ì—…ê³¼ ì´í•´ê´€ê³„ì ê´€ë¦¬ ëŠ¥ë ¥ì„ í™•ì¸í•˜ëŠ” ì§ˆë¬¸',
            'ì¡°ì§_ë¬¸í™”_ì ì‘': 'ìƒˆë¡œìš´ í™˜ê²½ì— ëŒ€í•œ ì ì‘ë ¥ê³¼ ì¡°ì§ ë¬¸í™” ì´í•´ë„ë¥¼ í‰ê°€í•˜ëŠ” ì§ˆë¬¸'
        }
        
        # 3. ì•ˆì „ì¥ì¹˜: í•„ìˆ˜ DNA ì •ë³´ ë¶€ì¡± ì‹œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        critical_dna_missing = (
            talent_profile == 'ì •ì˜ë˜ì§€ ì•ŠìŒ' and 
            core_competencies == 'ì •ì˜ë˜ì§€ ì•ŠìŒ' and 
            tech_focus == 'ì •ì˜ë˜ì§€ ì•ŠìŒ' and 
            question_direction == 'ì •ì˜ë˜ì§€ ì•ŠìŒ'
        )
        
        if critical_dna_missing:
            # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°ë¡œ í´ë°±
            guideline = topic_guidelines.get(topic, f'{topic} ê´€ë ¨ ì „ë¬¸ì„±ì„ í‰ê°€í•˜ëŠ” ì§ˆë¬¸')
            return f"""
ë‹¹ì‹ ì€ {company_name}ì˜ {interviewer_role} ë‹´ë‹¹ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.

ë©´ì ‘ ì§êµ°: {position}
ë©´ì ‘ ì£¼ì œ: {topic}
ì§ˆë¬¸ ê°€ì´ë“œë¼ì¸: {guideline}

ìœ„ ì£¼ì œì— ëŒ€í•´ ë‘ ì§€ì›ìë¥¼ ë™ì‹œì— í‰ê°€í•  ìˆ˜ ìˆëŠ” ë©”ì¸ ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ ìƒì„±í•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
{{"question": "ì§ˆë¬¸ ë‚´ìš©", "intent": "ì§ˆë¬¸ ì˜ë„"}}
"""
        
        # 4. [ì „ì‚¬ì  DNA ë§¤íŠ¸ë¦­ìŠ¤ í”„ë¡¬í”„íŠ¸] êµ¬ì„±
        prompt = f"""
### ë‹¹ì‹ ì˜ ë¯¸ì…˜ ###
ë‹¹ì‹ ì€ '{company_name}'ì˜ ì±„ìš© ì² í•™ì„ ì™„ë²½í•˜ê²Œ ì´í•´í•œ ìµœê³  ìˆ˜ì¤€ì˜ {interviewer_role} ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
ì•„ë˜ì— ì£¼ì–´ì§„ [íšŒì‚¬ DNA]ì™€ [ì§€ì›ì ì»¨í…ìŠ¤íŠ¸]ë¥¼ 'ëª¨ë‘' ìœ ê¸°ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬, ì§€ì›ìì˜ ì—­ëŸ‰ì„ ë‹¤ê°ë„ë¡œ ê²€ì¦í•  ìˆ˜ ìˆëŠ” ë§¤ìš° ë‚ ì¹´ë¡­ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ 'ë‹¨ í•˜ë‚˜ë§Œ' ìƒì„±í•˜ì„¸ìš”.

### [íšŒì‚¬ DNA ë¶„ì„] ###
- **ì¸ì¬ìƒ (WHO):** ìš°ë¦¬ëŠ” '{talent_profile}'ì¸ ì‚¬ëŒì„ ì›í•©ë‹ˆë‹¤.
- **í•µì‹¬ ì—­ëŸ‰ (WHAT):** ìš°ë¦¬ëŠ” '{core_competencies}' ì—­ëŸ‰ì„ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.
- **ê¸°ìˆ  ì¤‘ì  ë¶„ì•¼ (WHERE):** ìš°ë¦¬ì˜ ê¸°ìˆ ì€ '{tech_focus}' ë¶„ì•¼ì— ì§‘ì¤‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- **í‰ê°€ ë°©í–¥ (HOW):** ìš°ë¦¬ëŠ” '{question_direction}' ë°©ì‹ìœ¼ë¡œ ì§€ì›ìë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
{f"- **í•µì‹¬ê°€ì¹˜:** {', '.join(core_values[:3])}" if core_values else ""}
{f"- **ì—…ë¬´ë¬¸í™”:** {work_style}" if work_style else ""}

### [ì§€ì›ì ì»¨í…ìŠ¤íŠ¸] ###
- **ì§êµ°:** {position}
- **ì£¼ìš” ì—…ë¬´ ì˜ì—­:** {position_context}
- **ë©´ì ‘ ì£¼ì œ:** {topic_guidelines.get(topic, f'{topic} ê´€ë ¨ ì „ë¬¸ì„±ì„ í‰ê°€í•˜ëŠ” ì§ˆë¬¸')}

### [ì§ˆë¬¸ ìƒì„± ì‚¬ê³  í”„ë¡œì„¸ìŠ¤] (ë°˜ë“œì‹œ ì´ ìˆœì„œëŒ€ë¡œ ìƒê°í•˜ê³  ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”) ###
1. **DNA ìœµí•©:** ìš°ë¦¬ íšŒì‚¬ì˜ **ì¸ì¬ìƒ(WHO)**ê³¼ **í•µì‹¬ ì—­ëŸ‰(WHAT)**ì„ ê³ ë ¤í–ˆì„ ë•Œ, {position} ì§ë¬´ì—ì„œëŠ” ì–´ë–¤ í–‰ë™ì´ë‚˜ ê¸°ìˆ ì  ê²°ì •ì´ ê°€ì¥ ì¤‘ìš”í• ì§€ ì •ì˜í•˜ì„¸ìš”.

2. **ìƒí™© ì„¤ì •:** ìœ„ì—ì„œ ì •ì˜í•œ í–‰ë™/ê²°ì •ì´ í•„ìš”í•œ êµ¬ì²´ì ì¸ ë¬¸ì œ ìƒí™©ì„ ìš°ë¦¬ íšŒì‚¬ì˜ **ê¸°ìˆ  ì¤‘ì  ë¶„ì•¼(WHERE)** ë‚´ì—ì„œ ë§¤ìš° í˜„ì‹¤ì ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.

3. **ì§ˆë¬¸ ê³µì‹í™”:** ì„¤ì •ëœ ìƒí™© ì†ì—ì„œ, ì§€ì›ìê°€ ì–´ë–»ê²Œ ë¬¸ì œë¥¼ í•´ê²°í–ˆëŠ”ì§€ ê·¸ ê²½í—˜ì„ ë¬»ëŠ” ìµœì¢… ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”. ì´ ì§ˆë¬¸ì€ ë°˜ë“œì‹œ ìš°ë¦¬ íšŒì‚¬ì˜ **í‰ê°€ ë°©í–¥(HOW)**ì— ë¶€í•©í•´ì•¼ í•˜ë©°, ì§€ì›ìì˜ ê¸°ìˆ ì  ê¹Šì´ì™€ ìš°ë¦¬ íšŒì‚¬ì™€ì˜ ë¬¸í™”ì  ì í•©ì„±ì„ ë™ì‹œì— íŒŒì•…í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

### [ì§ˆë¬¸ ìƒì„±ì˜ ì˜ˆì‹œ] ###
**ë§Œì•½ íšŒì‚¬ê°€ 'ë„¤ì´ë²„', ì¸ì¬ìƒì´ 'ê¸°ìˆ ë¡œ ëª¨ë“  ê²ƒì„ ì—°ê²°í•˜ëŠ” í”Œë«í¼ ë¹Œë”', ê¸°ìˆ  ì¤‘ì ì´ 'AI', ì§êµ°ì´ 'ë°±ì—”ë“œ'ë¼ë©´:**
- **ì‚¬ê³  ê³¼ì •:** 'ê¸°ìˆ ë¡œ ëª¨ë“  ê²ƒì„ ì—°ê²°'í•˜ëŠ” 'ë°±ì—”ë“œ' ê°œë°œìëŠ” ê¸°ì¡´ ì‹œìŠ¤í…œì˜ í•œê³„ë¥¼ ë„˜ì–´ì„œëŠ” ê²ƒì„ ë‘ë ¤ì›Œí•˜ì§€ ì•Šì•„ì•¼ í•œë‹¤. ë„¤ì´ë²„ì˜ 'AI' ê¸°ìˆ  ì¤‘ì  ë¶„ì•¼ì—ì„œ, ëŒ€ê·œëª¨ AI ëª¨ë¸ì˜ ë°ì´í„° ì„œë¹™ íŒŒì´í”„ë¼ì¸ì€ í•­ìƒ ë„ì „ì ì¸ ê³¼ì œì´ë‹¤.
- **ì¢‹ì€ ì§ˆë¬¸:** "ë„¤ì´ë²„ì˜ í•µì‹¬ ê°€ì¹˜ ì¤‘ í•˜ë‚˜ëŠ” 'ê¸°ìˆ ë¡œ ëª¨ë“  ê²ƒì„ ì—°ê²°'í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê³¼ê±° ëŒ€ê·œëª¨ AI ëª¨ë¸ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê³µê¸‰í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì—ì„œ ë³‘ëª© í˜„ìƒì„ ê²ªê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê¸°ì¡´ ì‹œìŠ¤í…œì˜ êµ¬ì¡°ì— 'ë„ì „'í•˜ì—¬ ìƒˆë¡­ê²Œ ê°œì„ í–ˆë˜ ê²½í—˜ì´ ìˆë‹¤ë©´ ë§ì”€í•´ì£¼ì„¸ìš”. ì–´ë–¤ ê¸°ìˆ ì  ê·¼ê±°ë¡œ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¥¼ ì œì•ˆí–ˆê³ , ê·¸ ê²°ê³¼ëŠ” ì–´ë• ë‚˜ìš”?"

### [ìµœì¢… ì¶œë ¥] ###
ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ì—†ì´, ì•„ë˜ JSON í˜•ì‹ì— ë§ì¶°ì„œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
{{
  "question": "ìƒì„±ëœ ìµœì¢… ì§ˆë¬¸ ë‚´ìš©",
  "intent": "ì´ ì§ˆë¬¸ì„ í†µí•´ í‰ê°€í•˜ë ¤ëŠ” ì—­ëŸ‰ (ì˜ˆ: ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ëŠ¥ë ¥ê³¼ ê³ ê° ì¤‘ì‹¬ ì‚¬ê³ ì˜ ê²°í•©)",
  "related_dna": ["í‰ê°€ì™€ ê´€ë ¨ëœ íšŒì‚¬ DNA í‚¤ì›Œë“œ (ì˜ˆ: 'ê³ ê° ì¤‘ì‹¬', 'ìµœê³ ì˜ ê¸°ìˆ ë ¥')"]
}}
"""
        return prompt
    
    def _generate_follow_up_question(self, previous_question: str, user_answer: str, 
                                   chun_sik_answer: str, company_info: Dict, 
                                   interviewer_role: str, user_resume: Dict = None) -> Dict:
        """ë™ì  ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± - ë‹µë³€ ê¸°ë°˜ ì‹¤ì‹œê°„ ì‹¬ì¸µ íƒêµ¬"""
        
        company_name = company_info.get('name', 'íšŒì‚¬')
        
        # ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± í”„ë¡¬í”„íŠ¸
        prompt = f"""
ë‹¹ì‹ ì€ {company_name}ì˜ {interviewer_role} ë‹´ë‹¹ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.

ì´ì „ ì§ˆë¬¸: {previous_question}

ì‚¬ìš©ì ë‹µë³€: {user_answer}

ì¶˜ì‹ì´ ë‹µë³€: {chun_sik_answer}

ìœ„ ìƒí™©ì„ ë¶„ì„í•˜ì—¬, ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ëª©í‘œë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ë‚ ì¹´ë¡œìš´ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ì¦‰ì„ì—ì„œ ë‹¨ í•˜ë‚˜ë§Œ ìƒì„±í•˜ì„¸ìš”:

ëª©í‘œ 1 (ì‹¬ì¸µ ê²€ì¦): í•œ ì§€ì›ìì˜ ë‹µë³€ì—ì„œ ë” ê¹Šê²Œ íŒŒê³ ë“¤ ë§Œí•œ ë¶€ë¶„ì„ ì°¾ì•„ êµ¬ì²´ì ì¸ ì¶”ê°€ ì„¤ëª…ì„ ìš”êµ¬í•˜ì„¸ìš”.
ëª©í‘œ 2 (ë¹„êµ ë¶„ì„): ë‘ ë‹µë³€ì˜ ì°¨ì´ì ì„ ì •í™•íˆ ì§šì–´ë‚´ê³ , ê°ìì˜ ì„ íƒì— ëŒ€í•œ ì´ìœ ë‚˜ ìƒê°ì„ í† ë¡ í•˜ë„ë¡ ìœ ë„í•˜ì„¸ìš”.  
ëª©í‘œ 3 (ìˆ˜ì¤€ ì¡°ì ˆ): í•œ ì§€ì›ìê°€ ë‹µë³€ì„ ì˜ ëª»í–ˆë‹¤ë©´, ê·¸ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ë” ì‰¬ìš´ ê°œë… ì§ˆë¬¸ìœ¼ë¡œ ì „í™˜í•˜ì„¸ìš”.

ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ê°€ì´ë“œë¼ì¸:
- ì´ì „ ë‹µë³€ì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì–¸ê¸‰í•˜ë©° ì§ˆë¬¸í•˜ì„¸ìš”
- "ë°©ê¸ˆ ë§ì”€í•˜ì‹  ~ì— ëŒ€í•´", "~ë¼ê³  í•˜ì…¨ëŠ”ë°" ë“±ì˜ í‘œí˜„ í™œìš©
- ë‹¨ìˆœí•œ Yes/No ì§ˆë¬¸ë³´ë‹¤ëŠ” êµ¬ì²´ì ì¸ ì„¤ëª…ì„ ìš”êµ¬í•˜ëŠ” ì—´ë¦° ì§ˆë¬¸
- ë‘ ì§€ì›ì ëª¨ë‘ì—ê²Œ ê³µì •í•˜ê²Œ ë‹µë³€ ê¸°íšŒ ì œê³µ
- ë©´ì ‘ê´€ì˜ ì „ë¬¸ì„±ì´ ë“œëŸ¬ë‚˜ëŠ” ë‚ ì¹´ë¡œìš´ ê´€ì 

ì‘ë‹µ í˜•ì‹:
{{"question": "ì§ˆë¬¸ ë‚´ìš©", "intent": "ì§ˆë¬¸ ì˜ë„"}}
"""
        
        # LLM í˜¸ì¶œ
        try:
            response = self.openai_client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": """ë‹¹ì‹ ì€ ê²½í—˜ ë§ì€ ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìë“¤ì˜ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ì„ íŒŒê³ ë“œëŠ” ë‚ ì¹´ë¡œìš´ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

ğŸš¨ **ì ˆëŒ€ ì¤€ìˆ˜ ì‚¬í•­** ğŸš¨
- ì˜¤ì§ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”
- ë‹¤ë¥¸ ì–´ë–¤ í…ìŠ¤íŠ¸, ì„¤ëª…, ì£¼ì„ë„ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- JSON ì•ë’¤ì— ```jsonì´ë‚˜ ê¸°íƒ€ í…ìŠ¤íŠ¸ ê¸ˆì§€

**í•„ìˆ˜ ì‘ë‹µ í˜•ì‹:**
{"question": "ì§ˆë¬¸ ë‚´ìš©", "intent": "ì§ˆë¬¸ ì˜ë„"}

**ì˜ˆì‹œ:**
{"question": "ë°©ê¸ˆ ë§ì”€í•˜ì‹  ì„±ëŠ¥ ìµœì í™” ë°©ë²•ì—ì„œ ê°€ì¥ íš¨ê³¼ì ì´ì—ˆë˜ ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€ìš”?", "intent": "êµ¬ì²´ì ì¸ ê¸°ìˆ ì  ì„±ê³¼ì™€ íŒë‹¨ ê·¼ê±° í™•ì¸"}

ìœ„ í˜•ì‹ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ë‹¤ë¥¸ í˜•íƒœì˜ ì‘ë‹µì€ ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤."""},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=MAX_TOKENS,
                temperature=0.7  # ì°½ì˜ì ì¸ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìœ„í•´ ì¡°ê¸ˆ ë†’ì„
            )
            
            # JSON íŒŒì‹± ê°œì„  (ê¼¬ë¦¬ ì§ˆë¬¸ìš©)
            result_text = response.choices[0].message.content.strip()
            
            if not result_text:
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")
            
            # JSON ë¸”ë¡ ì¶”ì¶œ (```json ë¸”ë¡ ë˜ëŠ” { } ë¸”ë¡)
            if '```json' in result_text:
                json_start = result_text.find('```json') + 7
                json_end = result_text.find('```', json_start)
                result_text = result_text[json_start:json_end].strip()
            elif '{' in result_text and '}' in result_text:
                json_start = result_text.find('{')
                json_end = result_text.rfind('}') + 1
                result_text = result_text[json_start:json_end]
            
            result = json.loads(result_text)
            
            # ê²°ê³¼ ê²€ì¦ ë° ë³´ì •
            if not result.get('question'):
                raise ValueError("question í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # ì´ë¦„ í˜¸ëª… ì¶”ê°€
            candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
            result['question'] = self._add_candidate_name_to_question(result['question'], candidate_name)
            
            result['interviewer_type'] = interviewer_role
            result['question_type'] = 'follow_up'
            result['question_source'] = 'llm_follow_up'
            return result
            
        except Exception as e:
            print(f"âŒ ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ê¼¬ë¦¬ ì§ˆë¬¸
            return self._get_fallback_follow_up_question(interviewer_role, previous_question, user_resume)
    
    def _get_fallback_follow_up_question(self, interviewer_role: str, previous_question: str, user_resume: Dict = None) -> Dict:
        """ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ ì‹œ í´ë°± ì§ˆë¬¸"""
        
        fallback_follow_ups = {
            'HR': {
                'question': 'ë°©ê¸ˆ ë§ì”€í•´ì£¼ì‹  ë‚´ìš©ì—ì„œ ê°€ì¥ ì–´ë ¤ì› ë˜ ì ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?',
                'intent': 'ì–´ë ¤ì›€ ê·¹ë³µ ê³¼ì •ê³¼ ì„±ì¥ ê²½í—˜ í™•ì¸'
            },
            'TECH': {
                'question': 'í•´ë‹¹ ê¸°ìˆ ì´ë‚˜ ë°©ë²•ì„ ì„ íƒí•œ êµ¬ì²´ì ì¸ ì´ìœ ê°€ ìˆë‹¤ë©´ ì„¤ëª…í•´ì£¼ì„¸ìš”.',
                'intent': 'ê¸°ìˆ ì  íŒë‹¨ ê·¼ê±°ì™€ ì˜ì‚¬ê²°ì • ê³¼ì • í‰ê°€'
            },
            'COLLABORATION': {
                'question': 'ê·¸ ìƒí™©ì—ì„œ ë‹¤ë¥¸ íŒ€ì›ë“¤ì˜ ë°˜ì‘ì€ ì–´ë• ë‚˜ìš”?',
                'intent': 'íŒ€ ë™ë£Œì™€ì˜ ìƒí˜¸ì‘ìš©ê³¼ ì˜í–¥ë ¥ í‰ê°€'
            }
        }
        
        fallback = fallback_follow_ups.get(interviewer_role, fallback_follow_ups['HR'])
        
        # ì´ë¦„ í˜¸ëª… ì¶”ê°€
        candidate_name = user_resume.get('name', 'ì§€ì›ì') if user_resume else 'ì§€ì›ì'
        fallback['question'] = self._add_candidate_name_to_question(fallback['question'], candidate_name)
        
        fallback['interviewer_type'] = interviewer_role
        fallback['question_type'] = 'follow_up'
        fallback['question_source'] = 'fallback'
        
        return fallback
    
    def _add_candidate_name_to_question(self, question: str, candidate_name: str, is_intro_question: bool = False) -> str:
        """ì§ˆë¬¸ì— ì§€ì›ì ì´ë¦„ í˜¸ëª… ì¶”ê°€"""
        if not candidate_name or candidate_name == 'ì§€ì›ì':
            return question
        
        # ìê¸°ì†Œê°œ ì§ˆë¬¸ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬ (ì´ë¯¸ ì´ë¦„ì„ ëª¨ë¥´ëŠ” ìƒí™©)
        if is_intro_question:
            return question
        
        # ì´ë¦„ í˜¸ëª… íŒ¨í„´ë“¤ (ìì—°ìŠ¤ëŸ¬ìš´ ë‹¤ì–‘ì„± í™•ë³´)
        name_patterns = [
            f"{candidate_name}ë‹˜, {question}",
            f"{candidate_name}ë‹˜ê»˜ì„œëŠ” {question}",
            f"{candidate_name}ë‹˜, {question}",
            f"ê·¸ë ‡ë‹¤ë©´ {candidate_name}ë‹˜, {question}",
            f"{candidate_name}ë‹˜ì˜ ê²½ìš° {question}"
        ]
        
        # ëœë¤í•˜ê²Œ íŒ¨í„´ ì„ íƒ (80% í™•ë¥ ë¡œ ì´ë¦„ í˜¸ëª…)
        if random.random() < 0.8:
            selected_pattern = random.choice(name_patterns[:3])  # ê¸°ë³¸ íŒ¨í„´ ìš°ì„  ì‚¬ìš©
            return selected_pattern
        else:
            return question  # 20%ëŠ” ì´ë¦„ ì—†ì´ (ìì—°ìŠ¤ëŸ¬ìš´ ë‹¤ì–‘ì„±)
    
    def _inject_data_to_template(self, template: str, user_resume: Dict, company_info: Dict) -> str:
        """í…œí”Œë¦¿ì— ì‹¤ì œ ë°ì´í„° ë™ì  ì£¼ì…"""
        result = template
        
        # íšŒì‚¬ ì •ë³´ ì¹˜í™˜
        result = result.replace('{company_name}', company_info.get('name', 'íšŒì‚¬'))
        result = result.replace('{talent_profile}', company_info.get('talent_profile', ''))
        result = result.replace('{tech_focus}', ', '.join(company_info.get('tech_focus', [])))
        
        # ì§€ì›ì ì •ë³´ ì¹˜í™˜ 
        if user_resume:
            result = result.replace('{candidate_name}', user_resume.get('name', 'ì§€ì›ì'))
            result = result.replace('{experience_years}', str(user_resume.get('career_years', '0')))
            result = result.replace('{main_skills}', ', '.join(user_resume.get('technical_skills', [])[:3]))
        
        # AI ì§€ì›ì ì´ë¦„ í†µì¼
        result = result.replace('{persona_name}', 'ì¶˜ì‹ì´')
        
        return result
    

def main():
    """í„´ì œ ë©´ì ‘ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ¯ ì§€ëŠ¥í˜• í„´ì œ ë©´ì ‘ê´€ íŒ¨ë„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    try:
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        service = InterviewerService(total_question_limit=10)
        
        # ìƒ˜í”Œ ë°ì´í„°
        user_resume = {
            'name': 'ê¹€ê°œë°œ',
            'position': 'ë°±ì—”ë“œ ê°œë°œì',  # ì§êµ° ì •ë³´ ì¶”ê°€
            'career_years': '3',
            'technical_skills': ['Python', 'Django', 'PostgreSQL', 'AWS']
        }
        
        persona = CandidatePersona(
            name='ì¶˜ì‹ì´', summary='3ë…„ì°¨ Python ë°±ì—”ë“œ ê°œë°œì',
            background={'career_years': '3', 'current_position': 'ë°±ì—”ë“œ ê°œë°œì'},
            technical_skills=['Python', 'Django', 'PostgreSQL', 'AWS'],
            projects=[{'name': 'ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼', 'description': 'ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬'}],
            experiences=[{'company': 'ìŠ¤íƒ€íŠ¸ì—…', 'position': 'ê°œë°œì', 'period': '3ë…„'}],
            strengths=['ë¬¸ì œ í•´ê²°', 'í•™ìŠµ ëŠ¥ë ¥'], weaknesses=['ì™„ë²½ì£¼ì˜'],
            motivation='ì¢‹ì€ ì„œë¹„ìŠ¤ë¥¼ ë§Œë“¤ê³  ì‹¶ì–´ì„œ',
            inferred_personal_experiences=[{'experience': 'ì„±ì¥', 'lesson': 'ëŠì„ì—†ëŠ” í•™ìŠµ'}],
            career_goal='ì‹œë‹ˆì–´ ê°œë°œìë¡œ ì„±ì¥', personality_traits=['ì¹œê·¼í•¨', 'ì „ë¬¸ì„±'],
            interview_style='ìƒí˜¸ì‘ìš©ì ', resume_id=1
        )
        
        qa_history = []
        total_topics = sum(len(topics) for topics in service.topic_pools.values())
        
        print(f"ğŸ’¼ ì§ˆë¬¸ í•œë„: {service.total_question_limit}ê°œ")
        print(f"ğŸ‘¥ ë©´ì ‘ê´€: {', '.join(service.interviewer_roles)}")
        print(f"ğŸ² ì£¼ì œ í’€: {total_topics}ê°œ")
        
        # ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜
        while service.questions_asked_count < service.total_question_limit:
            question = service.generate_next_question(
                user_resume, persona, '1', qa_history,
                user_answer="API ì‘ë‹µ ì‹œê°„ì„ 50% ê°œì„ í•œ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤." if qa_history else None,
                chun_sik_answer="ì½”ë“œ ë¦¬ë·°ì™€ í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±ì„ ì¤‘ì‹œí•©ë‹ˆë‹¤." if qa_history else None
            )
            
            if question.get('is_final'):
                print(f"\nâœ… {question['question']}")
                break
                
            if question.get('force_turn_switch'):
                print(f"ğŸ”„ {question['interviewer_type']} ë©´ì ‘ê´€ í„´ ì¢…ë£Œ")
                continue
            
            # ì§ˆë¬¸ ì¶œë ¥
            num = service.questions_asked_count
            interviewer = service._get_current_interviewer()
            state = service.interviewer_turn_state[interviewer]
            
            print(f"\nğŸ“ ì§ˆë¬¸ {num}ë²ˆ - {question['interviewer_type']}")
            if question.get('topic'):
                print(f"ğŸ¯ ì£¼ì œ: {question['topic']}")
            print(f"â“ {question['question']}")
            print(f"ğŸ“ˆ í„´ ìƒíƒœ: ë©”ì¸ {'âœ“' if state['main_question_asked'] else 'âœ—'}, ê¼¬ë¦¬ {state['follow_up_count']}ê°œ")
            
            qa_history.append({'question': question['question'], 'interviewer_type': question['interviewer_type']})
            
            if num >= 8:  # í…ŒìŠ¤íŠ¸ ì œí•œ
                break
        
        # ìµœì¢… í†µê³„
        print(f"\nğŸ“Š ì´ ì§ˆë¬¸ ìˆ˜: {service.questions_asked_count}ê°œ")
        for role in service.interviewer_roles:
            state = service.interviewer_turn_state[role]
            print(f"   {role}: ë©”ì¸ {'âœ“' if state['main_question_asked'] else 'âœ—'}, ê¼¬ë¦¬ {state['follow_up_count']}ê°œ")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()