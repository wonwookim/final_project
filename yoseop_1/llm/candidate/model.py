#!/usr/bin/env python3
"""
AI ì§€ì›ì ëª¨ë¸ - LLM ê¸°ë°˜ ì‹¤ì‹œê°„ í˜ë¥´ì†Œë‚˜ ìƒì„±
ì‹¤ì œ ì´ë ¥ì„œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ì¸ê°„ë¯¸ ë„˜ì¹˜ëŠ” í˜ë¥´ì†Œë‚˜ë¥¼ ì‹¤ì‹œê°„ ìƒì„±
"""

import os
import json
import sys
import random
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dotenv import load_dotenv
from pydantic import BaseModel

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ database ëª¨ë“ˆ ì ‘ê·¼ì„ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
try:
    from backend.services.supabase_client import get_supabase_client
except ImportError:
    print("âš ï¸ Supabase í´ë¼ì´ì–¸íŠ¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê¸°ë°˜ fallbackë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
    get_supabase_client = None

from ..shared.models import LLMProvider, LLMResponse
from .quality_controller import AnswerQualityController, QualityLevel
from .prompt import CandidatePromptBuilder
from ..shared.models import QuestionType, QuestionAnswer, AnswerRequest, AnswerResponse
from ..session.models import InterviewSession
from ..shared.utils import safe_json_load, get_fixed_questions

# ì§êµ° ë§¤í•‘ (position_name -> position_id)
POSITION_MAPPING = {
    "í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì": 1,
    "í”„ë¡ íŠ¸": 1,
    "frontend": 1,
    "ë°±ì—”ë“œ": 2,
    "ë°±ì—”ë“œ ê°œë°œì": 2,
    "backend": 2,
    "ê¸°íš": 3,
    "ê¸°íšì": 3,
    "pm": 3,
    "product manager": 3,
    "AI": 4,
    "ai": 4,
    "ì¸ê³µì§€ëŠ¥": 4,
    "ë¨¸ì‹ ëŸ¬ë‹": 4,
    "ml": 4,
    "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤": 5,
    "ë°ì´í„°": 5,
    "data science": 5,
    "data scientist": 5,
    "ds": 5,
    # ğŸ†• ëª¨ë°”ì¼ ê°œë°œì ë§¤í•‘ ì¶”ê°€
    "ëª¨ë°”ì¼": 6,
    "ëª¨ë°”ì¼ê°œë°œì": 6,
    "ëª¨ë°”ì¼ê°œë°œìandroid": 6,
    "ëª¨ë°”ì¼ê°œë°œìios": 6,
    "android": 6,
    "ios": 6,
    "mobile": 6,
    "ì•±ê°œë°œì": 6,
    "ì•±": 6,
    # ğŸ†• ê¸°íƒ€ ì¼ë°˜ì ì¸ ì§êµ°ë“¤ ì¶”ê°€
    "í’€ìŠ¤íƒ": 7,
    "fullstack": 7,
    "í’€ìŠ¤íƒê°œë°œì": 7,
    "devops": 8,
    "ë°ë¸Œì˜µìŠ¤": 8,
    "ì¸í”„ë¼": 8,
    "qa": 9,
    "í…ŒìŠ¤í„°": 9,
    "í’ˆì§ˆê´€ë¦¬": 9
}

# ìƒˆë¡œìš´ CandidatePersona ëª¨ë¸ (LLM ìƒì„±ìš©)
class CandidatePersona(BaseModel):
    """LLMì´ ìƒì„±í•˜ëŠ” ì¸ê°„ë¯¸ ë„˜ì¹˜ëŠ” í˜ë¥´ì†Œë‚˜ ëª¨ë¸"""
    # --- LLM ìƒì„± ì •ë³´ ---
    name: str
    summary: str  # ì˜ˆ: "5ë…„ì°¨ Java ë°±ì—”ë“œ ê°œë°œìë¡œ, ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬ì™€ MSA ì„¤ê³„ì— ê°•ì ì´ ìˆìŠµë‹ˆë‹¤."
    background: Dict[str, Any]
    technical_skills: List[str]
    projects: List[Dict[str, Any]]  # ê° í”„ë¡œì íŠ¸ì— 'achievements'ì™€ 'challenges' í¬í•¨
    experiences: List[Dict[str, Any]]
    strengths: List[str]
    weaknesses: List[str]  # ê°œì„ í•˜ê³  ì‹¶ì€ ì 
    motivation: str  # ê°œë°œì/ê¸°ìˆ ì— ëŒ€í•œ ê°œì¸ì  ë™ê¸°ë‚˜ ìŠ¤í† ë¦¬
    inferred_personal_experiences: List[Dict[str, str]]  # ì´ë ¥ì„œ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ ëœ ê°œì¸ì  êµí›ˆ
    career_goal: str
    personality_traits: List[str]
    interview_style: str
    
    # --- ë©”íƒ€ë°ì´í„° ---
    generated_by: str = "gpt-4o-mini"
    resume_id: int  # ì›ë³¸ ì´ë ¥ì„œ ID

# ëª¨ë¸ë³„ AI ì§€ì›ì ì´ë¦„ ë§¤í•‘ (í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
AI_CANDIDATE_NAMES = {
    LLMProvider.OPENAI_GPT4: "ì¶˜ì‹ì´",
    LLMProvider.OPENAI_GPT35: "ì¶˜ì‹ì´", 
    LLMProvider.OPENAI_GPT4O_MINI: "ì¶˜ì‹ì´",
    LLMProvider.GOOGLE_GEMINI_PRO: "ì œë¯¸ë‹ˆ",
    LLMProvider.GOOGLE_GEMINI_FLASH: "ì œë¯¸ë‹ˆ",
    LLMProvider.KT_BELIEF: "ë¯¿ìŒì´"
}

class AICandidateSession(InterviewSession):
    """AI ì§€ì›ì ì „ìš© ë©´ì ‘ ì„¸ì…˜ - ë©´ì ‘ìì™€ ë™ì¼í•œ í”Œë¡œìš°"""
    
    def __init__(self, company_id: str, position: str, persona: CandidatePersona):
        super().__init__(company_id, position, persona.name)
        self.persona = persona
        self.ai_answers: List[QuestionAnswer] = []
        
        # ë©´ì ‘ìì™€ ë™ì¼í•œ 20ê°œ ì§ˆë¬¸ ê³„íš ì‚¬ìš©
        # ì´ë¯¸ ë¶€ëª¨ í´ë˜ìŠ¤ì—ì„œ self.question_planì´ 20ê°œë¡œ ì„¤ì •ë¨
        
    def add_ai_answer(self, qa_pair: QuestionAnswer):
        """AI ë‹µë³€ ì¶”ê°€"""
        self.ai_answers.append(qa_pair)
        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ add_qa_pair ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
        super().add_qa_pair(qa_pair)
    
    @property
    def question_answers(self) -> List[QuestionAnswer]:
        """FeedbackService í˜¸í™˜ì„±ì„ ìœ„í•œ question_answers property - conversation_history ë°˜í™˜"""
        return super().question_answers
        
    def get_persona_context(self) -> str:
        """í˜ë¥´ì†Œë‚˜ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context = f"""
=== AI ì§€ì›ì í˜ë¥´ì†Œë‚˜ ì •ë³´ ===
ì´ë¦„: {self.persona.name}
ê²½ë ¥: {self.persona.background.get('career_years', '0')}ë…„
í˜„ì¬ ì§ì±…: {self.persona.background.get('current_position', 'ì§€ì›ì')}
ì£¼ìš” ê¸°ìˆ : {', '.join(self.persona.technical_skills[:5])}
ê°•ì : {', '.join(self.persona.strengths[:3])}
ì»¤ë¦¬ì–´ ëª©í‘œ: {self.persona.career_goal}
ì„±ê²© íŠ¹ì„±: {', '.join(self.persona.personality_traits)}
ë©´ì ‘ ìŠ¤íƒ€ì¼: {self.persona.interview_style}

=== ì£¼ìš” í”„ë¡œì íŠ¸ ===
"""
        for i, project in enumerate(self.persona.projects[:2], 1):
            context += f"{i}. {project.get('name', 'í”„ë¡œì íŠ¸')}: {project.get('description', '')}\n"
            context += f"   ê¸°ìˆ ìŠ¤íƒ: {', '.join(project.get('tech_stack', []))}\n"
            if project.get('achievements'):
                context += f"   ì„±ê³¼: {', '.join(project['achievements'])}\n"
        
        context += f"""
=== ì—…ë¬´ ê²½í—˜ ===
"""
        for exp in self.persona.experiences[:2]:
            context += f"- {exp.get('company', 'íšŒì‚¬')}: {exp.get('position', 'ê°œë°œì')} ({exp.get('period', 'ê¸°ê°„')})\n"
            if exp.get('achievements'):
                context += f"  ì„±ê³¼: {', '.join(exp['achievements'])}\n"
        
        return context
    
    def get_previous_answers_context(self) -> str:
        """ì´ì „ ë‹µë³€ ì»¨í…ìŠ¤íŠ¸ (ì¼ê´€ì„± ìœ ì§€ìš©)"""
        if not self.ai_answers:
            return ""
        
        context = "\n=== ì´ì „ ë‹µë³€ ë‚´ì—­ (ì¼ê´€ì„± ìœ ì§€) ===\n"
        for i, qa in enumerate(self.ai_answers[-3:], 1):  # ìµœê·¼ 3ê°œë§Œ
            context += f"{i}. [{qa.question_type.value}] {qa.question_content}\n"
            context += f"   ë‹µë³€: {qa.answer_content[:100]}...\n\n"
        
        return context

class AICandidateModel:
    """AI ì§€ì›ì ëª¨ë¸ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, api_key: str = None, quality_controller: AnswerQualityController = None):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì´ˆê¸°í™”
        import openai
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if self.api_key:
            self.openai_client = openai.OpenAI(api_key=self.api_key)
            print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            self.openai_client = None
            print("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì§ì ‘ ì „ë‹¬í•˜ì„¸ìš”.")
        
        # ğŸ†• ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´: quality_controllerë¥¼ ì™¸ë¶€ì—ì„œ ì£¼ì… ë°›ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
        if quality_controller is not None:
            self.quality_controller = quality_controller
            print("âœ… [DI] ì™¸ë¶€ì—ì„œ ì£¼ì…ëœ QualityController ì‚¬ìš©")
        else:
            self.quality_controller = AnswerQualityController()
            print("âœ… [DI] ê¸°ë³¸ QualityController ìƒì„±")
            
        # ğŸ†• í”„ë¡¬í”„íŠ¸ ë¹Œë” ì´ˆê¸°í™”
        self.prompt_builder = CandidatePromptBuilder()
        print("âœ… [DI] CandidatePromptBuilder ì´ˆê¸°í™” ì™„ë£Œ")
            
        self.companies_data = self._load_companies_data()
        
        # AI ì§€ì›ì ì„¸ì…˜ ê´€ë¦¬
        self.ai_sessions: Dict[str, 'AICandidateSession'] = {}
        self.fixed_questions = self._load_fixed_questions()
        
        # ìƒˆë¡œìš´ LLM ê¸°ë°˜ ì‹œìŠ¤í…œì—ì„œëŠ” í˜ë¥´ì†Œë‚˜ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ë¯€ë¡œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
        self.candidate_personas: Dict[str, CandidatePersona] = {}
        self.personas_data = {"personas": {}}
    
    def create_persona_for_interview(self, company_name: str, position_name: str) -> Optional[CandidatePersona]:
        """
        ì£¼ì–´ì§„ íšŒì‚¬ì™€ ì§êµ°ì— ë§ëŠ” AI ì§€ì›ì í˜ë¥´ì†Œë‚˜ë¥¼ LLMìœ¼ë¡œ ì‹¤ì‹œê°„ ìƒì„±
        """
        try:
            print(f"ğŸ”¥ [PERSONA DEBUG] í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹œì‘: company='{company_name}', position='{position_name}'")
            
            company_korean_name = self._get_company_korean_name(company_name)
            position_id = self._get_position_id(position_name, company_korean_name)
            
            if not position_id:
                print(f"âŒ [PERSONA DEBUG] ì§€ì›í•˜ì§€ ì•ŠëŠ” ì§êµ°: {position_name}, ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹œë„")
                return self._create_default_persona(company_korean_name, position_name)
            
            resume_data = self._get_random_resume_from_db(position_id)
            if not resume_data:
                print(f"âŒ [PERSONA DEBUG] ì´ë ¥ì„œ ì—†ìŒ: position_id {position_id}, ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹œë„")
                return self._create_default_persona(company_korean_name, position_name)
            
            company_info = self._get_company_info(company_name)
            
            prompt = self.prompt_builder.build_persona_generation_prompt(resume_data, company_name, position_name, company_info)
            system_prompt = self.prompt_builder.build_system_prompt_for_persona_generation()
            
            llm_response = self._generate_persona_with_extended_tokens(prompt, system_prompt)
            
            if llm_response.error:
                print(f"âŒ [PERSONA DEBUG] LLM ì‘ë‹µ ì˜¤ë¥˜: {llm_response.error}")
                return None
            
            persona = self._parse_llm_response_to_persona(llm_response.content, resume_data.get('ai_resume_id', 0))
            
            if persona:
                print(f"âœ… [PERSONA DEBUG] í˜ë¥´ì†Œë‚˜ ìƒì„± ì™„ë£Œ: {persona.name} ({company_name} {position_name})")
                return persona
            else:
                print(f"âŒ [PERSONA DEBUG] í˜ë¥´ì†Œë‚˜ íŒŒì‹± ì‹¤íŒ¨")
                return None
                
        except Exception as e:
            print(f"âŒ [PERSONA DEBUG] í˜ë¥´ì†Œë‚˜ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _get_position_id(self, position_name: str, company_name: str = None) -> Optional[int]:
        """ì§êµ°ëª…ì„ position_idë¡œ ë³€í™˜"""
        # ... (ì´í•˜ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼)
        try:
            # ğŸ†• 1ìˆœìœ„: DBì—ì„œ ì§ì ‘ ì¡°íšŒ (company_nameì´ ìˆëŠ” ê²½ìš°)
            if company_name and get_supabase_client:
                from backend.services.existing_tables_service import existing_tables_service
                import asyncio
                import concurrent.futures
                
                def run_async_safely():
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        return loop.run_until_complete(
                            existing_tables_service.find_posting_by_company_position(company_name, position_name)
                        )
                    finally:
                        loop.close()
                
                try:
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(run_async_safely)
                        posting_info = future.result(timeout=10)
                    
                    if posting_info and posting_info.get('position', {}).get('position_id'):
                        return posting_info['position']['position_id']
                except Exception as db_error:
                    print(f"âš ï¸ [DB] ì§êµ° ì¡°íšŒ ì‹¤íŒ¨: {db_error}")
            
            position_lower = position_name.lower().replace(" ", "").replace("(", "").replace(")", "")
            return POSITION_MAPPING.get(position_lower)
            
        except Exception as e:
            print(f"âŒ [POSITION] ì§êµ° ID ë³€í™˜ ì˜¤ë¥˜: {e}")
            return None
    
    def _get_random_resume_from_db(self, position_id: int) -> Optional[Dict[str, Any]]:
        """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•´ë‹¹ ì§êµ°ì˜ ì´ë ¥ì„œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒ"""
        if get_supabase_client is None:
            return None
        try:
            response = get_supabase_client().table('ai_resume').select('*').eq('position_id', position_id).execute()
            return random.choice(response.data) if response.data else None
        except Exception as e:
            print(f"âŒ ì´ë ¥ì„œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def _get_company_info(self, company_name: str) -> Dict[str, Any]:
        """íšŒì‚¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        for company in self.companies_data.get("companies", []):
            if company.get("name", "").lower() == company_name.lower() or company.get("id", "").lower() == company_name.lower():
                return company
        return {"name": company_name, "core_competencies": [], "tech_focus": [], "talent_profile": ""}
    
    def _generate_persona_with_extended_tokens(self, prompt: str, system_prompt: str) -> LLMResponse:
        """í˜ë¥´ì†Œë‚˜ ìƒì„±ìš© í™•ì¥ëœ í† í°ìœ¼ë¡œ LLM í˜¸ì¶œ"""
        if not self.openai_client:
            return LLMResponse(content="", provider=LLMProvider.OPENAI_GPT4O_MINI, model_name="gpt-4o-mini", error="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        try:
            import time
            start_time = time.time()
            messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}]
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini", messages=messages, max_tokens=1500, temperature=0.7, timeout=60.0
            )
            response_time = time.time() - start_time
            return LLMResponse(
                content=response.choices[0].message.content.strip(),
                provider=LLMProvider.OPENAI_GPT4O_MINI,
                model_name="gpt-4o-mini",
                token_count=response.usage.total_tokens if response.usage else None,
                response_time=response_time
            )
        except Exception as e:
            return LLMResponse(content="", provider=LLMProvider.OPENAI_GPT4O_MINI, model_name="gpt-4o-mini", error=f"í˜ë¥´ì†Œë‚˜ ìƒì„± LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
    
    def _parse_llm_response_to_persona(self, llm_response: str, resume_id: int) -> Optional[CandidatePersona]:
        """LLM JSON ì‘ë‹µì„ CandidatePersona ê°ì²´ë¡œ íŒŒì‹±"""
        try:
            response_clean = llm_response.strip()
            if response_clean.startswith('```json'):
                response_clean = response_clean.replace('```json', '').replace('```', '').strip()
            persona_data = json.loads(response_clean)
            return CandidatePersona(**persona_data, resume_id=resume_id)
        except (json.JSONDecodeError, TypeError) as e:
            print(f"âŒ JSON íŒŒì‹± ë˜ëŠ” í˜ë¥´ì†Œë‚˜ ê°ì²´ ìƒì„± ì˜¤ë¥˜: {e}")
            return None
    
    def _load_companies_data(self) -> Dict[str, Any]:
        """íšŒì‚¬ ë°ì´í„° ë¡œë“œ"""
        return safe_json_load("llm/data/companies_data.json", {"companies": []})
    
    def _create_default_persona(self, company_name: str, position_name: str) -> Optional[CandidatePersona]:
        """fallback: ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ìƒì„±"""
        # ... (ì´í•˜ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼)
        try:
            print(f"ğŸ”„ [DEFAULT PERSONA] ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹œì‘: {company_name} - {position_name}")
            company_info = self._get_company_info(company_name)
            company_name = company_info.get("name", company_name.capitalize())
            
            default_persona = CandidatePersona(
                name=f"{company_name} ì§€ì›ì",
                summary=f"{position_name} ê°œë°œìë¡œ {company_name}ì— ì§€ì›í•˜ëŠ” ê²½ë ¥ 3ë…„ì°¨ ê°œë°œìì…ë‹ˆë‹¤.",
                background={"career_years": "3", "current_position": f"{position_name} ê°œë°œì", "education": ["ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ ì¡¸ì—…"], "total_experience": "3ë…„"},
                technical_skills=self._get_default_tech_skills(position_name),
                projects=[{"name": f"{position_name} í”„ë¡œì íŠ¸", "description": f"{position_name} ê°œë°œ í”„ë¡œì íŠ¸ ê²½í—˜", "tech_stack": self._get_default_tech_skills(position_name)[:3], "achievements": ["ì„±ê³µì ì¸ í”„ë¡œì íŠ¸ ì™„ìˆ˜", "íŒ€ì›Œí¬ í–¥ìƒì— ê¸°ì—¬"]}],
                experiences=[{"company": "ê¸°ì¡´ íšŒì‚¬", "position": f"{position_name} ê°œë°œì", "period": "2021 - í˜„ì¬", "achievements": ["í”„ë¡œì íŠ¸ ì„±ê³µì  ì™„ìˆ˜", "ê¸°ìˆ  ì—­ëŸ‰ í–¥ìƒ"]}],
                strengths=["ë¬¸ì œ í•´ê²° ëŠ¥ë ¥", "íŒ€ì›Œí¬", "í•™ìŠµ ì˜ì§€"],
                weaknesses=["ì™„ë²½ì£¼ì˜ì  ì„±í–¥"],
                motivation=f"{company_name}ì—ì„œ {position_name} ê°œë°œìë¡œ ì„±ì¥í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.",
                inferred_personal_experiences=[{"category": "í•™ìŠµ", "experience": "ì§€ì†ì ì¸ ê¸°ìˆ  í•™ìŠµì„ í†µí•´ ì„±ì¥í•´ì™”ìŠµë‹ˆë‹¤.", "lesson": "ê¾¸ì¤€í•œ í•™ìŠµì˜ ì¤‘ìš”ì„±ì„ ê¹¨ë‹¬ì•˜ìŠµë‹ˆë‹¤."}],
                career_goal=f"{company_name}ì—ì„œ {position_name} ì „ë¬¸ê°€ë¡œ ì„±ì¥í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.",
                personality_traits=["ì„±ì‹¤í•¨", "ì ê·¹ì„±", "í˜‘ë ¥ì "],
                interview_style="ì§„ì •ì„± ìˆê³  ë…¼ë¦¬ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ìŠ¤íƒ€ì¼",
                resume_id=0
            )
            return default_persona
        except Exception as e:
            print(f"âŒ [DEFAULT PERSONA] ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _get_company_korean_name(self, company_code: str) -> str:
        """íšŒì‚¬ ì½”ë“œë¥¼ í•œêµ­ì–´ íšŒì‚¬ëª…ìœ¼ë¡œ ë³€í™˜"""
        company_mapping = {
            "naver": "ë„¤ì´ë²„", "kakao": "ì¹´ì¹´ì˜¤", "toss": "í† ìŠ¤", "line": "ë¼ì¸",
            "ë¼ì¸í”ŒëŸ¬ìŠ¤": "ë¼ì¸í”ŒëŸ¬ìŠ¤", "coupang": "ì¿ íŒ¡", "baemin": "ë°°ë‹¬ì˜ë¯¼ì¡±", "daangn": "ë‹¹ê·¼ë§ˆì¼“",
            "ë„¤ì´ë²„": "ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤": "ì¹´ì¹´ì˜¤", "í† ìŠ¤": "í† ìŠ¤", "ë¼ì¸": "ë¼ì¸", 
            "ì¿ íŒ¡": "ì¿ íŒ¡", "ë°°ë‹¬ì˜ë¯¼ì¡±": "ë°°ë‹¬ì˜ë¯¼ì¡±", "ë‹¹ê·¼ë§ˆì¼“": "ë‹¹ê·¼ë§ˆì¼“"
        }
        return company_mapping.get(company_code.lower(), company_code.capitalize())
    
    def _load_fixed_questions(self) -> Dict[str, List[Dict]]:
        """ê³ ì • ì§ˆë¬¸ ë°ì´í„° ë¡œë“œ"""
        return get_fixed_questions()

    def generate_answer(self, request: AnswerRequest, persona: CandidatePersona = None) -> AnswerResponse:
        """ì§ˆë¬¸ì— ëŒ€í•œ AI ì§€ì›ì ë‹µë³€ ìƒì„±"""
        start_time = datetime.now()
        
        if not persona:
            persona = self.create_persona_for_interview(request.company_id, request.position)
            if not persona:
                persona = self._create_default_persona(request.company_id, request.position)
        
        if not persona:
            return AnswerResponse(
                answer_content="ì£„ì†¡í•©ë‹ˆë‹¤. ì§€ì›ì ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                quality_level=request.quality_level, llm_provider=request.llm_provider,
                persona_name="ì˜¤ë¥˜", confidence_score=0.1, response_time=0.1,
                reasoning="í˜ë¥´ì†Œë‚˜ ìƒì„± ìµœì¢… ì‹¤íŒ¨", error="Persona creation failed"
            )
        
        company_data = self._get_company_info(request.company_id)
        
        prompt = self.prompt_builder.build_prompt(request, persona, company_data)
        
        config = self.quality_controller.get_quality_config(request.quality_level)
        quality_prompt = self.quality_controller.generate_quality_prompt(prompt, request.quality_level, request.question_type.value)
        
        # system_prompt ìƒì„±
        system_prompt = self.prompt_builder.build_system_prompt(persona, company_data, request.question_type, request.llm_provider)
        
        llm_response = self._generate_llm_answer(quality_prompt, system_prompt, config)
        
        response_time = (datetime.now() - start_time).total_seconds()
        confidence_score = self._calculate_confidence_score(llm_response, request.quality_level)
        processed_answer = self.quality_controller.process_complete_answer(llm_response.content, request.quality_level, request.question_type.value)
        
        return AnswerResponse(
            answer_content=processed_answer,
            quality_level=request.quality_level,
            llm_provider=llm_response.provider,
            persona_name=self.get_ai_name(llm_response.provider),
            confidence_score=confidence_score,
            response_time=response_time,
            reasoning=f"{self.get_ai_name(llm_response.provider)}ì˜ ë‹µë³€",
            error=llm_response.error,
            metadata={
                "token_count": llm_response.token_count,
                "company_id": request.company_id,
                "question_type": request.question_type.value,
                "original_prompt_length": len(prompt),
                "persona_name_internal": persona.name
            }
        )

    def _generate_llm_answer(self, prompt: str, system_prompt: str, config) -> LLMResponse:
        """LLM ë‹µë³€ ìƒì„± ë¡œì§"""
        if not self.openai_client:
            return LLMResponse(content="", provider=LLMProvider.OPENAI_GPT4O_MINI, model_name=config.model_name, error="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        try:
            import time
            start_time = time.time()
            messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}]
            response = self.openai_client.chat.completions.create(
                model=config.model_name, messages=messages, max_tokens=400,
                temperature=config.temperature, timeout=60.0
            )
            response_time = time.time() - start_time
            provider = LLMProvider.OPENAI_GPT4O if config.model_name == "gpt-4o" else LLMProvider.OPENAI_GPT4O_MINI
            return LLMResponse(
                content=response.choices[0].message.content.strip(),
                provider=provider, model_name=config.model_name,
                token_count=response.usage.total_tokens if response.usage else None,
                response_time=response_time
            )
        except Exception as e:
            provider = LLMProvider.OPENAI_GPT4O if config.model_name == "gpt-4o" else LLMProvider.OPENAI_GPT4O_MINI
            return LLMResponse(content="", provider=provider, model_name=config.model_name, error=f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}")

    def _calculate_confidence_score(self, llm_response: LLMResponse, quality_level: QualityLevel) -> float:
        """ë‹µë³€ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        if llm_response.error: return 0.0
        base_score = 0.7
        quality_bonus = (quality_level.value - 5) * 0.05
        length_bonus = min(len(llm_response.content) / 1000, 0.2)
        time_bonus = 0.1 if llm_response.response_time and llm_response.response_time < 3.0 else 0.0
        confidence = min(base_score + quality_bonus + length_bonus + time_bonus, 1.0)
        return round(confidence, 2)

    def get_ai_name(self, llm_provider: LLMProvider) -> str:
        """ëª¨ë¸ì— ë”°ë¥¸ AI ì§€ì›ì ì´ë¦„ ë°˜í™˜"""
        return AI_CANDIDATE_NAMES.get(llm_provider, "ì¶˜ì‹ì´")

    def _get_default_tech_skills(self, position: str) -> List[str]:
        """ì§êµ°ë³„ ê¸°ë³¸ ê¸°ìˆ  ìŠ¤íƒ"""
        tech_mapping = {
            "í”„ë¡ íŠ¸ì—”ë“œ": ["JavaScript", "React", "HTML/CSS", "TypeScript", "Vue.js"],
            "ë°±ì—”ë“œ": ["Java", "Spring Boot", "MySQL", "Python", "Node.js"],
            "í’€ìŠ¤íƒ": ["JavaScript", "React", "Node.js", "MySQL", "TypeScript"],
            "AI": ["Python", "TensorFlow", "PyTorch", "Machine Learning", "Data Science"],
            "ë°ì´í„°": ["Python", "SQL", "Pandas", "Tableau", "R"],
            "ê¸°íš": ["Product Management", "ê¸°íš", "ë¶„ì„", "Communication", "Strategy"]
        }
        return tech_mapping.get(position, tech_mapping["ë°±ì—”ë“œ"])