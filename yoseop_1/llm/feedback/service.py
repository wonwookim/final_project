#!/usr/bin/env python3
"""
í”¼ë“œë°± ì„œë¹„ìŠ¤
ë‹µë³€ í‰ê°€ ë° í”¼ë“œë°± ìƒì„±ì„ ë‹´ë‹¹
"""

import openai
import json
import os
from typing import Dict, List, Any, Optional
from datetime import datetime
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from ..shared.models import QuestionAnswer, QuestionType, CandidatePersona
from ..session.models import InterviewSession
from ..core.llm_manager import LLMManager, LLMProvider
from ..shared.utils import safe_json_load

class FeedbackService:
    """ë©´ì ‘ ë‹µë³€ í‰ê°€ ë° í”¼ë“œë°± ìƒì„± ì„œë¹„ìŠ¤"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.client = openai.OpenAI(api_key=self.api_key)
        self.llm_manager = LLMManager()
        self.companies_data = self._load_companies_data()
    
    def evaluate_answer(self, question_answer: QuestionAnswer, 
                       company_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ë‹¨ì¼ ë‹µë³€ í‰ê°€"""
        try:
            evaluation_prompt = self._create_evaluation_prompt(question_answer, company_context)
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ë‹µë³€ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            result = response.choices[0].message.content.strip()
            return self._parse_evaluation_result(result)
            
        except Exception as e:
            return {
                "score": 70,
                "feedback": "í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "strengths": [],
                "improvements": [],
                "detailed_feedback": str(e)
            }
    
    def evaluate_session(self, session: InterviewSession, 
                        company_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì „ì²´ ì„¸ì…˜ ì¢…í•© í‰ê°€"""
        if not session.question_answers:
            return {
                "total_score": 0,
                "category_scores": {},
                "overall_feedback": "ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤.",
                "recommendations": []
            }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        category_scores = self._calculate_category_scores(session.question_answers)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        total_score = sum(category_scores.values()) // len(category_scores)
        
        # ì¢…í•© í”¼ë“œë°± ìƒì„±
        overall_feedback = self._generate_overall_feedback(session, category_scores, company_context)
        
        # ê°œì„  ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(category_scores, session.question_answers)
        
        return {
            "total_score": total_score,
            "category_scores": category_scores,
            "overall_feedback": overall_feedback,
            "recommendations": recommendations,
            "detailed_evaluations": [self.evaluate_answer(qa, company_context) for qa in session.question_answers]
        }
    
    def _create_evaluation_prompt(self, question_answer: QuestionAnswer, 
                                company_context: Dict[str, Any] = None) -> str:
        """í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        context_info = ""
        if company_context:
            context_info = f"""
=== ê¸°ì—… ì •ë³´ ===
íšŒì‚¬: {company_context.get('name', 'ì•Œ ìˆ˜ ì—†ìŒ')}
ì¸ì¬ìƒ: {company_context.get('talent_profile', 'ìš°ìˆ˜í•œ ì¸ì¬')}
í•µì‹¬ ì—­ëŸ‰: {', '.join(company_context.get('core_competencies', ['ì „ë¬¸ì„±', 'í˜‘ì—…']))}
"""
        
        return f"""
{context_info}

=== ë©´ì ‘ ì§ˆë¬¸ ë° ë‹µë³€ ===
ì§ˆë¬¸ ìœ í˜•: {question_answer.question_type.value}
ì§ˆë¬¸: {question_answer.question_content}
ë‹µë³€: {question_answer.answer_content}

ìœ„ ë‹µë³€ì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:

1. ì§ˆë¬¸ ì´í•´ë„ (ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í–ˆëŠ”ê°€?)
2. ë‹µë³€ ì™„ì„±ë„ (êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì¸ê°€?)
3. ì „ë¬¸ì„± (í•´ë‹¹ ë¶„ì•¼ì˜ ì „ë¬¸ ì§€ì‹ì„ ë³´ì—¬ì£¼ëŠ”ê°€?)
4. ë…¼ë¦¬ì„± (ë‹µë³€ì˜ êµ¬ì¡°ì™€ ë…¼ë¦¬ê°€ ëª…í™•í•œê°€?)
5. ì§„ì •ì„± (ì§„ì†”í•˜ê³  ì„¤ë“ë ¥ ìˆëŠ” ë‹µë³€ì¸ê°€?)

í‰ê°€ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:

ì ìˆ˜: [0-100ì ]
ê°•ì : [êµ¬ì²´ì ì¸ ê°•ì  2-3ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„]
ê°œì„ ì : [êµ¬ì²´ì ì¸ ê°œì„ ì‚¬í•­ 2-3ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„]
í”¼ë“œë°±: [ì¢…í•©ì ì¸ í”¼ë“œë°± 2-3ë¬¸ì¥]
"""
    
    def _parse_evaluation_result(self, result: str) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ íŒŒì‹±"""
        try:
            lines = result.strip().split('\n')
            evaluation = {
                "score": 70,
                "strengths": [],
                "improvements": [],
                "feedback": "í‰ê°€ ì™„ë£Œ"
            }
            
            for line in lines:
                line = line.strip()
                if line.startswith('ì ìˆ˜:'):
                    try:
                        score_str = line.replace('ì ìˆ˜:', '').strip().replace('ì ', '')
                        evaluation["score"] = min(100, max(0, int(score_str)))
                    except:
                        evaluation["score"] = 70
                        
                elif line.startswith('ê°•ì :'):
                    strengths_str = line.replace('ê°•ì :', '').strip()
                    evaluation["strengths"] = [s.strip() for s in strengths_str.split(',') if s.strip()]
                    
                elif line.startswith('ê°œì„ ì :'):
                    improvements_str = line.replace('ê°œì„ ì :', '').strip()
                    evaluation["improvements"] = [i.strip() for i in improvements_str.split(',') if i.strip()]
                    
                elif line.startswith('í”¼ë“œë°±:'):
                    evaluation["feedback"] = line.replace('í”¼ë“œë°±:', '').strip()
            
            return evaluation
            
        except Exception as e:
            return {
                "score": 70,
                "strengths": ["ë‹µë³€ ì œì¶œ ì™„ë£Œ"],
                "improvements": ["ë” êµ¬ì²´ì ì¸ ì„¤ëª… í•„ìš”"],
                "feedback": "í‰ê°€ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
    
    def _calculate_category_scores(self, question_answers: List[QuestionAnswer]) -> Dict[str, int]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°"""
        category_scores = {}
        category_counts = {}
        
        for qa in question_answers:
            category = qa.question_type.value
            score = qa.individual_score or 70  # ê¸°ë³¸ê°’
            
            if category not in category_scores:
                category_scores[category] = 0
                category_counts[category] = 0
            
            category_scores[category] += score
            category_counts[category] += 1
        
        # í‰ê·  ê³„ì‚°
        for category in category_scores:
            if category_counts[category] > 0:
                category_scores[category] = category_scores[category] // category_counts[category]
        
        # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ë“¤ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        default_categories = ["ìê¸°ì†Œê°œ", "ì§€ì›ë™ê¸°", "ì¸ì‚¬", "ê¸°ìˆ ", "í˜‘ì—…"]
        for category in default_categories:
            if category not in category_scores:
                category_scores[category] = 75
        
        return category_scores
    
    def _generate_overall_feedback(self, session: InterviewSession,
                                 category_scores: Dict[str, int],
                                 company_context: Dict[str, Any] = None) -> str:
        """ì¢…í•© í”¼ë“œë°± ìƒì„±"""
        avg_score = sum(category_scores.values()) // len(category_scores)
        
        if avg_score >= 90:
            feedback = f"{session.candidate_name}ë‹˜ì€ ì „ ì˜ì—­ì—ì„œ ìš°ìˆ˜í•œ ë©´ì ‘ ì—­ëŸ‰ì„ ë³´ì—¬ì£¼ì…¨ìŠµë‹ˆë‹¤."
        elif avg_score >= 80:
            feedback = f"{session.candidate_name}ë‹˜ì€ ëŒ€ì²´ë¡œ ì¢‹ì€ ë©´ì ‘ ì—­ëŸ‰ì„ ë³´ì—¬ì£¼ì…¨ìŠµë‹ˆë‹¤."
        elif avg_score >= 70:
            feedback = f"{session.candidate_name}ë‹˜ì€ ê¸°ë³¸ì ì¸ ë©´ì ‘ ì—­ëŸ‰ì„ ë³´ì—¬ì£¼ì…¨ìŠµë‹ˆë‹¤."
        else:
            feedback = f"{session.candidate_name}ë‹˜ì€ ì¶”ê°€ ì¤€ë¹„ê°€ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤."
        
        # ê°€ì¥ ê°•í•œ ì˜ì—­ê³¼ ì•½í•œ ì˜ì—­ ì‹ë³„
        best_category = max(category_scores, key=category_scores.get)
        worst_category = min(category_scores, key=category_scores.get)
        
        feedback += f" íŠ¹íˆ {best_category} ì˜ì—­ì—ì„œ ê°•ì ì„ ë³´ì´ì…¨ê³ , {worst_category} ì˜ì—­ì—ì„œ ë” ë³´ì™„í•˜ì‹œë©´ ì¢‹ê² ìŠµë‹ˆë‹¤."
        
        return feedback
    
    def _generate_recommendations(self, category_scores: Dict[str, int],
                                question_answers: List[QuestionAnswer]) -> List[str]:
        """ê°œì„  ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì ìˆ˜ê°€ ë‚®ì€ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ì¶”ì²œ
        for category, score in category_scores.items():
            if score < 75:
                if category == "ê¸°ìˆ ":
                    recommendations.append("ê¸°ìˆ ì  ê¹Šì´ë¥¼ ë” ë³´ì™„í•˜ì‹œê³  êµ¬ì²´ì ì¸ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ì¤€ë¹„í•˜ì„¸ìš”")
                elif category == "í˜‘ì—…":
                    recommendations.append("íŒ€ì›Œí¬ ê²½í—˜ê³¼ ì†Œí†µ ëŠ¥ë ¥ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì–´í•„í•˜ì„¸ìš”")
                elif category == "ì¸ì‚¬":
                    recommendations.append("ê°œì¸ì  ì„±ì¥ ìŠ¤í† ë¦¬ì™€ ê°€ì¹˜ê´€ì„ ë” ëª…í™•íˆ í‘œí˜„í•˜ì„¸ìš”")
        
        # ë‹µë³€ Ğ´Ğ»åº¦ ê¸°ë°˜ ì¶”ì²œ
        avg_answer_length = sum(len(qa.answer_content) for qa in question_answers) / len(question_answers)
        if avg_answer_length < 100:
            recommendations.append("ë‹µë³€ì„ ë” êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ ì¤€ë¹„í•˜ì„¸ìš”")
        elif avg_answer_length > 500:
            recommendations.append("ë‹µë³€ì„ ë” ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”")
        
        # ê¸°ë³¸ ì¶”ì²œì‚¬í•­
        if not recommendations:
            recommendations = [
                "êµ¬ì²´ì ì¸ ì‚¬ë¡€ì™€ ìˆ«ìë¡œ ì„±ê³¼ë¥¼ ì–´í•„í•˜ì„¸ìš”",
                "íšŒì‚¬ì— ëŒ€í•œ ì´í•´ë„ë¥¼ ë†’ì´ê³  ì§€ì› ë™ê¸°ë¥¼ ëª…í™•íˆ í•˜ì„¸ìš”",
                "ë©´ì ‘ ì—°ìŠµì„ í†µí•´ ìì‹ ê°ì„ ê¸°ë¥´ì„¸ìš”"
            ]
        
        return recommendations[:3]  # ìµœëŒ€ 3ê°œ
    
    def _load_companies_data(self) -> Dict[str, Any]:
        """íšŒì‚¬ ë°ì´í„° ë¡œë“œ"""
        # llm/shared/data/companies_data.json ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        return safe_json_load("llm/shared/data/companies_data.json", {"companies": []})
    
    def _get_company_data(self, company_id: str) -> Dict[str, Any]:
        """íšŒì‚¬ ë°ì´í„° ì¡°íšŒ"""
        for company in self.companies_data.get("companies", []):
            if company["id"] == company_id:
                return company
        return {}
    
    def evaluate_ai_interview(self, ai_session: InterviewSession) -> Dict[str, Any]:
        """AI ì§€ì›ì ë©´ì ‘ í‰ê°€ (ë©´ì ‘ìì™€ ë™ì¼í•œ êµ¬ì¡°)"""
        if not ai_session:
            return {"error": "AI ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
        
        company_data = self._get_company_data(ai_session.company_id)
        
        # 1. ê° ë‹µë³€ì„ ê°œë³„ì ìœ¼ë¡œ í‰ê°€
        individual_feedbacks = []
        total_score = 0
        category_scores = {}
        
        for qa in ai_session.question_answers:
            # ê°œë³„ ë‹µë³€ í‰ê°€ (ë©´ì ‘ìì™€ ë™ì¼í•œ ë¡œì§)
            individual_evaluation = self._evaluate_ai_single_answer(qa, company_data)
            
            # í‰ê°€ ê²°ê³¼ë¥¼ qa_pairì— ì €ì¥
            qa.individual_score = individual_evaluation["score"]
            qa.individual_feedback = individual_evaluation["feedback"]
            
            individual_feedbacks.append({
                "question_number": len(individual_feedbacks) + 1,
                "question_type": qa.question_type.value,
                "question": qa.question_content,
                "question_intent": qa.question_intent,
                "answer": qa.answer_content,
                "score": qa.individual_score,
                "feedback": qa.individual_feedback,
                "personalized": False  # AIëŠ” í‘œì¤€ ì§ˆë¬¸ ì‚¬ìš©
            })
            
            total_score += qa.individual_score
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
            category = qa.question_type.value
            if category not in category_scores:
                category_scores[category] = []
            category_scores[category].append(qa.individual_score)
        
        # ì „ì²´ í‰ê·  ê³„ì‚°
        overall_score = int(total_score / len(ai_session.question_answers))
        
        # ì¹´í…Œê³ ë¦¬ë³„ í‰ê· 
        for category in category_scores:
            category_scores[category] = int(sum(category_scores[category]) / len(category_scores[category]))
        
        # 2. ì¢…í•© í‰ê°€ ìƒì„±
        overall_evaluation = self._generate_ai_overall_evaluation(ai_session, company_data, overall_score)
        
        # AICandidateSession ì†ì„± ì•ˆì „ ì ‘ê·¼
        session_id = getattr(ai_session, 'session_id', f"{ai_session.company_id}_{ai_session.position}_ai")
        candidate_name = getattr(ai_session, 'persona', None)
        candidate_name = candidate_name.name if candidate_name else ai_session.candidate_name
        
        return {
            "session_id": session_id,
            "company": company_data.get("name", ""),
            "position": ai_session.position,
            "candidate": candidate_name,
            "candidate_type": "AI",
            "individual_feedbacks": individual_feedbacks,
            "evaluation": {
                "overall_score": overall_score,
                "category_scores": category_scores,
                **overall_evaluation
            },
            "completed_at": datetime.now().isoformat()
        }
    
    def _evaluate_ai_single_answer(self, qa: QuestionAnswer, company_data: Dict[str, Any]) -> Dict[str, Any]:
        """AI ë‹µë³€ ê°œë³„ í‰ê°€ (ë©´ì ‘ìì™€ ë™ì¼í•œ ì—„ê²©í•œ ê¸°ì¤€)"""
        
        answer = qa.answer_content.strip()
        
        # ê¸°ë³¸ ê²€ì¦
        if len(answer) < 10:
            return {
                "score": 20,
                "feedback": f"ğŸ“ ì§ˆë¬¸ ì˜ë„: {qa.question_intent}\n\nğŸ’¬ í‰ê°€: AI ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.\n\nğŸ”§ ê°œì„  ë°©ë²•: ë” êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì´ í•„ìš”í•©ë‹ˆë‹¤."
            }
        
        prompt = f"""
ë‹¤ìŒ AI ì§€ì›ìì˜ ë©´ì ‘ ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

=== ì§ˆë¬¸ ì •ë³´ ===
ì§ˆë¬¸ ìœ í˜•: {qa.question_type.value}
ì§ˆë¬¸: {qa.question_content}
ì§ˆë¬¸ ì˜ë„: {qa.question_intent}

=== AI ì§€ì›ì ë‹µë³€ ===
{answer}

=== í‰ê°€ ê¸°ì¤€ ===
- 65-75ì : AI ë‹µë³€ì˜ ê¸°ë³¸ í’ˆì§ˆ ë²”ìœ„
- 75-85ì : êµ¬ì²´ì ì´ê³  ì¸ìƒì ì¸ ë‹µë³€
- 85-95ì : ë§¤ìš° ìš°ìˆ˜í•œ ë‹µë³€
- 95-100ì : ì™„ë²½ì— ê°€ê¹Œìš´ ë‹µë³€

í‰ê°€ ìš”ì†Œ:
1. ì§ˆë¬¸ ì˜ë„ ì´í•´ë„
2. ë‹µë³€ì˜ êµ¬ì²´ì„±ê³¼ ì‚¬ì‹¤ì„±
3. ë…¼ë¦¬ì  êµ¬ì„±
4. ì „ë¬¸ì„±ê³¼ ê¹Šì´
5. ì¼ê´€ì„±

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "score": ì ìˆ˜,
  "feedback": "ğŸ“ ì§ˆë¬¸ ì˜ë„: {qa.question_intent}\\n\\nğŸ’¬ í‰ê°€: êµ¬ì²´ì ì¸ í‰ê°€ ë‚´ìš©\\n\\nğŸ”§ ê°œì„  ë°©ë²•: ì‹¤ì§ˆì ì¸ ê°œì„  ì œì•ˆ"
}}
"""
        
        try:
            response = self.llm_manager.generate_response(
                LLMProvider.OPENAI_GPT4O_MINI,
                prompt,
                "ë‹¹ì‹ ì€ AI ì§€ì›ì ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
            )
            
            result = response.content.strip()
            
            # JSON íŒŒì‹±
            start_idx = result.find('{')
            end_idx = result.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = result[start_idx:end_idx]
                evaluation = json.loads(json_str)
                
                # AI ë‹µë³€ì€ ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ í’ˆì§ˆì´ë¯€ë¡œ ê¸°ë³¸ ì ìˆ˜ ì¡°ì •
                score = max(evaluation["score"], 65)  # ìµœì†Œ 65ì 
                evaluation["score"] = score
                
                return evaluation
            else:
                raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            print(f"AI ë‹µë³€ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ê¸°ë³¸ ì ìˆ˜ (AIëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ë‹µë³€ì„ ìƒì„±)
            return {
                "score": 75,
                "feedback": f"ğŸ“ ì§ˆë¬¸ ì˜ë„: {qa.question_intent}\n\nğŸ’¬ í‰ê°€: AI ë‹µë³€ì´ ì ì ˆí•©ë‹ˆë‹¤.\n\nğŸ”§ ê°œì„  ë°©ë²•: ë” êµ¬ì²´ì ì¸ ê²½í—˜ê³¼ ì‚¬ë¡€ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            }
    
    def _generate_ai_overall_evaluation(self, ai_session: InterviewSession, company_data: Dict[str, Any], overall_score: int) -> Dict[str, Any]:
        """AI ì§€ì›ì ì¢…í•© í‰ê°€ ìƒì„±"""
        
        conversation_summary = ""
        for qa in ai_session.question_answers:
            conversation_summary += f"[{qa.question_type.value}] {qa.question_content}\në‹µë³€: {qa.answer_content}\nê°œë³„ ì ìˆ˜: {qa.individual_score}ì \n\n"
        
        # AICandidateSession ì†ì„± ì•ˆì „ ì ‘ê·¼
        candidate_name = getattr(ai_session, 'persona', None)
        candidate_name = candidate_name.name if candidate_name else ai_session.candidate_name
        
        prompt = f"""
{company_data.get('name', '')} {ai_session.position} AI ì§€ì›ì ì¢…í•© í‰ê°€ë¥¼ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.

=== AI ì§€ì›ì ì •ë³´ ===
- ì´ë¦„: {candidate_name}
- ì§€ì› ì§êµ°: {ai_session.position}
- ì „ì²´ í‰ê·  ì ìˆ˜: {overall_score}ì 
- í˜ë¥´ì†Œë‚˜ ìœ í˜•: AI ì§€ì›ì

=== ë©´ì ‘ ë‚´ìš© ===
{conversation_summary}

=== ê¸°ì—… ìš”êµ¬ì‚¬í•­ ===
- ì¸ì¬ìƒ: {company_data.get('talent_profile', '')}
- í•µì‹¬ ì—­ëŸ‰: {', '.join(company_data.get('core_competencies', []))}

AI ì§€ì›ìì˜ ë‹µë³€ í’ˆì§ˆê³¼ ì¼ê´€ì„±ì„ í‰ê°€í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "strengths": ["êµ¬ì²´ì ì¸ ê°•ì 1", "êµ¬ì²´ì ì¸ ê°•ì 2", "êµ¬ì²´ì ì¸ ê°•ì 3"],
  "improvements": ["êµ¬ì²´ì ì¸ ê°œì„ ì 1", "êµ¬ì²´ì ì¸ ê°œì„ ì 2", "êµ¬ì²´ì ì¸ ê°œì„ ì 3"],
  "recommendation": "AI ì§€ì›ì ì„±ëŠ¥ í‰ê°€",
  "next_steps": "ì‹¤ì œ ë©´ì ‘ ì¤€ë¹„ ë‹¨ê³„ ì œì•ˆ",
  "overall_assessment": "AI ì§€ì›ìì˜ ì „ì²´ì ì¸ ì„±ëŠ¥ í‰ê°€"
}}
"""
        
        try:
            response = self.llm_manager.generate_response(
                LLMProvider.OPENAI_GPT4O_MINI,
                prompt,
                f"{company_data.get('name', '')} AI ì§€ì›ì ë©´ì ‘ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
            )
            
            result = response.content.strip()
            
            start_idx = result.find('{')
            end_idx = result.rfind('}') + 1
            
            if start_idx != -1 and end_idx != -1:
                json_str = result[start_idx:end_idx]
                return json.loads(json_str)
            
        except Exception as e:
            print(f"AI ì¢…í•© í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # ê¸°ë³¸ í‰ê°€ (AIëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥)
        return {
            "strengths": ["ì¼ê´€ëœ ë‹µë³€", "ë…¼ë¦¬ì  êµ¬ì„±", "ì „ë¬¸ì  í‘œí˜„"],
            "improvements": ["ê°œì¸ ê²½í—˜ êµ¬ì²´í™”", "ê°ì •ì  í‘œí˜„", "ì°½ì˜ì„± í–¥ìƒ"],
            "recommendation": f"AI ì§€ì›ì ì„±ëŠ¥: {overall_score}ì  ìˆ˜ì¤€",
            "next_steps": "ì‹¤ì œ ë©´ì ‘ ì¤€ë¹„ ì‹œ ì°¸ê³  ìë£Œë¡œ í™œìš©",
            "overall_assessment": f"AI ì§€ì›ìê°€ {overall_score}ì  ìˆ˜ì¤€ì˜ ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤."
        }

