"""
ì‹¤ì‹œê°„ ë©´ì ‘ í‰ê°€ ì‹œìŠ¤í…œ - ìµœì¢… í‰ê°€ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ realtime_result.jsonì— ëˆ„ì ëœ ê°œë³„ í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ
ìµœì¢… í†µí•© í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì—¬ final_evaluation_results.jsonì„ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ì‹¤ì‹œê°„ ê²°ê³¼ë¥¼ ìµœì¢… í‰ê°€ í˜•íƒœë¡œ ë³€í™˜ (MLì ìˆ˜ + LLMí‰ê°€ â†’ í†µí•© ì ìˆ˜)
2. ê°œë³„ ì§ˆë¬¸ë³„ ìµœì¢… ì ìˆ˜, ì˜ë„, í‰ê°€, ê°œì„ ì‚¬í•­ ìƒì„±
3. ì „ì²´ ë©´ì ‘ì— ëŒ€í•œ ì¢…í•© í‰ê°€ ë° ì ìˆ˜ ì‚°ì¶œ

ì‘ì„±ì: AI Assistant  
"""

from openai import OpenAI
import json
import re
import os
import datetime
import statistics
from .num_eval import score_interview_data, load_interview_data, load_encoder, load_model
from .text_eval import evaluate_all

client = OpenAI()

# ìµœì¢… í‰ê°€ìš© ëª¨ë¸ ì„¤ì • (ê°œë³„ í‰ê°€ì™€ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)
FINAL_EVAL_MODEL = "gpt-4o"  # ë˜ëŠ” "gpt-4", "gpt-3.5-turbo", "claude-3-sonnet" ë“±
OVERALL_EVAL_MODEL = "gpt-4o"  # ì „ì²´ ì¢…í•© í‰ê°€ìš© ëª¨ë¸ (ë‹¤ì‹œ ë‹¤ë¥¸ ëª¨ë¸ ê°€ëŠ¥)

# === í•¸ìˆ˜ ì •ì˜ ì„¹ì…˜ ===

def build_final_prompt(q, a, ml_score, llm_feedback, existing_intent, company_info=None, position_info=None, posting_info=None, resume_info=None):
    """
    ê°œë³„ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… í†µí•© í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
    
    Args:
        q (str): ë©´ì ‘ ì§ˆë¬¸
        a (str): ì§€ì›ì ë‹µë³€
        ml_score (float): ML ëª¨ë¸ ì˜ˆì¸¡ ì ìˆ˜
        llm_feedback (str): LLMì—ì„œ ìƒì„±ëœ ìƒì„¸ í‰ê°€ ê²°ê³¼
        existing_intent (str): text_eval.pyì—ì„œ ì´ë¯¸ ì¶”ì¶œëœ ì§ˆë¬¸ ì˜ë„
        company_info (dict): íšŒì‚¬ ì •ë³´
        position_info (dict): ì§êµ° ì •ë³´
        posting_info (dict): ê³µê³  ì •ë³´
        resume_info (dict): ì´ë ¥ì„œ ì •ë³´
        
    Returns:
        str: GPT-4oì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
    """
    # ì¶”ê°€ ì •ë³´ ì„¹ì…˜ë“¤ êµ¬ì„±
    additional_context = ""
    
    if company_info:
        additional_context += f"""
[íšŒì‚¬ ì •ë³´]:
- íšŒì‚¬ëª…: {company_info.get('name', 'N/A')}
- ì¸ì¬ìƒ: {company_info.get('talent_profile', 'N/A')}
- í•µì‹¬ì—­ëŸ‰: {', '.join(company_info.get('core_competencies', []))}
- ê¸°ìˆ  ì¤‘ì : {', '.join(company_info.get('tech_focus', []))}
"""
    
    if position_info:
        additional_context += f"""
[ğŸ’¼ ì§€ì› ì§êµ° ì •ë³´]:
ì§€ì›ìê°€ ì§€ì›í•œ ì§êµ°: "{position_info.get('position_name', 'N/A')}"
â†’ í•´ë‹¹ ì§êµ°ì˜ ì „ë¬¸ì„±ê³¼ ìš”êµ¬ì‚¬í•­ì„ ê³ ë ¤í•˜ì—¬ í‰ê°€í•˜ì„¸ìš”.
"""
    
    if posting_info:
        content = posting_info.get('content', 'N/A')
        if len(content) > 250:
            content = content[:250] + "..."
        additional_context += f"""
[ğŸ“¢ ì±„ìš© ê³µê³  ì •ë³´]:
ê³µê³  ì£¼ìš” ë‚´ìš©: {content}
â†’ ê³µê³  ìš”êµ¬ì‚¬í•­ê³¼ ì§€ì›ì ë‹µë³€ì˜ ì í•©ì„±ì„ í‰ê°€í•˜ì„¸ìš”.
"""
    
    if resume_info:
        resume_type = "AI ìƒì„± ì´ë ¥ì„œ" if 'ai_resume_id' in resume_info else "ì§€ì›ì ì œì¶œ ì´ë ¥ì„œ"
        career = resume_info.get('career', 'N/A')
        if len(career) > 150:
            career = career[:150] + "..."
        additional_context += f"""
[ğŸ“„ {resume_type} ì •ë³´]:
- í•™ë ¥: {resume_info.get('academic_record', 'N/A')}
- ê²½ë ¥: {career}
- ê¸°ìˆ : {resume_info.get('tech', 'N/A')}
â†’ ì´ë ¥ì„œ ë‚´ìš©ê³¼ ë‹µë³€ì˜ ì¼ê´€ì„±ì„ í‰ê°€í•˜ì„¸ìš”.
"""

    return fr"""
[ì§ˆë¬¸]: {q}
[ë‹µë³€]: {a}
[ì§ˆë¬¸ ì˜ë„]: {existing_intent}
[ë¨¸ì‹ ëŸ¬ë‹ ì ìˆ˜]: {ml_score:.1f} (ML ëª¨ë¸ ì˜ˆì¸¡ ì ìˆ˜, ì¼ë°˜ì  ë²”ìœ„: 10~50ì )
[LLM í‰ê°€ê²°ê³¼]: {llm_feedback}
{additional_context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íšŒì‚¬, ì§êµ°, ê³µê³ , ì´ë ¥ì„œ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ì•„ë˜ í•­ëª©ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ì¤‘ìš” ì§€ì¹¨**: 
- LLM í‰ê°€ê²°ê³¼ì— ì´ë¯¸ ë°˜ë§ ê°ì§€ ë° ì ì ˆí•œ ê°ì ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤
- ë…ìì ìœ¼ë¡œ ì–¸ì–´ ì‚¬ìš©ì´ë‚˜ ì˜ˆì˜ ê´€ë ¨ íŒë‹¨ì„ í•˜ì§€ ë§ˆì„¸ìš”
- ì œê³µëœ LLM í‰ê°€ê²°ê³¼ë¥¼ ì‹ ë¢°í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© í‰ê°€í•˜ì„¸ìš”

1. ğŸ’¬ í‰ê°€: ë‹µë³€ì˜ ê°•ì ê³¼ ì•½ì ì„ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. í•µì‹¬ ìš”ì†Œê°€ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ë¶€ì¡±í•œ ê²½ìš° ë¶„ëª…íˆ ì§€ì í•´ì£¼ì„¸ìš”. í‰ê°€ëŠ” ì „ë¬¸ì ì´ë©´ì„œë„ ì¼ê´€ëœ ì–´ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

2. ğŸ”§ ê°œì„  ë°©ë²•: ë©´ì ‘ìê°€ ë‹µë³€ì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ë©´ ì¢‹ì„ì§€ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”. ê°œì„  ì œì•ˆì€ ê±´ì„¤ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

3. [ìµœì¢… ì ìˆ˜]: 100ì  ë§Œì  ê¸°ì¤€ìœ¼ë¡œ ì •ìˆ˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì£¼ì„¸ìš”. ì ìˆ˜ë¥¼ í›„í•˜ê²Œ ì£¼ì§€ ë§ê³  ëƒ‰ì •í•˜ê²Œ íŒë‹¨í•´ì£¼ì„¸ìš”.
"""

def call_llm(prompt):
    """
    OpenAI GPT-4oë¥¼ í˜¸ì¶œí•˜ì—¬ í‰ê°€ ê²°ê³¼ ìƒì„±
    
    Args:
        prompt (str): GPT-4oì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
        
    Returns:
        str: GPT-4oì˜ ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì¸ì‚¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤. ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì ì´ê³  ì¼ê´€ëœ í‰ê°€ë¥¼ ìˆ˜í–‰í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ì¶œë ¥ í˜•ì‹(1. ğŸ’¬ í‰ê°€, 2. ğŸ”§ ê°œì„  ë°©ë²•, 3. [ìµœì¢… ì ìˆ˜])ì„ ì§€ì¼œì£¼ì„¸ìš”. ë…ìì ìœ¼ë¡œ ì–¸ì–´ ì‚¬ìš©ì´ë‚˜ ì˜ˆì˜ë¥¼ íŒë‹¨í•˜ì§€ ë§ê³ , ì´ë¯¸ ì²˜ë¦¬ëœ LLM í‰ê°€ê²°ê³¼ë¥¼ ì‹ ë¢°í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© í‰ê°€ë§Œ ìˆ˜í–‰í•˜ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1
    )
    return response.choices[0].message.content.strip()

def call_llm_with_ensemble(prompt, num_evaluations=3):
    """
    3ë²ˆ í‰ê°€ í›„ ì•™ìƒë¸”ë¡œ ìµœì¢… ê²°ê³¼ ìƒì„±
    
    Args:
        prompt (str): GPT-4oì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
        num_evaluations (int): í‰ê°€ íšŸìˆ˜ (ê¸°ë³¸ 3íšŒ)
        
    Returns:
        dict: {"result": str, "confidence": float, "scores": list}
    """
    print(f"ğŸ”„ ì•™ìƒë¸” í‰ê°€ ì‹œì‘ ({num_evaluations}íšŒ)")
    
    evaluations = []
    scores = []
    
    # ë‹¤ì¤‘ í‰ê°€ ì‹¤í–‰
    for i in range(num_evaluations):
        try:
            result = call_llm(prompt)
            evaluations.append(result)
            
            # ì ìˆ˜ ì¶”ì¶œ (ê°•í™”ëœ íŒ¨í„´ - ë” ë§ì€ ê²½ìš°ì˜ ìˆ˜ ì²˜ë¦¬)
            score_match = (
                # ê¸°ë³¸ í˜•ì‹ë“¤
                re.search(r'3\.\s*\[ìµœì¢…\s*ì ìˆ˜\]:\s*(\d+)', result, re.IGNORECASE) or
                re.search(r'\[ìµœì¢…\s*ì ìˆ˜\]:\s*(\d+)', result, re.IGNORECASE) or
                re.search(r'ìµœì¢…\s*ì ìˆ˜\s*:\s*(\d+)', result, re.IGNORECASE) or
                re.search(r'final\s*score\s*:\s*(\d+)', result, re.IGNORECASE) or
                # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°ë“¤
                re.search(r'ì ìˆ˜:\s*(\d+)', result) or
                re.search(r'ì´ì :\s*(\d+)', result) or
                re.search(r'ì \s*:\s*(\d+)', result) or
                re.search(r'(\d+)\s*ì ', result) or
                # ë§ˆì§€ë§‰ í´ë°±: 0-100 ì‚¬ì´ ìˆ«ì
                re.search(r'\b([0-9]{1,2}|100)\b(?=\s*[ì ë¶„/]|$)', result)
            )
            if score_match:
                score = int(score_match.group(1))
                scores.append(score)
                print(f"  í‰ê°€ {i+1}: {score}ì ")
            else:
                print(f"  í‰ê°€ {i+1}: ì ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"  í‰ê°€ {i+1} ì‹¤íŒ¨: {e}")
            continue
    
    if not evaluations:
        print("âŒ ëª¨ë“  í‰ê°€ ì‹¤íŒ¨")
        return {"result": "í‰ê°€ ì‹¤íŒ¨", "confidence": 0.0, "scores": []}
    
    # ì ìˆ˜ ì•ˆì •í™”
    if scores:
        final_score = int(round(statistics.median(scores)))  # ì¤‘ì•™ê°’ìœ¼ë¡œ ë³€ê²½í•˜ê³  ì •ìˆ˜ ë³´ì¥
        score_variance = statistics.variance(scores) if len(scores) > 1 else 0.0
        confidence = max(0.0, min(1.0, 1.0 - score_variance / 100.0))
        
        # ì¤‘ì•™ê°’ì— ê°€ì¥ ê°€ê¹Œìš´ í‰ê°€ ì„ íƒ
        best_idx = min(range(len(scores)), key=lambda i: abs(scores[i] - final_score))
        best_evaluation = evaluations[best_idx]
        
        # ì ìˆ˜ë¥¼ ìµœì¢… ì ìˆ˜ë¡œ êµì²´
        final_result = re.sub(
            r'\[ìµœì¢… ì ìˆ˜\]:\s*\d+', 
            f'[ìµœì¢… ì ìˆ˜]: {final_score}', 
            best_evaluation
        )
        
        print(f"âœ… ìµœì¢… ì ìˆ˜: {final_score}ì  (ì‹ ë¢°ë„: {confidence:.2f})")
        print(f"   ì ìˆ˜ ë¶„í¬: {scores} â†’ ì¤‘ì•™ê°’: {final_score}")
        
    else:
        # ì ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë°œìƒ
        print("âŒ ì ìˆ˜ ì¶”ì¶œ ì™„ì „ ì‹¤íŒ¨")
        raise ValueError(f"ì ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: ëª¨ë“  í‰ê°€ì—ì„œ ì ìˆ˜ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‰ê°€ ê²°ê³¼: {evaluations[:100] if evaluations else 'None'}...")
    
    return {
        "result": final_result,
        "confidence": confidence,
        "scores": scores,
        "final_score": final_score
    }

def parse_llm_result(llm_result):
    """
    GPT-4oì˜ ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
    
    Args:
        llm_result (str): GPT-4oì˜ ì›ì‹œ ì‘ë‹µ í…ìŠ¤íŠ¸
        
    Returns:
        tuple: (ìµœì¢…ì ìˆ˜, í‰ê°€ë‚´ìš©, ê°œì„ ë°©ì•ˆ)
    """
    eval_match = re.search(r"1\. ğŸ’¬ í‰ê°€:\s*(.+?)(?:\n2\.|$)", llm_result, re.DOTALL)
    improve_match = re.search(r"2\. ğŸ”§ ê°œì„  ë°©ë²•:\s*(.+?)(?:\n3\.|$)", llm_result, re.DOTALL)
    # ì ìˆ˜ ì¶”ì¶œ (ê°•í™”ëœ íŒ¨í„´)
    score_match = (
        re.search(r"3\.\s*\[ìµœì¢…\s*ì ìˆ˜\]:\s*(\d+)", llm_result, re.IGNORECASE) or
        re.search(r"\[ìµœì¢…\s*ì ìˆ˜\]:\s*(\d+)", llm_result, re.IGNORECASE) or
        re.search(r"ìµœì¢…\s*ì ìˆ˜\s*:\s*(\d+)", llm_result, re.IGNORECASE) or
        re.search(r"(\d+)\s*ì ", llm_result) or
        re.search(r"\b([0-9]{1,2}|100)\b(?=\s*[ì ë¶„/]|$)", llm_result)
    )

    evaluation = eval_match.group(1).strip() if eval_match else "í‰ê°€ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    improvement = improve_match.group(1).strip() if improve_match else "ê°œì„  ë°©ë²•ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ì ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë°œìƒ
    if not score_match:
        raise ValueError(f"ì ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: LLM ì‘ë‹µì—ì„œ ì ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ ë‚´ìš©: {llm_result[:200]}...")
    
    score = int(score_match.group(1))

    return score, evaluation, improvement



def build_overall_prompt(final_results):
    per_q = ""
    for i, item in enumerate(final_results, 1):
        per_q += f"{i}. ì§ˆë¬¸: {item['question']}\n   ë‹µë³€: {item['answer']}\n   ì ìˆ˜: {item['final_score']}\n   í‰ê°€: {item['evaluation']}\n   ê°œì„ ì‚¬í•­: {item['improvement']}\n\n"
    return fr"""
[ì „ì²´ ë‹µë³€ í‰ê°€]
ì•„ë˜ëŠ” ì§€ì›ìì˜ ê° ë¬¸í•­ë³„ ë‹µë³€, ì ìˆ˜, í‰ê°€, ê°œì„ ì‚¬í•­ì…ë‹ˆë‹¤.

{per_q}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•´ ì§€ì›ìì— ëŒ€í•´
- ìµœì¢… ì ìˆ˜(100ì  ë§Œì , ì •ìˆ˜)
- ì „ì²´ í”¼ë“œë°±(5~10ë¬¸ì¥, êµ¬ì²´ì ì´ê³  ê¸¸ê²Œ, ì „ë¬¸ì ì´ê³  ì¼ê´€ëœ ì–´ì¡°ë¡œ)
- 1ì¤„ ìš”ì•½(í•œ ë¬¸ì¥, ì„íŒ©íŠ¸ ìˆê²Œ)
ë¥¼ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

[ìµœì¢… ì ìˆ˜]: XX
[ì „ì²´ í”¼ë“œë°±]: ...
[1ì¤„ ìš”ì•½]: ...
"""

def parse_overall_llm_result(llm_result):
    # ì ìˆ˜ ì¶”ì¶œ (ê°•í™”ëœ íŒ¨í„´)
    score_match = (
        re.search(r"\[ìµœì¢…\s*ì ìˆ˜\]:\s*(\d+)", llm_result, re.IGNORECASE) or
        re.search(r"ìµœì¢…\s*ì ìˆ˜\s*:\s*(\d+)", llm_result, re.IGNORECASE) or
        re.search(r"ì´ì \s*:\s*(\d+)", llm_result) or
        re.search(r"(\d+)\s*ì ", llm_result) or
        re.search(r"\b([0-9]{1,2}|100)\b(?=\s*[ì ë¶„/]|$)", llm_result)
    )
    feedback_match = re.search(r"\[ì „ì²´\s*í”¼ë“œë°±\]:\s*(.+?)(?:\n\[|$)", llm_result, re.DOTALL | re.IGNORECASE)
    summary_match = re.search(r"\[1ì¤„\s*ìš”ì•½\]:\s*(.+)", llm_result, re.IGNORECASE)
    score = int(score_match.group(1)) if score_match else None
    feedback = feedback_match.group(1).strip() if feedback_match else ""
    summary = summary_match.group(1).strip() if summary_match else ""
    return score, feedback, summary


def process_realtime_results(realtime_data, company_info, position_info=None, posting_info=None, resume_info=None):
    """
    ì‹¤ì‹œê°„ í‰ê°€ ê²°ê³¼ë¥¼ ìµœì¢… í‰ê°€ í˜•íƒœë¡œ ë³€í™˜
    
    Args:
        realtime_data (list): ì‹¤ì‹œê°„ ê²°ê³¼ ë°ì´í„°
        company_info (dict): DBì—ì„œ ê°€ì ¸ì˜¨ íšŒì‚¬ ì •ë³´
        position_info (dict): ì§êµ° ì •ë³´
        posting_info (dict): ê³µê³  ì •ë³´
        resume_info (dict): ì´ë ¥ì„œ ì •ë³´
        
    Returns:
        list: ìµœì¢… í‰ê°€ í˜•íƒœë¡œ ë³€í™˜ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    
    final_results = []
    
    # ê° ì§ˆë¬¸ì— ëŒ€í•´ ìµœì¢… í†µí•© í‰ê°€ ìˆ˜í–‰
    for item in realtime_data:
        question = item["question"]
        answer = item["answer"]
        intent = item.get("intent", "")
        ml_score = item.get("ml_score", 0)  # num_eval.pyì—ì„œ ìƒì„±ëœ ì ìˆ˜
        llm_evaluation = item.get("llm_evaluation", "")  # text_eval.pyì—ì„œ ìƒì„±ëœ í‰ê°€
        
        # ML ì ìˆ˜ì™€ LLM í‰ê°€ë¥¼ ê²°í•©í•œ ìµœì¢… í†µí•© í‰ê°€ (ì•™ìƒë¸” ì ìš©)
        final_prompt = build_final_prompt(question, answer, ml_score, llm_evaluation, intent, company_info, position_info, posting_info, resume_info)
        ensemble_result = call_llm_with_ensemble(final_prompt)
        
        # ê²°ê³¼ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
        final_score, evaluation, improvement = parse_llm_result(ensemble_result["result"])
        
        # ìµœì¢… ê²°ê³¼ í˜•íƒœë¡œ êµ¬ì„±
        final_results.append({
            "question": question,
            "answer": answer,
            "intent": intent,  # text_eval.pyì—ì„œ ì´ë¯¸ ì¶”ì¶œëœ ì˜ë„ ì‚¬ìš©
            "final_score": final_score,
            "evaluation": evaluation,
            "improvement": improvement
        })
    
    return final_results

def run_final_evaluation_from_realtime(realtime_data=None, company_info=None, position_info=None, posting_info=None, resume_info=None, realtime_file="realtime_result.json", output_file="final_evaluation_results.json"):
    """
    ì‹¤ì‹œê°„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… í‰ê°€ ì‹¤í–‰
    
    Args:
        realtime_data (list): ì‹¤ì‹œê°„ ê²°ê³¼ ë°ì´í„° (ìš°ì„ ìˆœìœ„)
        company_info (dict): DBì—ì„œ ê°€ì ¸ì˜¨ íšŒì‚¬ ì •ë³´ (ìš°ì„ ìˆœìœ„)
        position_info (dict): ì§êµ° ì •ë³´
        posting_info (dict): ê³µê³  ì •ë³´
        resume_info (dict): ì´ë ¥ì„œ ì •ë³´
        realtime_file (str): ì‹¤ì‹œê°„ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ (í´ë°±)
        output_file (str): ìµœì¢… ê²°ê³¼ ì €ì¥ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        dict: ìµœì¢… í‰ê°€ ê²°ê³¼ (ê°œë³„+ì „ì²´)
    """
    print("ìµœì¢… í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
    
    # 1. ë°ì´í„° ê²€ì¦ (í•„ìˆ˜ ë°ì´í„°)
    if realtime_data is None:
        raise ValueError("realtime_dataëŠ” í•„ìˆ˜ íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ë©”ëª¨ë¦¬ì—ì„œ ì „ë‹¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
    
    if company_info is None:
        raise ValueError("company_infoëŠ” í•„ìˆ˜ íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. DBì—ì„œ ì¡°íšŒëœ ë°ì´í„°ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    # 2. ì‹¤ì‹œê°„ ê²°ê³¼ë¥¼ ìµœì¢… í‰ê°€ í˜•íƒœë¡œ ë³€í™˜
    #    (MLì ìˆ˜ + LLMí‰ê°€ â†’ í†µí•©ì ìˆ˜ + ìƒì„¸í‰ê°€)
    per_question = process_realtime_results(realtime_data, company_info, position_info, posting_info, resume_info)
    
    # 3. ì „ì²´ ë©´ì ‘ì— ëŒ€í•œ ì¢…í•© í‰ê°€ ìˆ˜í–‰ (ì•™ìƒë¸” ì ìš©)
    overall_prompt = build_overall_prompt(per_question)
    overall_ensemble_result = call_llm_with_ensemble(overall_prompt)
    overall_score, overall_feedback, summary = parse_overall_llm_result(overall_ensemble_result["result"])
    
    # 4. ìµœì¢… ê²°ê³¼ êµ¬ì„± (ê¸°ì¡´ í¬ë§·ê³¼ ë™ì¼)
    final_results = {
        "success": True,                   # ì„±ê³µ í”Œë˜ê·¸ ì¶”ê°€
        "per_question": per_question,      # ê°œë³„ ì§ˆë¬¸ í‰ê°€ ê²°ê³¼
        "overall_score": overall_score,    # ì „ì²´ ì ìˆ˜
        "overall_feedback": overall_feedback,  # ì „ì²´ í”¼ë“œë°±
        "summary": summary                 # 1ì¤„ ìš”ì•½
    }
    
    # 5. JSON íŒŒì¼ë¡œ ì €ì¥ (output_fileì´ ì œê³µëœ ê²½ìš°ì—ë§Œ)
    if output_file:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        print(f"ê²°ê³¼ëŠ” '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    print(f"ìµœì¢… í‰ê°€ ì™„ë£Œ! ì ìˆ˜: {overall_score}/100")
    
    return final_results

# ëª¨ë“ˆ íŒŒì¼ - APIë¥¼ í†µí•´ run_final_evaluation_from_realtime() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”