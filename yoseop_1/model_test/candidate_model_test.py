#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple Model Test
- Question Generation -> Answer Generation -> RAGAS Evaluation
"""

import os
import sys
import logging
import openai
import pandas as pd
import random
from datetime import datetime
from typing import Dict, List
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import answer_relevancy, answer_correctness
from tqdm import tqdm
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Suppress all logs except CRITICAL
logging.getLogger().setLevel(logging.CRITICAL)
logging.getLogger("httpx").setLevel(logging.CRITICAL)
logging.getLogger("backend.services.existing_tables_service").setLevel(logging.CRITICAL)
logging.getLogger("llm.shared.utils").setLevel(logging.CRITICAL)

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Import existing models
from llm.interviewer.question_generator import QuestionGenerator  
from llm.candidate.model import AICandidateModel
from llm.candidate.quality_controller import QualityLevel
from llm.shared.models import AnswerRequest, QuestionType, LLMProvider

# Available options
COMPANIES = {
    "1": ("naver", "ë„¤ì´ë²„"),
    "2": ("kakao", "ì¹´ì¹´ì˜¤"),
    "3": ("toss", "í† ìŠ¤"),
    "4": ("coupang", "ì¿ íŒ¡"),
    "5": ("baemin", "ë°°ë‹¬ì˜ë¯¼ì¡±"),
    "6": ("daangn", "ë‹¹ê·¼ë§ˆì¼“"),
    "7": ("line", "ë¼ì¸")
}

POSITIONS = {
    "1": ("backend", "ë°±ì—”ë“œ"),                    # POSITION_MAPPING: "backend": 2
    "2": ("frontend", "í”„ë¡ íŠ¸ì—”ë“œ"),                # POSITION_MAPPING: "frontend": 1  
    "3": ("ai", "AI/ML"),                         # POSITION_MAPPING: "ai": 4
    "4": ("ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤", "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤"),        # POSITION_MAPPING: "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤": 5 
    "5": ("ê¸°íš", "ê¸°íš")                         # POSITION_MAPPING: "ê¸°íš": 3
}

INTERVIEWER_ROLES = {
    "1": ("HR", "ì¸ì‚¬"),
    "2": ("TECH", "ê¸°ìˆ "),
    "3": ("COLLABORATION", "í˜‘ì—…")
}

def generate_ground_truth_answer(question: str, company_id: str, position: str, interviewer_role: str, persona_resume: Dict) -> str:
    """ë©´ì ‘ê´€ ê´€ì ì—ì„œ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëª¨ë²”ë‹µì•ˆ ìƒì„±"""
    # API í‚¤ í™•ì¸
    openai_api_key = os.getenv('OPENAI_API_KEY')
    if not openai_api_key:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return "ëª¨ë²”ë‹µì•ˆ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (API Key ì—†ìŒ)"
    
    client = openai.OpenAI(api_key=openai_api_key)
    
    # Convert resume dict to a readable string for the prompt
    resume_summary = f"""ì§€ì›ì ì •ë³´:
- ì´ë¦„: {persona_resume.get('name', 'Unknown')}
- ê²½ë ¥: {persona_resume.get('background', {}).get('career_years', 'N/A')}ë…„
- ì£¼ìš” ê¸°ìˆ : {', '.join(persona_resume.get('technical_skills', [])[:5])}
- ê°•ì : {', '.join(persona_resume.get('strengths', [])[:3])}
- ëª©í‘œ: {persona_resume.get('career_goal', 'N/A')}"""
    
    system_prompt = f"""ë‹¹ì‹ ì€ {company_id} íšŒì‚¬ì˜ ê²½í—˜ ë§ì€ {interviewer_role} ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
{position} ì§êµ° ì§€ì›ìì—ê²Œ í•œ ì§ˆë¬¸ì— ëŒ€í•´, ì•„ë˜ ì œê³µëœ ì§€ì›ìì˜ ì´ë ¥ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì´ìƒì ì¸ ëª¨ë²”ë‹µì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

{resume_summary}

ëª¨ë²”ë‹µì•ˆ ì‘ì„± ê°€ì´ë“œ:
- ì œê³µëœ ì§€ì›ìì˜ ì´ë ¥ê³¼ ê²½í—˜ì— ê¸°ë°˜í•œ ë‹µë³€
- ì§€ì›ìì˜ ê¸°ìˆ  ìŠ¤íƒê³¼ ê°•ì ì„ í™œìš©í•œ êµ¬ì²´ì ì¸ ì˜ˆì‹œ í¬í•¨
- ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ êµ¬ì¡°
- ì ì ˆí•œ ê¸¸ì´ (150-250ë‹¨ì–´)
- ì§€ì›ìì˜ ëª©í‘œì™€ ì—°ê²°ëœ ë‹µë³€"""

    user_prompt = f"""ì§ˆë¬¸: {question}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ì§€ì›ì ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë²”ë‹µì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 
ì§€ì›ìì˜ ì‹¤ì œ ê²½í—˜ê³¼ ê¸°ìˆ ì„ í™œìš©í•œ í˜„ì‹¤ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ë‹µë³€ì´ì–´ì•¼ í•©ë‹ˆë‹¤."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=400,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ëª¨ë²”ë‹µì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
        return "ëª¨ë²”ë‹µì•ˆ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

def evaluate_with_ragas(question: str, ground_truth: str, candidate_answers: List[Dict]) -> Dict:
    """RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë‹µë³€ í‰ê°€ - ragas_rerun.py ë°©ì‹ ì™„ì „ ì ìš©"""
    try:
        from langchain_openai import ChatOpenAI
        
        # OpenAI API í‚¤ í™•ì¸
        openai_api_key = os.getenv('OPENAI_API_KEY')
        if not openai_api_key:
            return {"error": "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        # OpenAI LLM ì„¤ì • (ragas_rerun.pyì™€ ë™ì¼)
        llm = ChatOpenAI(model="gpt-4o", api_key=openai_api_key)
        
        # 3ê°œ ë‹µë³€ ìˆ˜ì§‘ (ragas_rerun.pyì™€ ë™ì¼í•œ ìœ íš¨ì„± ê²€ì‚¬)
        valid_answers = []
        for ans in candidate_answers:
            content = ans["content"]
            if content and str(content).strip() and not str(content).startswith("ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨"):
                valid_answers.append(str(content).strip())
            else:
                valid_answers.append("ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # RAGAS Dataset ìƒì„± (ragas_rerun.pyì™€ ë™ì¼)
        data = {
            "question": [question] * 3,
            "contexts": [["ë©´ì ‘ ìƒí™©"]] * 3,
            "answer": valid_answers,
            "ground_truth": [ground_truth] * 3
        }
        
        dataset = Dataset.from_dict(data)
        
        # RAGAS í‰ê°€ ì‹¤í–‰ (ragas_rerun.pyì™€ ë™ì¼)
        result = evaluate(
            dataset,
            metrics=[answer_relevancy, answer_correctness],
            llm=llm
        )
        
        # ì ìˆ˜ ì¶”ì¶œ - ragas_rerun.pyì˜ evaluate_single_row_with_ragas í•¨ìˆ˜ì™€ ë™ì¼í•œ ë¡œì§
        scores = None
        
        # ë°©ë²• 1: scores ì†ì„± ì ‘ê·¼
        if hasattr(result, 'scores'):
            scores = result.scores
        # ë°©ë²• 2: dict ë³€í™˜ ì‹œë„  
        elif hasattr(result, '__getitem__'):
            try:
                scores = dict(result)
            except:
                pass
        # ë°©ë²• 3: ì§ì ‘ í• ë‹¹
        else:
            scores = result
        
        # RAGAS ê²°ê³¼ê°€ list í˜•íƒœì¸ ê²½ìš° ì²˜ë¦¬ (ragas_rerun.pyì—ì„œ ì¶”ê°€ëœ ë¡œì§)
        if scores is not None and isinstance(scores, list) and len(scores) >= 3:
            # ê° ë‹µë³€ë³„ ì ìˆ˜ ì¶”ì¶œ
            beginner_scores = scores[0]
            intermediate_scores = scores[1] 
            advanced_scores = scores[2]
            
            # ì ìˆ˜ ê°’ ì¶”ì¶œ
            beginner_rel = beginner_scores.get('answer_relevancy', 0)
            beginner_cor = beginner_scores.get('answer_correctness', 0)
            intermediate_rel = intermediate_scores.get('answer_relevancy', 0)
            intermediate_cor = intermediate_scores.get('answer_correctness', 0)
            advanced_rel = advanced_scores.get('answer_relevancy', 0)
            advanced_cor = advanced_scores.get('answer_correctness', 0)
            
            return {
                'answer_relevancy': [beginner_rel, intermediate_rel, advanced_rel],
                'answer_correctness': [beginner_cor, intermediate_cor, advanced_cor],
                'success': True
            }
        
        # scoresê°€ dictionaryì¸ ê²½ìš° (ragas_rerun.py ê¸°ì¡´ ë°©ì‹)
        elif scores is not None and hasattr(scores, 'keys') and 'answer_relevancy' in scores and 'answer_correctness' in scores:
            relevancy_list = scores['answer_relevancy'] 
            correctness_list = scores['answer_correctness']
            
            # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
            if not isinstance(relevancy_list, list):
                relevancy_list = [relevancy_list] if relevancy_list is not None else [0, 0, 0]
            if not isinstance(correctness_list, list):
                correctness_list = [correctness_list] if correctness_list is not None else [0, 0, 0]
            
            # ê¸¸ì´ê°€ 3ì´ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
            while len(relevancy_list) < 3:
                relevancy_list.append(0)
            while len(correctness_list) < 3:
                correctness_list.append(0)
            
            return {
                'answer_relevancy': relevancy_list[:3],
                'answer_correctness': correctness_list[:3],
                'success': True
            }
        else:
            # ë””ë²„ê¹… ì •ë³´ ì œê³µ (ragas_rerun.pyì™€ ë™ì¼)
            if scores is not None:
                if hasattr(scores, 'keys'):
                    available_keys = list(scores.keys())
                    error_msg = f"Missing expected keys. Available keys: {available_keys}"
                else:
                    error_msg = f"Scores is not dict-like. Type: {type(scores)}, Content: {scores}"
            else:
                error_msg = "Scores is None"
            
            return {'success': False, 'error': error_msg}
            
    except Exception as e:
        return {'success': False, 'error': str(e)}

def save_results_to_excel(test_data: Dict, filename: str = None) -> str:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"model_test/ragas_evaluation_results_{timestamp}.xlsx"
    
    try:
        # Create DataFrame
        df = pd.DataFrame([test_data])
        
        # Save to Excel
        df.to_excel(filename, index=False, engine='openpyxl')
        print(f"ğŸ“Š ê²°ê³¼ê°€ Excel íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
        return filename
        
    except Exception as e:
        print(f"âŒ Excel ì €ì¥ ì‹¤íŒ¨: {e}")
        return ""

def get_user_choice(options_dict, prompt_text):
    """ì‚¬ìš©ìì—ê²Œ ì„ íƒì§€ë¥¼ ë³´ì—¬ì£¼ê³  ì„ íƒì„ ë°›ëŠ” í•¨ìˆ˜"""
    print(f"\n{prompt_text}")
    print("-" * 30)
    for key, (value, korean) in options_dict.items():
        print(f"{key}. {korean} ({value})")
    
    while True:
        try:
            choice = input("\nì„ íƒí•˜ì„¸ìš” (ë²ˆí˜¸ ì…ë ¥): ").strip()
            if choice in options_dict:
                return options_dict[choice]
            print("âŒ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except EOFError:
            # ë¹„ëŒ€í™”í˜• í™˜ê²½ì—ì„œëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
            print("\n[ë¹„ëŒ€í™”í˜• ëª¨ë“œ] ê¸°ë³¸ê°’ ì‚¬ìš©: ë„¤ì´ë²„ ë°±ì—”ë“œ ê¸°ìˆ ë©´ì ‘")
            if "1" in options_dict:
                return options_dict["1"]
            return list(options_dict.values())[0]

def main():
    print("ğŸ¯ AI Model Test")
    print("=" * 50)
    
    # User selections
    print("ì„¤ì •ì„ ì„ íƒí•´ì£¼ì„¸ìš”:")
    
    # 1. Company selection
    company_id, company_name = get_user_choice(COMPANIES, "ğŸ“ íšŒì‚¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    
    # 2. Position selection  
    position, position_name = get_user_choice(POSITIONS, "ğŸ’¼ ì§êµ°ì„ ì„ íƒí•˜ì„¸ìš”:")
    
    # 3. Interviewer role selection
    interviewer_role, role_name = get_user_choice(INTERVIEWER_ROLES, "ğŸ‘¨â€ğŸ’¼ ë©´ì ‘ê´€ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:")
    
    print(f"\nâœ… ì„ íƒì™„ë£Œ: {company_name} {position_name} {role_name} ë©´ì ‘")
    
    # API í‚¤ í™•ì¸
    openai_api_key = os.getenv('OPENAI_API_KEY')
    if not openai_api_key:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì—ì„œ OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    # Initialize models
    print("\nğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    try:
        question_gen = QuestionGenerator()
        answer_gen = AICandidateModel()
        print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì‹¤ì œ DBì—ì„œ ë¡œë”©ëœ íšŒì‚¬ ëª©ë¡ í™•ì¸ ë° ì¶œë ¥
        print(f"\nğŸ” ì‹¤ì œ ë¡œë”©ëœ íšŒì‚¬ ëª©ë¡: {list(question_gen.companies_data.keys())}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ íšŒì‚¬ë¡œ ê°•ì œ ë³€ê²½
        if question_gen.companies_data:
            available_company_id = list(question_gen.companies_data.keys())[0]
            if company_id not in question_gen.companies_data:
                print(f"âš ï¸  íšŒì‚¬ë¥¼ {company_id}ì—ì„œ {available_company_id}ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
                company_id = available_company_id
                company_name = question_gen.companies_data[company_id].get('name', company_id)
        else:
            print("âŒ DBì—ì„œ íšŒì‚¬ ë°ì´í„°ë¥¼ ë¡œë”©í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # 1. Create single persona for consistency (MOVED TO FIRST)
    print(f"\nğŸ‘¤ í˜ë¥´ì†Œë‚˜ ìƒì„± ì¤‘...")
    persona = answer_gen.create_persona_for_interview(company_id, position)
    if not persona:
        persona = answer_gen._create_default_persona(company_id, position)
    
    print(f"âœ… í˜ë¥´ì†Œë‚˜: {persona.name}")
    print(f"   ê²½ë ¥: {persona.background.get('career_years', 'ë¯¸ì •')}ë…„")
    print(f"   ì£¼ìš”ê¸°ìˆ : {', '.join(persona.technical_skills[:3])}")
    
    # 2. Create a resume dictionary from the persona object
    resume_data = {
        "name": persona.name,
        "background": persona.background,
        "technical_skills": persona.technical_skills,
        "experiences": persona.experiences,
        "projects": persona.projects,
        "strengths": persona.strengths,
        "career_goal": persona.career_goal
    }
    
    # 3. Generate context-aware question
    print("\nğŸ“ ì§ˆë¬¸ ìƒì„± ì¤‘...")
    try:
        question_result = question_gen.generate_question_by_role(
            interviewer_role=interviewer_role,
            company_id=company_id, 
            user_resume=resume_data  # Use actual persona data instead of placeholder
        )
        question = question_result['question']
        question_source = question_result.get('question_source', 'unknown')
        
        print(f"\nğŸ“ Generated Question (Source: {question_source}):")
        print(f"{question}")
        
        # ì§ˆë¬¸ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if not question or len(question.strip()) < 10:
            raise ValueError("ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€ì²´
        question = f"{company_name} {position_name} ì§êµ°ì— ì§€ì›í•œ ì´ìœ ì™€ ë³¸ì¸ì˜ ê°•ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        print(f"ğŸ“ ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©: {question}")
    
    # 4. Generate context-aware ground truth answer
    print(f"\nğŸ¯ ëª¨ë²”ë‹µì•ˆ ìƒì„± ì¤‘...")
    ground_truth = generate_ground_truth_answer(question, company_id, position, interviewer_role, persona_resume=resume_data)
    print(f"\nğŸ“– ëª¨ë²”ë‹µì•ˆ:")
    print(f"{ground_truth}")
    print("-" * 50)
    
    # Map interviewer role to question type
    question_type_mapping = {
        "HR": QuestionType.HR,
        "TECH": QuestionType.TECH,
        "COLLABORATION": QuestionType.COLLABORATION
    }
    question_type = question_type_mapping.get(interviewer_role, QuestionType.TECH)
    
    # 2. Generate answers by level with same persona
    print(f"\nğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘...")
    print("\nğŸ¤– AI Answers:")
    print("-" * 50)
    
    levels = [
        ("ğŸ”° ì´ˆê¸‰", QualityLevel.INADEQUATE), 
        ("ğŸ”¸ ì¤‘ê¸‰", QualityLevel.AVERAGE), 
        ("ğŸ”¥ ê³ ê¸‰", QualityLevel.EXCELLENT)
    ]
    
    candidate_answers = []
    
    for level_name, quality_level in levels:
        request = AnswerRequest(
            question_content=question,
            question_type=question_type,
            question_intent=f"{role_name} ì—­ëŸ‰ í‰ê°€",
            company_id=company_id,
            position=position, 
            quality_level=quality_level,
            llm_provider=LLMProvider.OPENAI_GPT4O
        )
        
        print(f"\n{level_name} ë‹µë³€ ìƒì„± ì¤‘...")
        response = answer_gen.generate_answer(request, persona=persona)  # ê°™ì€ í˜ë¥´ì†Œë‚˜ ì‚¬ìš©
        print(f"\n{level_name}:")
        print(f"{response.answer_content}")
        print("-" * 30)
        
        # Store for RAGAS evaluation
        candidate_answers.append({
            "level": level_name,
            "content": response.answer_content
        })
    
    # 3. RAGAS Evaluation
    print(f"\nğŸ” RAGAS í‰ê°€ ì‹¤í–‰ ì¤‘...")
    ragas_result = evaluate_with_ragas(question, ground_truth, candidate_answers)
    
    # RAGAS ê²°ê³¼ ì²˜ë¦¬ - ê°œì„ ëœ ë²„ì „
    try:
        if isinstance(ragas_result, dict) and "error" in ragas_result:
            print(f"\nâŒ RAGAS í‰ê°€ ì˜¤ë¥˜: {ragas_result['error']}")
        elif ragas_result.get('success', True):  # success í‚¤ê°€ ìˆìœ¼ë©´ ì²´í¬, ì—†ìœ¼ë©´ Trueë¡œ ê°„ì£¼
            print(f"\nğŸ“Š RAGAS í‰ê°€ ê²°ê³¼:")
            print("=" * 50)
            
            # RAGAS ê²°ê³¼ì—ì„œ ì ìˆ˜ ì¶”ì¶œ
            try:
                relevancy_scores = ragas_result['answer_relevancy']
                correctness_scores = ragas_result['answer_correctness']
                
                # Display results for each answer
                level_names = ["ğŸ”° ì´ˆê¸‰", "ğŸ”¸ ì¤‘ê¸‰", "ğŸ”¥ ê³ ê¸‰"]
                for i, (ans, level_name) in enumerate(zip(candidate_answers, level_names)):
                    if i < len(relevancy_scores) and i < len(correctness_scores):
                        relevancy = relevancy_scores[i]
                        correctness = correctness_scores[i]
                        combined = (relevancy + correctness) / 2
                        
                        print(f"\n{level_name} í‰ê°€:")
                        print(f"  â€¢ Answer Relevancy: {relevancy:.3f}")
                        print(f"  â€¢ Answer Correctness: {correctness:.3f}")  
                        print(f"  â€¢ ì¢…í•© ì ìˆ˜: {combined:.3f}")
                    
                # í‰ê·  ì ìˆ˜ ê³„ì‚°
                if relevancy_scores and correctness_scores:
                    avg_relevancy = sum(relevancy_scores) / len(relevancy_scores)
                    avg_correctness = sum(correctness_scores) / len(correctness_scores)
                    overall_avg = (avg_relevancy + avg_correctness) / 2
                    
                    print(f"\nğŸ“ˆ í‰ê·  ì ìˆ˜:")
                    print(f"  â€¢ Answer Relevancy: {avg_relevancy:.3f}")
                    print(f"  â€¢ Answer Correctness: {avg_correctness:.3f}")
                    print(f"  â€¢ ì „ì²´ í‰ê· : {overall_avg:.3f}")
                
            except Exception as score_error:
                print(f"\nâŒ ì ìˆ˜ ì¶”ì¶œ ì˜¤ë¥˜: {score_error}")
                print(f"  ğŸ“Š RAGAS ì ìˆ˜ (ì›ë³¸ ê²°ê³¼):")
                print(f"     ê²°ê³¼ íƒ€ì…: {type(ragas_result)}")
                print(f"     ê²°ê³¼ ë‚´ìš©: {ragas_result}")
        else:
            print(f"\nâŒ RAGAS í‰ê°€ ì‹¤íŒ¨: {ragas_result.get('error', 'Unknown error')}")
                
    except Exception as e:
        print(f"\nâŒ RAGAS ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        print(f"   ê²°ê³¼ íƒ€ì…: {type(ragas_result)}")
        print(f"   ê²°ê³¼: {ragas_result}")
    
    # 5. Save results to Excel
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # Prepare test data for Excel
    test_data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "company": company_name,
        "position": position_name,
        "interviewer_role": role_name,
        "persona_name": persona.name,
        "persona_experience": persona.background.get('career_years', 'ë¯¸ì •'),
        "persona_skills": ', '.join(persona.technical_skills[:5]),
        "question": question,
        "ground_truth": ground_truth,
        "beginner_answer": candidate_answers[0]["content"] if len(candidate_answers) > 0 else "",
        "intermediate_answer": candidate_answers[1]["content"] if len(candidate_answers) > 1 else "",
        "advanced_answer": candidate_answers[2]["content"] if len(candidate_answers) > 2 else "",
    }
    
    # Add RAGAS scores if available - ê°œì„ ëœ ë²„ì „
    try:
        if isinstance(ragas_result, dict) and "error" in ragas_result:
            print(f"âš ï¸ RAGAS í‰ê°€ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì ìˆ˜ ì‚¬ìš©")
            # ê¸°ë³¸ê°’ ì¶”ê°€
            test_data.update({
                "beginner_relevancy": 0, "beginner_correctness": 0,
                "intermediate_relevancy": 0, "intermediate_correctness": 0, 
                "advanced_relevancy": 0, "advanced_correctness": 0,
                "avg_relevancy": 0, "avg_correctness": 0,
                "ragas_error": ragas_result["error"]
            })
        elif ragas_result.get('success', True):
            print(f"ğŸ” RAGAS ì ìˆ˜ Excel ì €ì¥ ì¤‘...")
            
            # ì ìˆ˜ ì¶”ì¶œ
            relevancy_list = ragas_result.get('answer_relevancy', [0, 0, 0])
            correctness_list = ragas_result.get('answer_correctness', [0, 0, 0])
            
            # ì•ˆì „í•œ ì¸ë±ìŠ¤ ì ‘ê·¼
            beginner_rel = relevancy_list[0] if len(relevancy_list) > 0 else 0
            beginner_cor = correctness_list[0] if len(correctness_list) > 0 else 0
            intermediate_rel = relevancy_list[1] if len(relevancy_list) > 1 else 0
            intermediate_cor = correctness_list[1] if len(correctness_list) > 1 else 0
            advanced_rel = relevancy_list[2] if len(relevancy_list) > 2 else 0
            advanced_cor = correctness_list[2] if len(correctness_list) > 2 else 0
            
            test_data.update({
                "beginner_relevancy": beginner_rel,
                "beginner_correctness": beginner_cor,
                "intermediate_relevancy": intermediate_rel,
                "intermediate_correctness": intermediate_cor,
                "advanced_relevancy": advanced_rel,
                "advanced_correctness": advanced_cor,
                "avg_relevancy": sum(relevancy_list) / len(relevancy_list) if relevancy_list else 0,
                "avg_correctness": sum(correctness_list) / len(correctness_list) if correctness_list else 0,
            })
            print(f"âœ… RAGAS ì ìˆ˜ ì¶”ê°€ ì™„ë£Œ")
        else:
            print(f"âš ï¸ RAGAS í‰ê°€ ì‹¤íŒ¨ - ê¸°ë³¸ê°’ ì‚¬ìš©")
            # ê¸°ë³¸ê°’ ì¶”ê°€
            test_data.update({
                "beginner_relevancy": 0, "beginner_correctness": 0,
                "intermediate_relevancy": 0, "intermediate_correctness": 0, 
                "advanced_relevancy": 0, "advanced_correctness": 0,
                "avg_relevancy": 0, "avg_correctness": 0,
                "ragas_error": ragas_result.get('error', 'Unknown RAGAS error')
            })
            
    except Exception as score_error:
        print(f"âš ï¸ ì ìˆ˜ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {score_error}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì¶”ê°€
        test_data.update({
            "beginner_relevancy": 0, "beginner_correctness": 0,
            "intermediate_relevancy": 0, "intermediate_correctness": 0,
            "advanced_relevancy": 0, "advanced_correctness": 0, 
            "avg_relevancy": 0, "avg_correctness": 0,
            "score_extraction_error": str(score_error)
        })
        
    # Save to Excel
    excel_file = save_results_to_excel(test_data)
    
    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    if excel_file:
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {excel_file}")

def batch_test_scenarios():
    """ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ ìë™ í…ŒìŠ¤íŠ¸ - ë°œí‘œìš© ë°ì´í„° ìƒì„±"""
    print("ğŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ - ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ ìë™ ì‹¤í–‰")
    print("=" * 60)
    
    # Test scenarios - 5ê°œ ì§êµ°ë§Œ ì‚¬ìš©, ì˜ë¬¸ company_id ì‚¬ìš©
    scenarios = [
        ("naver", "ë„¤ì´ë²„", "backend", "ë°±ì—”ë“œ", "TECH", "ê¸°ìˆ "),
        ("kakao", "ì¹´ì¹´ì˜¤", "frontend", "í”„ë¡ íŠ¸ì—”ë“œ", "HR", "ì¸ì‚¬"),
        ("toss", "í† ìŠ¤", "ai", "AI/ML", "TECH", "ê¸°ìˆ "),
        ("coupang", "ì¿ íŒ¡", "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤", "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤", "COLLABORATION", "í˜‘ì—…"),
        ("baemin", "ë°°ë‹¬ì˜ë¯¼ì¡±", "ê¸°íš", "ê¸°íš", "HR", "ì¸ì‚¬"),
    ]
    
    all_results = []
    
    for i, (company_id, company_name, position, position_name, interviewer_role, role_name) in enumerate(scenarios, 1):
        print(f"\n[{i}/{len(scenarios)}] í…ŒìŠ¤íŠ¸ ì¤‘: {company_name} {position_name} {role_name} ë©´ì ‘")
        print("-" * 50)
        
        try:
            # API í‚¤ í™•ì¸
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if not openai_api_key:
                print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue
            
            # Initialize models
            try:
                question_gen = QuestionGenerator()
                answer_gen = AICandidateModel()
                
                # ì‹¤ì œ DBì—ì„œ ë¡œë”©ëœ íšŒì‚¬ ëª©ë¡ í™•ì¸ ë° ê°•ì œ ë³€ê²½
                if question_gen.companies_data:
                    available_company_id = list(question_gen.companies_data.keys())[0]
                    if company_id not in question_gen.companies_data:
                        print(f"âš ï¸  íšŒì‚¬ë¥¼ {company_id}ì—ì„œ {available_company_id}ë¡œ ë³€ê²½")
                        company_id = available_company_id
                        company_name = question_gen.companies_data[company_id].get('name', company_id)
                else:
                    print("âŒ DBì—ì„œ íšŒì‚¬ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
                    continue
                    
            except Exception as init_error:
                print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {init_error}")
                continue
            
            # Create persona
            persona = answer_gen.create_persona_for_interview(company_id, position)
            if not persona:
                persona = answer_gen._create_default_persona(company_id, position)
            
            print(f"âœ… í˜ë¥´ì†Œë‚˜: {persona.name} ({persona.background.get('career_years', 'ë¯¸ì •')}ë…„)")
            
            # Create resume data
            resume_data = {
                "name": persona.name,
                "background": persona.background,
                "technical_skills": persona.technical_skills,
                "experiences": persona.experiences,
                "projects": persona.projects,
                "strengths": persona.strengths,
                "career_goal": persona.career_goal
            }
            
            # Generate question
            try:
                question_result = question_gen.generate_question_by_role(
                    interviewer_role=interviewer_role,
                    company_id=company_id, 
                    user_resume=resume_data
                )
                question = question_result['question']
                question_source = question_result.get('question_source', 'unknown')
                
                # ì§ˆë¬¸ ê²€ì¦
                if not question or len(question.strip()) < 10:
                    raise ValueError("ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    
                print(f"ğŸ“ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ (Source: {question_source})")
            except Exception as question_error:
                print(f"âŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {question_error}")
                # ê¸°ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€ì²´
                question = f"{company_name} {position_name} ì§êµ°ì— ì§€ì›í•œ ì´ìœ ì™€ ë³¸ì¸ì˜ ê°•ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                print(f"ğŸ“ ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©")
            
            # Generate ground truth
            ground_truth = generate_ground_truth_answer(question, company_id, position, interviewer_role, persona_resume=resume_data)
            print(f"ğŸ¯ ëª¨ë²”ë‹µì•ˆ ìƒì„± ì™„ë£Œ")
            
            # Generate answers by level
            question_type_mapping = {
                "HR": QuestionType.HR,
                "TECH": QuestionType.TECH,
                "COLLABORATION": QuestionType.COLLABORATION
            }
            question_type = question_type_mapping.get(interviewer_role, QuestionType.TECH)
            
            levels = [
                ("ì´ˆê¸‰", QualityLevel.INADEQUATE), 
                ("ì¤‘ê¸‰", QualityLevel.AVERAGE), 
                ("ê³ ê¸‰", QualityLevel.EXCELLENT)
            ]
            
            candidate_answers = []
            for level_name, quality_level in levels:
                request = AnswerRequest(
                    question_content=question,
                    question_type=question_type,
                    question_intent=f"{role_name} ì—­ëŸ‰ í‰ê°€",
                    company_id=company_id,
                    position=position, 
                    quality_level=quality_level,
                    llm_provider=LLMProvider.OPENAI_GPT4O
                )
                
                response = answer_gen.generate_answer(request, persona=persona)
                candidate_answers.append({
                    "level": level_name,
                    "content": response.answer_content
                })
            print(f"ğŸ¤– 3ê°€ì§€ ë ˆë²¨ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # RAGAS evaluation
            ragas_result = evaluate_with_ragas(question, ground_truth, candidate_answers)
            print(f"ğŸ” RAGAS í‰ê°€ ì™„ë£Œ")
            
            # Prepare result data
            result_data = {
                "scenario": f"{company_name}_{position_name}_{role_name}",
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "company": company_name,
                "position": position_name,
                "interviewer_role": role_name,
                "persona_name": persona.name,
                "persona_experience": persona.background.get('career_years', 'ë¯¸ì •'),
                "persona_skills": ', '.join(persona.technical_skills[:5]),
                "question": question,
                "ground_truth": ground_truth,
                "beginner_answer": candidate_answers[0]["content"] if len(candidate_answers) > 0 else "",
                "intermediate_answer": candidate_answers[1]["content"] if len(candidate_answers) > 1 else "",
                "advanced_answer": candidate_answers[2]["content"] if len(candidate_answers) > 2 else "",
            }
            
            # Add RAGAS scores - ê°œì„ ëœ ë²„ì „
            try:
                if isinstance(ragas_result, dict) and "error" in ragas_result:
                    # ê¸°ë³¸ê°’ ì¶”ê°€
                    result_data.update({
                        "beginner_relevancy": 0, "beginner_correctness": 0,
                        "intermediate_relevancy": 0, "intermediate_correctness": 0,
                        "advanced_relevancy": 0, "advanced_correctness": 0,
                        "avg_relevancy": 0, "avg_correctness": 0,
                        "ragas_error": ragas_result["error"]
                    })
                elif ragas_result.get('success', True):
                    # ì ìˆ˜ ì¶”ì¶œ
                    relevancy_list = ragas_result.get('answer_relevancy', [0, 0, 0])
                    correctness_list = ragas_result.get('answer_correctness', [0, 0, 0])
                    
                    # ì•ˆì „í•œ ì¸ë±ìŠ¤ ì ‘ê·¼
                    result_data.update({
                        "beginner_relevancy": relevancy_list[0] if len(relevancy_list) > 0 else 0,
                        "beginner_correctness": correctness_list[0] if len(correctness_list) > 0 else 0,
                        "intermediate_relevancy": relevancy_list[1] if len(relevancy_list) > 1 else 0,
                        "intermediate_correctness": correctness_list[1] if len(correctness_list) > 1 else 0,
                        "advanced_relevancy": relevancy_list[2] if len(relevancy_list) > 2 else 0,
                        "advanced_correctness": correctness_list[2] if len(correctness_list) > 2 else 0,
                        "avg_relevancy": sum(relevancy_list) / len(relevancy_list) if relevancy_list else 0,
                        "avg_correctness": sum(correctness_list) / len(correctness_list) if correctness_list else 0,
                    })
                else:
                    # ê¸°ë³¸ê°’ ì¶”ê°€
                    result_data.update({
                        "beginner_relevancy": 0, "beginner_correctness": 0,
                        "intermediate_relevancy": 0, "intermediate_correctness": 0,
                        "advanced_relevancy": 0, "advanced_correctness": 0,
                        "avg_relevancy": 0, "avg_correctness": 0,
                        "ragas_error": ragas_result.get('error', 'Unknown RAGAS error')
                    })
            except Exception as e:
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì¶”ê°€
                result_data.update({
                    "beginner_relevancy": 0, "beginner_correctness": 0,
                    "intermediate_relevancy": 0, "intermediate_correctness": 0,
                    "advanced_relevancy": 0, "advanced_correctness": 0,
                    "avg_relevancy": 0, "avg_correctness": 0,
                    "score_extraction_error": str(e)
                })
                
            all_results.append(result_data)
            print(f"âœ… ì‹œë‚˜ë¦¬ì˜¤ {i} ì™„ë£Œ\n")
            
        except Exception as e:
            print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ {i} ì‹¤íŒ¨: {e}\n")
            continue
    
    # Save all results to Excel
    if all_results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"model_test/batch_test_results_{timestamp}.xlsx"
        
        try:
            df = pd.DataFrame(all_results)
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"ğŸ‰ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ {len(all_results)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    return None

def random_batch_test(num_tests: int = 100):
    """ë¬´ì‘ìœ„ íšŒì‚¬ì™€ ì§êµ°ìœ¼ë¡œ Në²ˆ í…ŒìŠ¤íŠ¸í•˜ì—¬ í‰ê·  ì ìˆ˜ ê³„ì‚°"""
    print(f"ğŸ² ë¬´ì‘ìœ„ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ - {num_tests}ë²ˆ ì‹¤í–‰")
    print("=" * 60)
    
    # íšŒì‚¬ì™€ ì§êµ° ë¦¬ìŠ¤íŠ¸
    companies = list(COMPANIES.values())
    positions = list(POSITIONS.values()) 
    interviewer_roles = list(INTERVIEWER_ROLES.values())
    
    all_results = []  # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ìš©
    all_scores = {
        'relevancy_beginner': [],
        'relevancy_intermediate': [],
        'relevancy_advanced': [],
        'correctness_beginner': [],
        'correctness_intermediate': [],
        'correctness_advanced': [],
        'overall_scores': []
    }
    
    successful_tests = 0
    failed_tests = 0
    
    # ë² ìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì €ì¥ìš©
    best_case = {
        'score': -1,
        'test_num': 0,
        'company_name': '',
        'position_name': '', 
        'role_name': '',
        'question': '',
        'ground_truth': '',
        'candidate_answers': [],
        'scores': {}
    }
    
    # tqdm ì§„í–‰ë¥  í‘œì‹œ
    progress_bar = tqdm(range(1, num_tests + 1), desc="í…ŒìŠ¤íŠ¸ ì§„í–‰", unit="test")
    
    for test_num in progress_bar:
        # ë¬´ì‘ìœ„ ì„ íƒ - í•œê¸€ íšŒì‚¬ëª… ì‚¬ìš©
        company_id, company_name = random.choice(companies)
        position, position_name = random.choice(positions)
        interviewer_role, role_name = random.choice(interviewer_roles)
        
        # tqdmì— í˜„ì¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        progress_bar.set_postfix({
            'í˜„ì¬': f"{company_name} {position_name} {role_name}",
            'ì„±ê³µ': successful_tests,
            'ì‹¤íŒ¨': failed_tests
        })
        
        try:
            # API í‚¤ í™•ì¸
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if not openai_api_key:
                progress_bar.write(f"[ERROR] OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                failed_tests += 1
                continue
            
            # Initialize models
            try:
                question_gen = QuestionGenerator()
                answer_gen = AICandidateModel()
                progress_bar.write(f"[DEBUG] ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # ì‹¤ì œ DBì—ì„œ ë¡œë”©ëœ íšŒì‚¬ ëª©ë¡ í™•ì¸ ë° ê°•ì œ ë³€ê²½
                if question_gen.companies_data:
                    available_company_id = list(question_gen.companies_data.keys())[0]
                    if company_id not in question_gen.companies_data:
                        progress_bar.write(f"[DEBUG] íšŒì‚¬ ë³€ê²½: {company_id} -> {available_company_id}")
                        company_id = available_company_id
                        company_name = question_gen.companies_data[company_id].get('name', company_id)
                else:
                    progress_bar.write(f"[ERROR] DB íšŒì‚¬ ë°ì´í„° ì—†ìŒ")
                    failed_tests += 1
                    continue
                    
            except Exception as init_error:
                progress_bar.write(f"[ERROR] ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {init_error}")
                failed_tests += 1
                continue
            
            # Create persona
            try:
                # ë””ë²„ê¹…: ì •í™•í•œ ë§¤í•‘ í™•ì¸
                progress_bar.write(f"[DEBUG] í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹œë„: company_id='{company_id}', position='{position}'")
                
                persona = answer_gen.create_persona_for_interview(company_id, position)
                if not persona:
                    progress_bar.write(f"[WARNING] í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ìƒì„± ì¤‘...")
                    persona = answer_gen._create_default_persona(company_id, position)
                progress_bar.write(f"[DEBUG] í˜ë¥´ì†Œë‚˜ ìƒì„± ì™„ë£Œ: {persona.name}")
            except Exception as persona_error:
                progress_bar.write(f"[ERROR] í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹¤íŒ¨: {persona_error}")
                # ì™„ì „í•œ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ìƒì„±
                persona = answer_gen._create_default_persona(company_id, position)
            
            # Create resume data
            resume_data = {
                "name": persona.name,
                "background": persona.background,
                "technical_skills": persona.technical_skills,
                "experiences": persona.experiences,
                "projects": persona.projects,
                "strengths": persona.strengths,
                "career_goal": persona.career_goal
            }
            
            # Generate question
            try:
                progress_bar.write(f"[DEBUG] ì§ˆë¬¸ ìƒì„± ì‹œë„: role='{interviewer_role}', company='{company_id}'")
                question_result = question_gen.generate_question_by_role(
                    interviewer_role=interviewer_role,
                    company_id=company_id, 
                    user_resume=resume_data
                )
                question = question_result['question']
                question_source = question_result.get('question_source', 'unknown')
                progress_bar.write(f"[DEBUG] ì§ˆë¬¸ ìƒì„± ì™„ë£Œ: source='{question_source}'")
                progress_bar.write(f"[DEBUG] ì§ˆë¬¸ ë‚´ìš©: {question[:100]}...")
            except Exception as question_error:
                progress_bar.write(f"[ERROR] ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {question_error}")
                raise
            
            # Generate ground truth
            ground_truth = generate_ground_truth_answer(question, company_id, position, interviewer_role, persona_resume=resume_data)
            
            # Generate answers by level
            question_type_mapping = {
                "HR": QuestionType.HR,
                "TECH": QuestionType.TECH,
                "COLLABORATION": QuestionType.COLLABORATION
            }
            question_type = question_type_mapping.get(interviewer_role, QuestionType.TECH)
            
            levels = [
                ("ì´ˆê¸‰", QualityLevel.INADEQUATE), 
                ("ì¤‘ê¸‰", QualityLevel.AVERAGE), 
                ("ê³ ê¸‰", QualityLevel.EXCELLENT)
            ]
            
            candidate_answers = []
            for level_name, quality_level in levels:
                request = AnswerRequest(
                    question_content=question,
                    question_type=question_type,
                    question_intent=f"{role_name} ì—­ëŸ‰ í‰ê°€",
                    company_id=company_id,
                    position=position, 
                    quality_level=quality_level,
                    llm_provider=LLMProvider.OPENAI_GPT4O
                )
                
                response = answer_gen.generate_answer(request, persona=persona)
                candidate_answers.append({
                    "level": level_name,
                    "content": response.answer_content
                })
            
            # RAGAS evaluation
            try:
                ragas_result = evaluate_with_ragas(question, ground_truth, candidate_answers)
                progress_bar.write(f"[DEBUG] RAGAS ê²°ê³¼ íƒ€ì…: {type(ragas_result)}")
                progress_bar.write(f"[DEBUG] RAGAS ê²°ê³¼: {ragas_result}")
            except Exception as ragas_error:
                progress_bar.write(f"[ERROR] RAGAS í‰ê°€ ì‹¤íŒ¨: {ragas_error}")
                ragas_result = {"error": str(ragas_error)}
            
            # Extract scores - ê°œì„ ëœ ë²„ì „
            scores_extracted = False
            if isinstance(ragas_result, dict) and "error" in ragas_result:
                progress_bar.write(f"[DEBUG] RAGAS í‰ê°€ ì‹¤íŒ¨: {ragas_result['error']}")
                scores_extracted = False
            elif ragas_result.get('success', True):
                # ì ìˆ˜ ì¶”ì¶œ
                relevancy_list = ragas_result.get('answer_relevancy', [0, 0, 0])
                correctness_list = ragas_result.get('answer_correctness', [0, 0, 0])
                
                progress_bar.write(f"[DEBUG] ì ìˆ˜ ì¶”ì¶œ: relevancy={relevancy_list}, correctness={correctness_list}")
                
                if len(relevancy_list) >= 3 and len(correctness_list) >= 3:
                    # ê°œë³„ ë ˆë²¨ë³„ ì ìˆ˜ ì €ì¥
                    all_scores['relevancy_beginner'].append(relevancy_list[0])
                    all_scores['relevancy_intermediate'].append(relevancy_list[1])
                    all_scores['relevancy_advanced'].append(relevancy_list[2])
                    all_scores['correctness_beginner'].append(correctness_list[0])
                    all_scores['correctness_intermediate'].append(correctness_list[1])
                    all_scores['correctness_advanced'].append(correctness_list[2])
                    
                    # ì „ì²´ í‰ê·  ì ìˆ˜ ê³„ì‚°
                    overall_avg = (sum(relevancy_list) + sum(correctness_list)) / (2 * len(relevancy_list))
                    all_scores['overall_scores'].append(overall_avg)
                    
                    # ë ˆë²¨ë³„ ì¢…í•© ì ìˆ˜ ê³„ì‚° (relevancy + correctness)
                    beginner_score = (relevancy_list[0] + correctness_list[0]) / 2
                    intermediate_score = (relevancy_list[1] + correctness_list[1]) / 2 
                    advanced_score = (relevancy_list[2] + correctness_list[2]) / 2
                    
                    # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
                    test_result = {
                        'test_num': test_num,
                        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        'company': company_name,
                        'position': position_name,
                        'interviewer_role': role_name,
                        'question': question,
                        'ground_truth': ground_truth,
                        'beginner_answer': candidate_answers[0]['content'],
                        'intermediate_answer': candidate_answers[1]['content'],
                        'advanced_answer': candidate_answers[2]['content'],
                        'beginner_relevancy': relevancy_list[0],
                        'beginner_correctness': correctness_list[0],
                        'intermediate_relevancy': relevancy_list[1],
                        'intermediate_correctness': correctness_list[1],
                        'advanced_relevancy': relevancy_list[2],
                        'advanced_correctness': correctness_list[2],
                        'avg_relevancy': sum(relevancy_list) / len(relevancy_list),
                        'avg_correctness': sum(correctness_list) / len(correctness_list)
                    }
                    all_results.append(test_result)
                    
                    # ë² ìŠ¤íŠ¸ ì¼€ì´ìŠ¤ íŒë³„ (í‰ê·  ì ìˆ˜ê°€ ë†’ê³ , ì´ˆê¸‰ < ì¤‘ê¸‰ < ê³ ê¸‰ ìˆœì„œì¸ ê²½ìš°)
                    is_progressive = beginner_score < intermediate_score < advanced_score
                    if overall_avg > best_case['score'] and is_progressive:
                        best_case.update({
                            'score': overall_avg,
                            'test_num': test_num,
                            'company_name': company_name,
                            'position_name': position_name,
                            'role_name': role_name,
                            'question': question,
                            'ground_truth': ground_truth,
                            'candidate_answers': candidate_answers.copy(),
                            'scores': {
                                'beginner': beginner_score,
                                'intermediate': intermediate_score, 
                                'advanced': advanced_score,
                                'relevancy': relevancy_list.copy(),
                                'correctness': correctness_list.copy()
                            }
                        })
                        
                    scores_extracted = True
                else:
                    progress_bar.write(f"[DEBUG] ì ìˆ˜ ë°°ì—´ ê¸¸ì´ ë¶€ì¡±: relevancy={len(relevancy_list)}, correctness={len(correctness_list)}")
            else:
                progress_bar.write(f"[DEBUG] RAGAS í‰ê°€ ì‹¤íŒ¨: {ragas_result.get('error', 'Unknown error')}")
            
            if scores_extracted:
                successful_tests += 1
            else:
                failed_tests += 1
                
        except Exception as e:
            failed_tests += 1
            continue
        
        # tqdm ìƒíƒœ ì—…ë°ì´íŠ¸
        progress_bar.set_postfix({
            'í˜„ì¬': f"{company_name} {position_name} {role_name}",
            'ì„±ê³µ': successful_tests,
            'ì‹¤íŒ¨': failed_tests
        })
    
    # ê²°ê³¼ ê³„ì‚° ë° ì¶œë ¥
    print(f"\nğŸ“Š ë¬´ì‘ìœ„ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("=" * 60)
    print(f"ì´ í…ŒìŠ¤íŠ¸: {num_tests}ê°œ")
    print(f"ì„±ê³µ: {successful_tests}ê°œ")
    print(f"ì‹¤íŒ¨: {failed_tests}ê°œ")
    print(f"ì„±ê³µë¥ : {(successful_tests/num_tests*100):.1f}%")
    
    if successful_tests > 0:
        print(f"\nğŸ“ˆ í‰ê·  ì ìˆ˜ (ì„±ê³µí•œ {successful_tests}ê°œ í…ŒìŠ¤íŠ¸ ê¸°ì¤€):")
        print("-" * 40)
        
        # ë ˆë²¨ë³„ í‰ê·  ì ìˆ˜
        avg_relevancy_beginner = sum(all_scores['relevancy_beginner']) / len(all_scores['relevancy_beginner'])
        avg_relevancy_intermediate = sum(all_scores['relevancy_intermediate']) / len(all_scores['relevancy_intermediate'])
        avg_relevancy_advanced = sum(all_scores['relevancy_advanced']) / len(all_scores['relevancy_advanced'])
        
        avg_correctness_beginner = sum(all_scores['correctness_beginner']) / len(all_scores['correctness_beginner'])
        avg_correctness_intermediate = sum(all_scores['correctness_intermediate']) / len(all_scores['correctness_intermediate'])
        avg_correctness_advanced = sum(all_scores['correctness_advanced']) / len(all_scores['correctness_advanced'])
        
        print("ğŸ”° ì´ˆê¸‰ ë ˆë²¨:")
        print(f"  â€¢ Answer Relevancy: {avg_relevancy_beginner:.3f}")
        print(f"  â€¢ Answer Correctness: {avg_correctness_beginner:.3f}")
        print(f"  â€¢ ë ˆë²¨ í‰ê· : {(avg_relevancy_beginner + avg_correctness_beginner) / 2:.3f}")
        
        print("\nğŸ”¸ ì¤‘ê¸‰ ë ˆë²¨:")
        print(f"  â€¢ Answer Relevancy: {avg_relevancy_intermediate:.3f}")
        print(f"  â€¢ Answer Correctness: {avg_correctness_intermediate:.3f}")
        print(f"  â€¢ ë ˆë²¨ í‰ê· : {(avg_relevancy_intermediate + avg_correctness_intermediate) / 2:.3f}")
        
        print("\nğŸ”¥ ê³ ê¸‰ ë ˆë²¨:")
        print(f"  â€¢ Answer Relevancy: {avg_relevancy_advanced:.3f}")
        print(f"  â€¢ Answer Correctness: {avg_correctness_advanced:.3f}")
        print(f"  â€¢ ë ˆë²¨ í‰ê· : {(avg_relevancy_advanced + avg_correctness_advanced) / 2:.3f}")
        
        # ì „ì²´ í‰ê· 
        overall_relevancy = (avg_relevancy_beginner + avg_relevancy_intermediate + avg_relevancy_advanced) / 3
        overall_correctness = (avg_correctness_beginner + avg_correctness_intermediate + avg_correctness_advanced) / 3
        overall_average = sum(all_scores['overall_scores']) / len(all_scores['overall_scores'])
        
        print(f"\nğŸ¯ ì „ì²´ í‰ê· :")
        print(f"  â€¢ Answer Relevancy: {overall_relevancy:.3f}")
        print(f"  â€¢ Answer Correctness: {overall_correctness:.3f}")
        print(f"  â€¢ ìµœì¢… í‰ê·  ì ìˆ˜: {overall_average:.3f}")
        
        # ê²°ê³¼ë¥¼ Excelì— ì €ì¥ (baselineê³¼ ë™ì¼í•œ í˜•íƒœ)
        summary_data = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'total_tests': num_tests,
            'successful_tests': successful_tests,
            'failed_tests': failed_tests,
            'success_rate': successful_tests/num_tests*100,
            # baseline í˜•íƒœë¡œ ì»¬ëŸ¼ëª… ë³€ê²½
            'beginner_relevancy': avg_relevancy_beginner,
            'beginner_correctness': avg_correctness_beginner,
            'intermediate_relevancy': avg_relevancy_intermediate,
            'intermediate_correctness': avg_correctness_intermediate,
            'advanced_relevancy': avg_relevancy_advanced,
            'advanced_correctness': avg_correctness_advanced,
            'avg_relevancy': overall_relevancy,
            'avg_correctness': overall_correctness,
            'final_average_score': overall_average
        }
        
        # ë² ìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶œë ¥
        if best_case['score'] > -1:
            print(f"\n{'='*80}")
            print(f"ğŸ† ë² ìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì˜ˆì‹œ (#{best_case['test_num']}: {best_case['company_name']} {best_case['position_name']} {best_case['role_name']} ë©´ì ‘)")
            print(f"   ì „ì²´ í‰ê·  ì ìˆ˜: {best_case['score']:.3f}")
            print(f"{'='*80}")
            
            print(f"â“ ì§ˆë¬¸:")
            print(f"{best_case['question']}")
            
            print(f"\nğŸ¯ ëª¨ë²”ë‹µì•ˆ:")
            print(f"{best_case['ground_truth']}")
            
            print(f"\nğŸ¤– AI ë‹µë³€ ë° ì ìˆ˜:")
            print("-" * 80)
            
            level_names = ["ğŸ”° ì´ˆê¸‰", "ğŸ”¸ ì¤‘ê¸‰", "ğŸ”¥ ê³ ê¸‰"]
            level_keys = ["beginner", "intermediate", "advanced"]
            
            for i, (level_name, level_key) in enumerate(zip(level_names, level_keys)):
                answer = best_case['candidate_answers'][i]['content']
                score = best_case['scores'][level_key]
                relevancy = best_case['scores']['relevancy'][i]
                correctness = best_case['scores']['correctness'][i]
                
                print(f"{level_name} (ì¢…í•©: {score:.3f}):")
                print(f"  ğŸ“Š Answer Relevancy: {relevancy:.3f} | Answer Correctness: {correctness:.3f}")
                print(f"  ğŸ’¬ ë‹µë³€: {answer}")
                
                if i < len(level_names) - 1:
                    print("-" * 60)
            
            print(f"{'='*80}")
        else:
            print(f"\nâš ï¸  ë² ìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. (ì´ˆê¸‰ < ì¤‘ê¸‰ < ê³ ê¸‰ ìˆœì„œì˜ ì¼€ì´ìŠ¤ ì—†ìŒ)")

        # Excel íŒŒì¼ ì €ì¥ (baselineê³¼ ë™ì¼í•œ í˜•íƒœë¡œ ëª¨ë“  ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼)
        if all_results:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"model_test/new_model_ragas_results_{timestamp}.xlsx"
            
            try:
                # ëª¨ë“  ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                results_df = pd.DataFrame(all_results)
                results_df.to_excel(filename, index=False, engine='openpyxl')
                print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {filename}")
                print(f"   ğŸ“Š {len(all_results)}ê°œ ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
                
                # ìš”ì•½ í†µê³„ë„ ë³„ë„ë¡œ ì¶œë ¥
                print(f"\nğŸ“ˆ ì €ì¥ëœ ë°ì´í„° ìš”ì•½:")
                print(f"   - ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {len(all_results)}ê°œ")
                print(f"   - í‰ê·  Relevancy: {results_df['avg_relevancy'].mean():.3f}")
                print(f"   - í‰ê·  Correctness: {results_df['avg_correctness'].mean():.3f}")
                
            except Exception as e:
                print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        else:
            print(f"\nâš ï¸ ì €ì¥í•  ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    else:
        print("\nâŒ ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ì–´ í‰ê· ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return successful_tests, failed_tests, all_scores if successful_tests > 0 else None

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        if sys.argv[1] == "--batch":
            batch_test_scenarios()
        elif sys.argv[1] == "--random":
            # ê¸°ë³¸ 100ë²ˆ, ì›í•˜ëŠ” ìˆ˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŒ
            num_tests = int(sys.argv[2]) if len(sys.argv) > 2 else 100
            random_batch_test(num_tests)
        else:
            print("ì‚¬ìš©ë²•:")
            print("  python candidate_model_test.py              # ë‹¨ì¼ í…ŒìŠ¤íŠ¸")
            print("  python candidate_model_test.py --batch      # 5ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë°°ì¹˜ í…ŒìŠ¤íŠ¸")
            print("  python candidate_model_test.py --random     # 100ë²ˆ ë¬´ì‘ìœ„ í…ŒìŠ¤íŠ¸")
            print("  python candidate_model_test.py --random 50  # 50ë²ˆ ë¬´ì‘ìœ„ í…ŒìŠ¤íŠ¸")
    else:
        main()