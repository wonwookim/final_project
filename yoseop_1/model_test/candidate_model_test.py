#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple Model Test
- Question Generation -> Answer Generation -> RAGAS Evaluation
"""

import os
import sys
import logging
import openai
import pandas as pd
from datetime import datetime
from typing import Dict, List
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import answer_relevancy, answer_correctness

# Suppress all logs except CRITICAL
logging.getLogger().setLevel(logging.CRITICAL)
logging.getLogger("httpx").setLevel(logging.CRITICAL)
logging.getLogger("backend.services.existing_tables_service").setLevel(logging.CRITICAL)
logging.getLogger("llm.shared.utils").setLevel(logging.CRITICAL)

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Import existing models
from llm.interviewer.question_generator import QuestionGenerator  
from llm.candidate.model import AICandidateModel
from llm.candidate.quality_controller import QualityLevel
from llm.shared.models import AnswerRequest, QuestionType, LLMProvider

# Available options
COMPANIES = {
    "1": ("naver", "ë„¤ì´ë²„"),
    "2": ("kakao", "ì¹´ì¹´ì˜¤"),
    "3": ("toss", "í† ìŠ¤"),
    "4": ("coupang", "ì¿ íŒ¡"),
    "5": ("baemin", "ë°°ë‹¬ì˜ë¯¼ì¡±"),
    "6": ("daangn", "ë‹¹ê·¼ë§ˆì¼“"),
    "7": ("line", "ë¼ì¸")
}

POSITIONS = {
    "1": ("backend", "ë°±ì—”ë“œ"),
    "2": ("frontend", "í”„ë¡ íŠ¸ì—”ë“œ"),
    "3": ("data", "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤"),
    "4": ("ai", "AI/ML"),
    "5": ("planning", "ê¸°íš")
}

INTERVIEWER_ROLES = {
    "1": ("HR", "ì¸ì‚¬"),
    "2": ("TECH", "ê¸°ìˆ "),
    "3": ("COLLABORATION", "í˜‘ì—…")
}

def generate_ground_truth_answer(question: str, company_id: str, position: str, interviewer_role: str, persona_resume: Dict) -> str:
    """ë©´ì ‘ê´€ ê´€ì ì—ì„œ íŽ˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëª¨ë²”ë‹µì•ˆ ìƒì„±"""
    client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    
    # Convert resume dict to a readable string for the prompt
    resume_summary = f"""ì§€ì›ìž ì •ë³´:
- ì´ë¦„: {persona_resume.get('name', 'Unknown')}
- ê²½ë ¥: {persona_resume.get('background', {}).get('career_years', 'N/A')}ë…„
- ì£¼ìš” ê¸°ìˆ : {', '.join(persona_resume.get('technical_skills', [])[:5])}
- ê°•ì : {', '.join(persona_resume.get('strengths', [])[:3])}
- ëª©í‘œ: {persona_resume.get('career_goal', 'N/A')}"""
    
    system_prompt = f"""ë‹¹ì‹ ì€ {company_id} íšŒì‚¬ì˜ ê²½í—˜ ë§Žì€ {interviewer_role} ë©´ì ‘ê´€ìž…ë‹ˆë‹¤.
{position} ì§êµ° ì§€ì›ìžì—ê²Œ í•œ ì§ˆë¬¸ì— ëŒ€í•´, ì•„ëž˜ ì œê³µëœ ì§€ì›ìžì˜ ì´ë ¥ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì´ìƒì ì¸ ëª¨ë²”ë‹µì•ˆì„ ìž‘ì„±í•´ì£¼ì„¸ìš”.

{resume_summary}

ëª¨ë²”ë‹µì•ˆ ìž‘ì„± ê°€ì´ë“œ:
- ì œê³µëœ ì§€ì›ìžì˜ ì´ë ¥ê³¼ ê²½í—˜ì— ê¸°ë°˜í•œ ë‹µë³€
- ì§€ì›ìžì˜ ê¸°ìˆ  ìŠ¤íƒê³¼ ê°•ì ì„ í™œìš©í•œ êµ¬ì²´ì ì¸ ì˜ˆì‹œ í¬í•¨
- ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ êµ¬ì¡°
- ì ì ˆí•œ ê¸¸ì´ (150-250ë‹¨ì–´)
- ì§€ì›ìžì˜ ëª©í‘œì™€ ì—°ê²°ëœ ë‹µë³€"""

    user_prompt = f"""ì§ˆë¬¸: {question}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ì§€ì›ìž ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë²”ë‹µì•ˆì„ ìž‘ì„±í•´ì£¼ì„¸ìš”. 
ì§€ì›ìžì˜ ì‹¤ì œ ê²½í—˜ê³¼ ê¸°ìˆ ì„ í™œìš©í•œ í˜„ì‹¤ì ì´ê³  ì„¤ë“ë ¥ ìžˆëŠ” ë‹µë³€ì´ì–´ì•¼ í•©ë‹ˆë‹¤."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=400,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ëª¨ë²”ë‹µì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
        return "ëª¨ë²”ë‹µì•ˆ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

def evaluate_with_ragas(question: str, ground_truth: str, candidate_answers: List[Dict]) -> Dict:
    """RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë‹µë³€ í‰ê°€"""
    try:
        # RAGAS Dataset ìƒì„±
        data = {
            "question": [question] * len(candidate_answers),
            "contexts": [["ë©´ì ‘ ìƒí™©"]] * len(candidate_answers),  # RAGASì—ì„œ contexts í•„ìˆ˜
            "answer": [ans["content"] for ans in candidate_answers],
            "ground_truth": [ground_truth] * len(candidate_answers)
        }
        
        dataset = Dataset.from_dict(data)
        
        # RAGAS í‰ê°€ ì‹¤í–‰
        result = evaluate(
            dataset,
            metrics=[answer_relevancy, answer_correctness]
        )
        
        return result
        
    except Exception as e:
        print(f"âŒ RAGAS í‰ê°€ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

def save_results_to_excel(test_data: Dict, filename: str = None) -> str:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ìž¥"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"model_test/ragas_evaluation_results_{timestamp}.xlsx"
    
    try:
        # Create DataFrame
        df = pd.DataFrame([test_data])
        
        # Save to Excel
        df.to_excel(filename, index=False, engine='openpyxl')
        print(f"ðŸ“Š ê²°ê³¼ê°€ Excel íŒŒì¼ë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
        return filename
        
    except Exception as e:
        print(f"âŒ Excel ì €ìž¥ ì‹¤íŒ¨: {e}")
        return ""

def get_user_choice(options_dict, prompt_text):
    """ì‚¬ìš©ìžì—ê²Œ ì„ íƒì§€ë¥¼ ë³´ì—¬ì£¼ê³  ì„ íƒì„ ë°›ëŠ” í•¨ìˆ˜"""
    print(f"\n{prompt_text}")
    print("-" * 30)
    for key, (value, korean) in options_dict.items():
        print(f"{key}. {korean} ({value})")
    
    while True:
        try:
            choice = input("\nì„ íƒí•˜ì„¸ìš” (ë²ˆí˜¸ ìž…ë ¥): ").strip()
            if choice in options_dict:
                return options_dict[choice]
            print("âŒ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ìž…ë ¥í•´ì£¼ì„¸ìš”.")
        except EOFError:
            # ë¹„ëŒ€í™”í˜• í™˜ê²½ì—ì„œëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
            print("\n[ë¹„ëŒ€í™”í˜• ëª¨ë“œ] ê¸°ë³¸ê°’ ì‚¬ìš©: ë„¤ì´ë²„ ë°±ì—”ë“œ ê¸°ìˆ ë©´ì ‘")
            if "1" in options_dict:
                return options_dict["1"]
            return list(options_dict.values())[0]

def main():
    print("ðŸŽ¯ AI Model Test")
    print("=" * 50)
    
    # User selections
    print("ì„¤ì •ì„ ì„ íƒí•´ì£¼ì„¸ìš”:")
    
    # 1. Company selection
    company_id, company_name = get_user_choice(COMPANIES, "ðŸ“ íšŒì‚¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    
    # 2. Position selection  
    position, position_name = get_user_choice(POSITIONS, "ðŸ’¼ ì§êµ°ì„ ì„ íƒí•˜ì„¸ìš”:")
    
    # 3. Interviewer role selection
    interviewer_role, role_name = get_user_choice(INTERVIEWER_ROLES, "ðŸ‘¨â€ðŸ’¼ ë©´ì ‘ê´€ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:")
    
    print(f"\nâœ… ì„ íƒì™„ë£Œ: {company_name} {position_name} {role_name} ë©´ì ‘")
    
    # Initialize models
    print("\nðŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    question_gen = QuestionGenerator()
    answer_gen = AICandidateModel()
    
    # 1. Create single persona for consistency (MOVED TO FIRST)
    print(f"\nðŸ‘¤ íŽ˜ë¥´ì†Œë‚˜ ìƒì„± ì¤‘...")
    persona = answer_gen.create_persona_for_interview(company_id, position)
    if not persona:
        persona = answer_gen._create_default_persona(company_id, position)
    
    print(f"âœ… íŽ˜ë¥´ì†Œë‚˜: {persona.name}")
    print(f"   ê²½ë ¥: {persona.background.get('career_years', 'ë¯¸ì •')}ë…„")
    print(f"   ì£¼ìš”ê¸°ìˆ : {', '.join(persona.technical_skills[:3])}")
    
    # 2. Create a resume dictionary from the persona object
    resume_data = {
        "name": persona.name,
        "background": persona.background,
        "technical_skills": persona.technical_skills,
        "experiences": persona.experiences,
        "projects": persona.projects,
        "strengths": persona.strengths,
        "career_goal": persona.career_goal
    }
    
    # 3. Generate context-aware question
    print("\nðŸ“ ì§ˆë¬¸ ìƒì„± ì¤‘...")
    question_result = question_gen.generate_question_by_role(
        interviewer_role=interviewer_role,
        company_id=company_id, 
        user_resume=resume_data  # Use actual persona data instead of placeholder
    )
    question = question_result['question']
    
    print(f"\nðŸ“ Generated Question:")
    print(f"{question}")
    
    # 4. Generate context-aware ground truth answer
    print(f"\nðŸŽ¯ ëª¨ë²”ë‹µì•ˆ ìƒì„± ì¤‘...")
    ground_truth = generate_ground_truth_answer(question, company_id, position, interviewer_role, persona_resume=resume_data)
    print(f"\nðŸ“– ëª¨ë²”ë‹µì•ˆ:")
    print(f"{ground_truth}")
    print("-" * 50)
    
    # Map interviewer role to question type
    question_type_mapping = {
        "HR": QuestionType.HR,
        "TECH": QuestionType.TECH,
        "COLLABORATION": QuestionType.COLLABORATION
    }
    question_type = question_type_mapping.get(interviewer_role, QuestionType.TECH)
    
    # 2. Generate answers by level with same persona
    print(f"\nðŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘...")
    print("\nðŸ¤– AI Answers:")
    print("-" * 50)
    
    levels = [
        ("ðŸ”° ì´ˆê¸‰", QualityLevel.INADEQUATE), 
        ("ðŸ”¸ ì¤‘ê¸‰", QualityLevel.AVERAGE), 
        ("ðŸ”¥ ê³ ê¸‰", QualityLevel.EXCELLENT)
    ]
    
    candidate_answers = []
    
    for level_name, quality_level in levels:
        request = AnswerRequest(
            question_content=question,
            question_type=question_type,
            question_intent=f"{role_name} ì—­ëŸ‰ í‰ê°€",
            company_id=company_id,
            position=position, 
            quality_level=quality_level,
            llm_provider=LLMProvider.OPENAI_GPT4O
        )
        
        print(f"\n{level_name} ë‹µë³€ ìƒì„± ì¤‘...")
        response = answer_gen.generate_answer(request, persona=persona)  # ê°™ì€ íŽ˜ë¥´ì†Œë‚˜ ì‚¬ìš©
        print(f"\n{level_name}:")
        print(f"{response.answer_content}")
        print("-" * 30)
        
        # Store for RAGAS evaluation
        candidate_answers.append({
            "level": level_name,
            "content": response.answer_content
        })
    
    # 3. RAGAS Evaluation
    print(f"\nðŸ” RAGAS í‰ê°€ ì‹¤í–‰ ì¤‘...")
    ragas_result = evaluate_with_ragas(question, ground_truth, candidate_answers)
    
    # RAGAS ê²°ê³¼ ì²˜ë¦¬
    try:
        if isinstance(ragas_result, dict) and "error" in ragas_result:
            print(f"\nâŒ RAGAS í‰ê°€ ì˜¤ë¥˜: {ragas_result['error']}")
        else:
            print(f"\nðŸ“Š RAGAS í‰ê°€ ê²°ê³¼:")
            print("=" * 50)
            
            # RAGAS ê²°ê³¼ì—ì„œ ì ìˆ˜ ì¶”ì¶œ
            try:
                # RAGAS 0.3.x ë²„ì „ì—ì„œëŠ” ì§ì ‘ ì ‘ê·¼
                scores = ragas_result
                
                # Display results for each answer
                for i, ans in enumerate(candidate_answers):
                    print(f"\n{ans['level']} í‰ê°€:")
                    print(f"  â€¢ Answer Relevancy: {scores['answer_relevancy'][i]:.3f}")
                    print(f"  â€¢ Answer Correctness: {scores['answer_correctness'][i]:.3f}")  
                    print(f"  â€¢ ì¢…í•© ì ìˆ˜: {(scores['answer_relevancy'][i] + scores['answer_correctness'][i]) / 2:.3f}")
                    
                print(f"\nðŸ“ˆ í‰ê·  ì ìˆ˜:")
                print(f"  â€¢ Answer Relevancy: {sum(scores['answer_relevancy']) / len(scores['answer_relevancy']):.3f}")
                print(f"  â€¢ Answer Correctness: {sum(scores['answer_correctness']) / len(scores['answer_correctness']):.3f}")
                print(f"  â€¢ ì „ì²´ í‰ê· : {(sum(scores['answer_relevancy']) + sum(scores['answer_correctness'])) / (2 * len(scores['answer_relevancy'])):.3f}")
                
            except Exception as score_error:
                print(f"\n  ðŸ“Š RAGAS ì ìˆ˜ (ì›ë³¸ ê²°ê³¼):")
                print(f"     ê²°ê³¼ íƒ€ìž…: {type(ragas_result)}")
                print(f"     ê²°ê³¼ ë‚´ìš©: {ragas_result}")
                
    except Exception as e:
        print(f"\nâŒ RAGAS ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        print(f"   ê²°ê³¼ íƒ€ìž…: {type(ragas_result)}")
        print(f"   ê²°ê³¼: {ragas_result}")
    
    # 5. Save results to Excel
    print(f"\nðŸ’¾ ê²°ê³¼ ì €ìž¥ ì¤‘...")
    
    # Prepare test data for Excel
    test_data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "company": company_name,
        "position": position_name,
        "interviewer_role": role_name,
        "persona_name": persona.name,
        "persona_experience": persona.background.get('career_years', 'ë¯¸ì •'),
        "persona_skills": ', '.join(persona.technical_skills[:5]),
        "question": question,
        "ground_truth": ground_truth,
        "beginner_answer": candidate_answers[0]["content"] if len(candidate_answers) > 0 else "",
        "intermediate_answer": candidate_answers[1]["content"] if len(candidate_answers) > 1 else "",
        "advanced_answer": candidate_answers[2]["content"] if len(candidate_answers) > 2 else "",
    }
    
    # Add RAGAS scores if available
    try:
        if not isinstance(ragas_result, dict) or "error" not in ragas_result:
            # RAGAS ê²°ê³¼ êµ¬ì¡° í™•ì¸ ë° ì ìˆ˜ ì¶”ì¶œ
            print(f"ðŸ” RAGAS ê²°ê³¼ íƒ€ìž…: {type(ragas_result)}")
            
            # ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì ìˆ˜ ì ‘ê·¼ ì‹œë„
            scores = None
            if hasattr(ragas_result, 'scores'):
                scores = ragas_result.scores
                print(f"ðŸ” scores ì†ì„± ì ‘ê·¼ ì„±ê³µ")
            elif hasattr(ragas_result, '__getitem__'):
                try:
                    scores = dict(ragas_result)
                    print(f"ðŸ” dict ë³€í™˜ ì„±ê³µ")
                except:
                    pass
            elif isinstance(ragas_result, dict):
                scores = ragas_result
                print(f"ðŸ” dict ì§ì ‘ ì‚¬ìš©")
            
            if scores and 'answer_relevancy' in scores and 'answer_correctness' in scores:
                relevancy_list = scores['answer_relevancy']
                correctness_list = scores['answer_correctness']
                
                test_data.update({
                    "beginner_relevancy": relevancy_list[0] if len(relevancy_list) > 0 else 0,
                    "beginner_correctness": correctness_list[0] if len(correctness_list) > 0 else 0,
                    "intermediate_relevancy": relevancy_list[1] if len(relevancy_list) > 1 else 0,
                    "intermediate_correctness": correctness_list[1] if len(correctness_list) > 1 else 0,
                    "advanced_relevancy": relevancy_list[2] if len(relevancy_list) > 2 else 0,
                    "advanced_correctness": correctness_list[2] if len(correctness_list) > 2 else 0,
                    "avg_relevancy": sum(relevancy_list) / len(relevancy_list) if relevancy_list else 0,
                    "avg_correctness": sum(correctness_list) / len(correctness_list) if correctness_list else 0,
                })
                print(f"âœ… RAGAS ì ìˆ˜ ì¶”ê°€ ì™„ë£Œ")
            else:
                print(f"âŒ RAGAS ì ìˆ˜ êµ¬ì¡° ì¸ì‹ ì‹¤íŒ¨: {scores}")
                # ê¸°ë³¸ê°’ ì¶”ê°€
                test_data.update({
                    "beginner_relevancy": 0, "beginner_correctness": 0,
                    "intermediate_relevancy": 0, "intermediate_correctness": 0, 
                    "advanced_relevancy": 0, "advanced_correctness": 0,
                    "avg_relevancy": 0, "avg_correctness": 0,
                })
    except Exception as score_error:
        print(f"âš ï¸ ì ìˆ˜ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {score_error}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì¶”ê°€
        test_data.update({
            "beginner_relevancy": 0, "beginner_correctness": 0,
            "intermediate_relevancy": 0, "intermediate_correctness": 0,
            "advanced_relevancy": 0, "advanced_correctness": 0, 
            "avg_relevancy": 0, "avg_correctness": 0,
        })
        
    # Save to Excel
    excel_file = save_results_to_excel(test_data)
    
    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    if excel_file:
        print(f"ðŸ“ ê²°ê³¼ íŒŒì¼: {excel_file}")

def batch_test_scenarios():
    """ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ ìžë™ í…ŒìŠ¤íŠ¸ - ë°œí‘œìš© ë°ì´í„° ìƒì„±"""
    print("ðŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ - ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ ìžë™ ì‹¤í–‰")
    print("=" * 60)
    
    # Test scenarios
    scenarios = [
        ("naver", "ë„¤ì´ë²„", "backend", "ë°±ì—”ë“œ", "TECH", "ê¸°ìˆ "),
        ("kakao", "ì¹´ì¹´ì˜¤", "frontend", "í”„ë¡ íŠ¸ì—”ë“œ", "HR", "ì¸ì‚¬"),
        ("toss", "í† ìŠ¤", "ai", "AI/ML", "TECH", "ê¸°ìˆ "),
        ("coupang", "ì¿ íŒ¡", "data", "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤", "COLLABORATION", "í˜‘ì—…"),
        ("baemin", "ë°°ë‹¬ì˜ë¯¼ì¡±", "planning", "ê¸°íš", "HR", "ì¸ì‚¬"),
    ]
    
    all_results = []
    
    for i, (company_id, company_name, position, position_name, interviewer_role, role_name) in enumerate(scenarios, 1):
        print(f"\n[{i}/{len(scenarios)}] í…ŒìŠ¤íŠ¸ ì¤‘: {company_name} {position_name} {role_name} ë©´ì ‘")
        print("-" * 50)
        
        try:
            # Initialize models
            question_gen = QuestionGenerator()
            answer_gen = AICandidateModel()
            
            # Create persona
            persona = answer_gen.create_persona_for_interview(company_id, position)
            if not persona:
                persona = answer_gen._create_default_persona(company_id, position)
            
            print(f"âœ… íŽ˜ë¥´ì†Œë‚˜: {persona.name} ({persona.background.get('career_years', 'ë¯¸ì •')}ë…„)")
            
            # Create resume data
            resume_data = {
                "name": persona.name,
                "background": persona.background,
                "technical_skills": persona.technical_skills,
                "experiences": persona.experiences,
                "projects": persona.projects,
                "strengths": persona.strengths,
                "career_goal": persona.career_goal
            }
            
            # Generate question
            question_result = question_gen.generate_question_by_role(
                interviewer_role=interviewer_role,
                company_id=company_id, 
                user_resume=resume_data
            )
            question = question_result['question']
            print(f"ðŸ“ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ")
            
            # Generate ground truth
            ground_truth = generate_ground_truth_answer(question, company_id, position, interviewer_role, persona_resume=resume_data)
            print(f"ðŸŽ¯ ëª¨ë²”ë‹µì•ˆ ìƒì„± ì™„ë£Œ")
            
            # Generate answers by level
            question_type_mapping = {
                "HR": QuestionType.HR,
                "TECH": QuestionType.TECH,
                "COLLABORATION": QuestionType.COLLABORATION
            }
            question_type = question_type_mapping.get(interviewer_role, QuestionType.TECH)
            
            levels = [
                ("ì´ˆê¸‰", QualityLevel.INADEQUATE), 
                ("ì¤‘ê¸‰", QualityLevel.AVERAGE), 
                ("ê³ ê¸‰", QualityLevel.EXCELLENT)
            ]
            
            candidate_answers = []
            for level_name, quality_level in levels:
                request = AnswerRequest(
                    question_content=question,
                    question_type=question_type,
                    question_intent=f"{role_name} ì—­ëŸ‰ í‰ê°€",
                    company_id=company_id,
                    position=position, 
                    quality_level=quality_level,
                    llm_provider=LLMProvider.OPENAI_GPT4O
                )
                
                response = answer_gen.generate_answer(request, persona=persona)
                candidate_answers.append({
                    "level": level_name,
                    "content": response.answer_content
                })
            print(f"ðŸ¤– 3ê°€ì§€ ë ˆë²¨ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # RAGAS evaluation
            ragas_result = evaluate_with_ragas(question, ground_truth, candidate_answers)
            print(f"ðŸ” RAGAS í‰ê°€ ì™„ë£Œ")
            
            # Prepare result data
            result_data = {
                "scenario": f"{company_name}_{position_name}_{role_name}",
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "company": company_name,
                "position": position_name,
                "interviewer_role": role_name,
                "persona_name": persona.name,
                "persona_experience": persona.background.get('career_years', 'ë¯¸ì •'),
                "persona_skills": ', '.join(persona.technical_skills[:5]),
                "question": question,
                "ground_truth": ground_truth,
                "beginner_answer": candidate_answers[0]["content"] if len(candidate_answers) > 0 else "",
                "intermediate_answer": candidate_answers[1]["content"] if len(candidate_answers) > 1 else "",
                "advanced_answer": candidate_answers[2]["content"] if len(candidate_answers) > 2 else "",
            }
            
            # Add RAGAS scores
            try:
                if not isinstance(ragas_result, dict) or "error" not in ragas_result:
                    # RAGAS ê²°ê³¼ êµ¬ì¡° í™•ì¸ ë° ì ìˆ˜ ì¶”ì¶œ
                    scores = None
                    if hasattr(ragas_result, 'scores'):
                        scores = ragas_result.scores
                    elif hasattr(ragas_result, '__getitem__'):
                        try:
                            scores = dict(ragas_result)
                        except:
                            pass
                    elif isinstance(ragas_result, dict):
                        scores = ragas_result
                    
                    if scores and 'answer_relevancy' in scores and 'answer_correctness' in scores:
                        relevancy_list = scores['answer_relevancy']
                        correctness_list = scores['answer_correctness']
                        
                        result_data.update({
                            "beginner_relevancy": relevancy_list[0] if len(relevancy_list) > 0 else 0,
                            "beginner_correctness": correctness_list[0] if len(correctness_list) > 0 else 0,
                            "intermediate_relevancy": relevancy_list[1] if len(relevancy_list) > 1 else 0,
                            "intermediate_correctness": correctness_list[1] if len(correctness_list) > 1 else 0,
                            "advanced_relevancy": relevancy_list[2] if len(relevancy_list) > 2 else 0,
                            "advanced_correctness": correctness_list[2] if len(correctness_list) > 2 else 0,
                            "avg_relevancy": sum(relevancy_list) / len(relevancy_list) if relevancy_list else 0,
                            "avg_correctness": sum(correctness_list) / len(correctness_list) if correctness_list else 0,
                        })
                    else:
                        # ê¸°ë³¸ê°’ ì¶”ê°€
                        result_data.update({
                            "beginner_relevancy": 0, "beginner_correctness": 0,
                            "intermediate_relevancy": 0, "intermediate_correctness": 0,
                            "advanced_relevancy": 0, "advanced_correctness": 0,
                            "avg_relevancy": 0, "avg_correctness": 0,
                        })
            except Exception as e:
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì¶”ê°€
                result_data.update({
                    "beginner_relevancy": 0, "beginner_correctness": 0,
                    "intermediate_relevancy": 0, "intermediate_correctness": 0,
                    "advanced_relevancy": 0, "advanced_correctness": 0,
                    "avg_relevancy": 0, "avg_correctness": 0,
                })
                
            all_results.append(result_data)
            print(f"âœ… ì‹œë‚˜ë¦¬ì˜¤ {i} ì™„ë£Œ\n")
            
        except Exception as e:
            print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ {i} ì‹¤íŒ¨: {e}\n")
            continue
    
    # Save all results to Excel
    if all_results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"model_test/batch_test_results_{timestamp}.xlsx"
        
        try:
            df = pd.DataFrame(all_results)
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"ðŸŽ‰ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print(f"ðŸ“Š ì´ {len(all_results)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ê°€ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
            return filename
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ê²°ê³¼ ì €ìž¥ ì‹¤íŒ¨: {e}")
    
    return None

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--batch":
        batch_test_scenarios()
    else:
        main()