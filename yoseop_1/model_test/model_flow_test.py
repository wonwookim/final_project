#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëª¨ë¸ í”Œë¡œìš° í…ŒìŠ¤íŠ¸ íŒŒì¼
- íšŒì‚¬/ì§êµ° ì…ë ¥ â†’ DBì—ì„œ ì´ë ¥ì„œ ê°€ì ¸ì˜¤ê¸° â†’ í˜ë¥´ì†Œë‚˜ í˜•ì„± â†’ ì§ˆë¬¸ ìƒì„± â†’ ë‹µë³€ ìƒì„±
- í…ŒìŠ¤íŠ¸ ê³„íš ë° í‰ê°€ì„œìš© ëª¨ë¸ ë™ì‘ í™•ì¸
"""

import os
import sys
import logging
import pandas as pd
from datetime import datetime
from typing import Dict, List, Any

# Suppress all logs except CRITICAL
logging.getLogger().setLevel(logging.CRITICAL)
logging.getLogger("httpx").setLevel(logging.CRITICAL)
logging.getLogger("backend.services.existing_tables_service").setLevel(logging.CRITICAL)
logging.getLogger("llm.shared.utils").setLevel(logging.CRITICAL)

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# Import existing models
from llm.interviewer.question_generator import QuestionGenerator  
from llm.candidate.model import AICandidateModel
from llm.candidate.quality_controller import QualityLevel
from llm.shared.models import AnswerRequest, QuestionType, LLMProvider

# Available options (ê¸°ì¡´ê³¼ ë™ì¼)
COMPANIES = {
    "1": ("naver", "ë„¤ì´ë²„"),
    "2": ("kakao", "ì¹´ì¹´ì˜¤"),
    "3": ("toss", "í† ìŠ¤"),
    "4": ("coupang", "ì¿ íŒ¡"),
    "5": ("baemin", "ë°°ë‹¬ì˜ë¯¼ì¡±"),
    "6": ("daangn", "ë‹¹ê·¼ë§ˆì¼“"),
    "7": ("line", "ë¼ì¸")
}

POSITIONS = {
    "1": ("backend", "ë°±ì—”ë“œ"),
    "2": ("frontend", "í”„ë¡ íŠ¸ì—”ë“œ"),
    "3": ("data", "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤"),
    "4": ("ai", "AI/ML"),
    "5": ("planning", "ê¸°íš")
}

INTERVIEWER_ROLES = {
    "1": ("HR", "ì¸ì‚¬"),
    "2": ("TECH", "ê¸°ìˆ "),
    "3": ("COLLABORATION", "í˜‘ì—…")
}

def print_header(title: str):
    """í—¤ë” ì¶œë ¥"""
    print(f"\n{'='*80}")
    print(f"ğŸ¯ {title}")
    print('='*80)

def print_section(title: str):
    """ì„¹ì…˜ ì¶œë ¥"""
    print(f"\n{'â”€'*60}")
    print(f"ğŸ“Œ {title}")
    print('â”€'*60)

def get_user_choice(options_dict, prompt_text):
    """ì‚¬ìš©ìì—ê²Œ ì„ íƒì§€ë¥¼ ë³´ì—¬ì£¼ê³  ì„ íƒì„ ë°›ëŠ” í•¨ìˆ˜"""
    print(f"\n{prompt_text}")
    print("-" * 30)
    for key, (value, korean) in options_dict.items():
        print(f"{key}. {korean} ({value})")
    
    while True:
        try:
            choice = input("\nì„ íƒí•˜ì„¸ìš” (ë²ˆí˜¸ ì…ë ¥): ").strip()
            if choice in options_dict:
                return options_dict[choice]
            print("âŒ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except EOFError:
            # ë¹„ëŒ€í™”í˜• í™˜ê²½ì—ì„œëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
            print("\n[ë¹„ëŒ€í™”í˜• ëª¨ë“œ] ê¸°ë³¸ê°’ ì‚¬ìš©: ë„¤ì´ë²„ ë°±ì—”ë“œ ê¸°ìˆ ë©´ì ‘")
            if "1" in options_dict:
                return options_dict["1"]
            return list(options_dict.values())[0]

def display_persona_details(persona):
    """í˜ë¥´ì†Œë‚˜ ìƒì„¸ ì •ë³´ ì¶œë ¥ (ì „ì²´ í•„ë“œ)"""
    print(f"\nğŸ¤– ìƒì„±ëœ AI ì§€ì›ì í˜ë¥´ì†Œë‚˜ (ì „ì²´ ì •ë³´):")
    print("="*70)
    
    # ê¸°ë³¸ ì •ë³´
    print(f"ğŸ“‹ ê¸°ë³¸ ì •ë³´:")
    print(f"   â€¢ ì´ë¦„: {persona.name}")
    print(f"   â€¢ ìš”ì•½: {persona.summary}")
    print(f"   â€¢ ì´ë ¥ì„œ ID: {persona.resume_id}")
    print(f"   â€¢ ìƒì„± ëª¨ë¸: {persona.generated_by}")
    
    # ë°°ê²½ ì •ë³´
    print(f"\nğŸ‘¤ ë°°ê²½ ì •ë³´:")
    for key, value in persona.background.items():
        print(f"   â€¢ {key}: {value}")
    
    # ì„±ê²© íŠ¹ì„± (ì „ì²´)
    print(f"\nğŸ§  ì„±ê²© íŠ¹ì„±:")
    for trait in persona.personality_traits:
        print(f"   â€¢ {trait}")
    
    # ê¸°ìˆ  ìŠ¤í‚¬ (ì „ì²´)
    print(f"\nğŸ›  ê¸°ìˆ  ìŠ¤í‚¬:")
    for skill in persona.technical_skills:
        print(f"   â€¢ {skill}")
    
    # ê²½ë ¥ ì‚¬í•­ (ì „ì²´)
    print(f"\nğŸ’¼ ê²½ë ¥ ì‚¬í•­:")
    for i, exp in enumerate(persona.experiences, 1):
        if isinstance(exp, dict):
            print(f"   {i}. {exp.get('company', 'íšŒì‚¬ëª…')} - {exp.get('position', 'ì§ì±…')}")
            print(f"      ê¸°ê°„: {exp.get('duration', 'ë¯¸ì •')}")
            print(f"      ì„¤ëª…: {exp.get('description', 'ì„¤ëª… ì—†ìŒ')}")
            if 'achievements' in exp:
                print(f"      ì„±ê³¼: {exp['achievements']}")
            if 'tech_stack' in exp:
                print(f"      ê¸°ìˆ ìŠ¤íƒ: {', '.join(exp['tech_stack'])}")
        else:
            print(f"   {i}. {exp}")
        print()
    
    # í”„ë¡œì íŠ¸ (ì „ì²´)
    print(f"ğŸš€ í”„ë¡œì íŠ¸:")
    for i, project in enumerate(persona.projects, 1):
        if isinstance(project, dict):
            print(f"   {i}. {project.get('name', 'í”„ë¡œì íŠ¸ëª…')}")
            print(f"      ì„¤ëª…: {project.get('description', 'ì„¤ëª… ì—†ìŒ')}")
            if 'tech_used' in project:
                print(f"      ì‚¬ìš©ê¸°ìˆ : {', '.join(project['tech_used'])}")
            if 'achievements' in project:
                print(f"      ì„±ê³¼: {project['achievements']}")
            if 'challenges' in project:
                print(f"      ë„ì „ê³¼ì œ: {project['challenges']}")
        else:
            print(f"   {i}. {project}")
        print()
    
    # ê°•ì  (ì „ì²´)
    print(f"ğŸ’ª ê°•ì :")
    for strength in persona.strengths:
        print(f"   â€¢ {strength}")
    
    # ì•½ì /ê°œì„ ì  (ì „ì²´)
    print(f"\nğŸ” ì•½ì /ê°œì„ í•˜ê³  ì‹¶ì€ ì :")
    for weakness in persona.weaknesses:
        print(f"   â€¢ {weakness}")
    
    # ê°œì¸ì  ê²½í—˜ (ì¶”ë¡ ëœ)
    print(f"\nğŸ’­ ê°œì¸ì  ê²½í—˜/ê¹¨ë‹¬ìŒ:")
    for exp in persona.inferred_personal_experiences:
        if isinstance(exp, dict):
            for key, value in exp.items():
                print(f"   â€¢ {key}: {value}")
        else:
            print(f"   â€¢ {exp}")
    
    # ë™ê¸° ë° ëª©í‘œ
    print(f"\nğŸ’¡ ê°œë°œ ë™ê¸°:")
    print(f"   {persona.motivation}")
    
    print(f"\nğŸ¯ ì»¤ë¦¬ì–´ ëª©í‘œ:")
    print(f"   {persona.career_goal}")
    
    print(f"\nğŸ—£ ë©´ì ‘ ìŠ¤íƒ€ì¼:")
    print(f"   {persona.interview_style}")
    
    print("="*70)

def display_question_analysis(question: str, company_name: str, role_name: str):
    """ì§ˆë¬¸ ë¶„ì„ ì •ë³´ ì¶œë ¥"""
    print(f"\nğŸ“ ìƒì„±ëœ ë©´ì ‘ ì§ˆë¬¸:")
    print(f"   {question}")
    
    print(f"\nğŸ” ì§ˆë¬¸ ë¶„ì„:")
    print(f"   â€¢ íšŒì‚¬: {company_name} íŠ¹í™” ì§ˆë¬¸")
    print(f"   â€¢ ë©´ì ‘ê´€ íƒ€ì…: {role_name} ë©´ì ‘ê´€")
    print(f"   â€¢ ê°œì¸í™” ìˆ˜ì¤€: ì§€ì›ì ì´ë ¥ì„œ ê¸°ë°˜ ë§ì¶¤í˜•")

def generate_and_display_answers(question_gen, answer_gen, persona, question, 
                               interviewer_role, role_name, company_id, position):
    """ë‹µë³€ ìƒì„± ë° ì¶œë ¥"""
    print(f"\nğŸ¤– AI ì§€ì›ì ë‹µë³€ ìƒì„±:")
    
    # ì§ˆë¬¸ íƒ€ì… ë§¤í•‘
    question_type_mapping = {
        "HR": QuestionType.HR,
        "TECH": QuestionType.TECH,
        "COLLABORATION": QuestionType.COLLABORATION
    }
    question_type = question_type_mapping.get(interviewer_role, QuestionType.TECH)
    
    # ì—¬ëŸ¬ í’ˆì§ˆ ë ˆë²¨ë¡œ ë‹µë³€ ìƒì„±
    levels = [
        ("ğŸ”° ì´ˆê¸‰ì ìˆ˜ì¤€ (ë¶€ì¡±í•¨)", QualityLevel.INADEQUATE), 
        ("ğŸ”¸ ì¤‘ê¸‰ì ìˆ˜ì¤€ (í‰ê· )", QualityLevel.AVERAGE), 
        ("ğŸ”¥ ê³ ê¸‰ì ìˆ˜ì¤€ (ìš°ìˆ˜í•¨)", QualityLevel.EXCELLENT)
    ]
    
    answers = []
    
    for level_name, quality_level in levels:
        print(f"\n{'-'*50}")
        print(f"{level_name}")
        print(f"{'-'*50}")
        
        request = AnswerRequest(
            question_content=question,
            question_type=question_type,
            question_intent=f"{role_name} ì—­ëŸ‰ í‰ê°€",
            company_id=company_id,
            position=position, 
            quality_level=quality_level,
            llm_provider=LLMProvider.OPENAI_GPT4O
        )
        
        print("ë‹µë³€ ìƒì„± ì¤‘...")
        response = answer_gen.generate_answer(request, persona=persona)
        
        print(f"\nğŸ“ ë‹µë³€:")
        print(f"{response.answer_content}")
        
        answers.append({
            "level": level_name,
            "content": response.answer_content,
            "quality": quality_level.value
        })
    
    return answers

def save_test_results(company_name, position_name, role_name, persona, question, answers):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"model_test/model_flow_test_results_{timestamp}.xlsx"
    
    # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
    result_data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "company": company_name,
        "position": position_name,
        "interviewer_role": role_name,
        
        # í˜ë¥´ì†Œë‚˜ ì •ë³´
        "persona_name": persona.name,
        "persona_summary": persona.summary,
        "persona_personality_traits": ', '.join(persona.personality_traits[:5]),
        "persona_career_years": persona.background.get('career_years', 'ë¯¸ì •'),
        "persona_education": persona.background.get('education', 'ë¯¸ì •'),
        "persona_skills": ', '.join(persona.technical_skills[:10]),
        "persona_strengths": ', '.join(persona.strengths[:5]),
        "persona_weaknesses": ', '.join(persona.weaknesses[:3]),
        "persona_motivation": persona.motivation,
        "persona_goal": persona.career_goal,
        "persona_interview_style": persona.interview_style,
        "persona_experiences": ' | '.join([str(exp) for exp in persona.experiences[:3]]),
        "persona_projects": ' | '.join([str(proj) for proj in persona.projects[:3]]),
        
        # ì§ˆë¬¸ ë° ë‹µë³€
        "generated_question": question,
        "beginner_answer": answers[0]["content"] if len(answers) > 0 else "",
        "intermediate_answer": answers[1]["content"] if len(answers) > 1 else "",
        "advanced_answer": answers[2]["content"] if len(answers) > 2 else "",
    }
    
    try:
        df = pd.DataFrame([result_data])
        df.to_excel(filename, index=False, engine='openpyxl')
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
        return filename
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return ""

def run_model_flow_test():
    """ëª¨ë¸ í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print_header("AI ë©´ì ‘ ì‹œìŠ¤í…œ ëª¨ë¸ í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ê³„íš ë° í‰ê°€ì„œìš© ëª¨ë¸ ë™ì‘ ê³¼ì • í™•ì¸")
    
    # 1. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    print_section("1ë‹¨ê³„: ë©´ì ‘ ì„¤ì • ì„ íƒ")
    
    company_id, company_name = get_user_choice(COMPANIES, "ğŸ“ íšŒì‚¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    position, position_name = get_user_choice(POSITIONS, "ğŸ’¼ ì§êµ°ì„ ì„ íƒí•˜ì„¸ìš”:")
    interviewer_role, role_name = get_user_choice(INTERVIEWER_ROLES, "ğŸ‘¨â€ğŸ’¼ ë©´ì ‘ê´€ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:")
    
    print(f"\nâœ… ì„ íƒ ì™„ë£Œ: {company_name} {position_name} {role_name} ë©´ì ‘")
    
    # 2. ëª¨ë¸ ì´ˆê¸°í™”
    print_section("2ë‹¨ê³„: ëª¨ë¸ ì´ˆê¸°í™”")
    print("ğŸ”§ ë©´ì ‘ê´€ ëª¨ë¸ ë° ì§€ì›ì ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    question_gen = QuestionGenerator()
    answer_gen = AICandidateModel()
    print("âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # 3. í˜ë¥´ì†Œë‚˜ ìƒì„± (DBì—ì„œ ì´ë ¥ì„œ ê¸°ë°˜)
    print_section("3ë‹¨ê³„: AI ì§€ì›ì í˜ë¥´ì†Œë‚˜ ìƒì„±")
    print(f"ğŸ“‹ {company_name} {position_name} ì§êµ° ì§€ì›ì í˜ë¥´ì†Œë‚˜ë¥¼ DBì—ì„œ ìƒì„± ì¤‘...")
    
    persona = answer_gen.create_persona_for_interview(company_id, position)
    if not persona:
        print("âš ï¸ íŠ¹í™” í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ì‚¬ìš©")
        persona = answer_gen._create_default_persona(company_id, position)
    
    print("âœ… í˜ë¥´ì†Œë‚˜ ìƒì„± ì™„ë£Œ")
    display_persona_details(persona)
    
    # 4. ì´ë ¥ì„œ ë°ì´í„° êµ¬ì„±
    print_section("4ë‹¨ê³„: ì´ë ¥ì„œ ë°ì´í„° êµ¬ì„±")
    print("ğŸ“ ìƒì„±ëœ í˜ë¥´ì†Œë‚˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ë ¥ì„œ ë°ì´í„° êµ¬ì„± ì¤‘...")
    
    resume_data = {
        "name": persona.name,
        "background": persona.background,
        "technical_skills": persona.technical_skills,
        "experiences": persona.experiences,
        "projects": persona.projects,
        "strengths": persona.strengths,
        "career_goal": persona.career_goal
    }
    
    print("âœ… ì´ë ¥ì„œ ë°ì´í„° êµ¬ì„± ì™„ë£Œ")
    print(f"   â€¢ ì´ë¦„: {resume_data['name']}")
    print(f"   â€¢ ê²½ë ¥: {resume_data['background'].get('career_years', 'ë¯¸ì •')}ë…„")
    print(f"   â€¢ ì£¼ìš” ê¸°ìˆ : {', '.join(resume_data['technical_skills'][:5])}")
    
    # 5. ë©´ì ‘ê´€ ì§ˆë¬¸ ìƒì„±
    print_section("5ë‹¨ê³„: ë©´ì ‘ê´€ ì§ˆë¬¸ ìƒì„±")
    print(f"ğŸ¯ {company_name} {role_name} ë©´ì ‘ê´€ì´ ì´ë ¥ì„œë¥¼ ë¶„ì„í•˜ì—¬ ë§ì¶¤ ì§ˆë¬¸ ìƒì„± ì¤‘...")
    
    question_result = question_gen.generate_question_by_role(
        interviewer_role=interviewer_role,
        company_id=company_id, 
        user_resume=resume_data
    )
    question = question_result['question']
    
    print("âœ… ì§ˆë¬¸ ìƒì„± ì™„ë£Œ")
    display_question_analysis(question, company_name, role_name)
    
    # 6. AI ì§€ì›ì ë‹µë³€ ìƒì„± (ì—¬ëŸ¬ ë ˆë²¨)
    print_section("6ë‹¨ê³„: AI ì§€ì›ì ë‹µë³€ ìƒì„±")
    print("ğŸ¤– ë™ì¼í•œ í˜ë¥´ì†Œë‚˜ê°€ ì„œë¡œ ë‹¤ë¥¸ í’ˆì§ˆ ë ˆë²¨ë¡œ ë‹µë³€ ìƒì„±...")
    
    answers = generate_and_display_answers(
        question_gen, answer_gen, persona, question,
        interviewer_role, role_name, company_id, position
    )
    
    # 7. ê²°ê³¼ ì €ì¥
    print_section("7ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
    filename = save_test_results(company_name, position_name, role_name, persona, question, answers)
    
    # 8. ìš”ì•½ ì¶œë ¥
    print_section("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ëª¨ë¸ ë™ì‘ ìš”ì•½")
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {company_name} {position_name} {role_name} ë©´ì ‘")
    print(f"ğŸ“Š í™•ì¸ëœ ëª¨ë¸ ê¸°ëŠ¥:")
    print(f"   âœ… íšŒì‚¬/ì§êµ° ì…ë ¥ì— ë”°ë¥¸ í˜ë¥´ì†Œë‚˜ ìë™ ìƒì„±")
    print(f"   âœ… í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ê°œì¸í™”ëœ ì´ë ¥ì„œ ë°ì´í„° êµ¬ì„±") 
    print(f"   âœ… ì´ë ¥ì„œ ë¶„ì„ì„ í†µí•œ ë§ì¶¤í˜• ì§ˆë¬¸ ìƒì„±")
    print(f"   âœ… í˜ë¥´ì†Œë‚˜ ì¼ê´€ì„± ìœ ì§€í•˜ë©° ë‹¤ì–‘í•œ í’ˆì§ˆ ë ˆë²¨ ë‹µë³€ ìƒì„±")
    print(f"   âœ… ì „ì²´ ë©´ì ‘ í”„ë¡œì„¸ìŠ¤ ì‹œë®¬ë ˆì´ì…˜")
    
    if filename:
        print(f"\nğŸ“ ìƒì„¸ ê²°ê³¼ íŒŒì¼: {filename}")
        print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ê³„íš ë° í‰ê°€ì„œì— í™œìš© ê°€ëŠ¥í•œ ë°ì´í„° í¬í•¨")

def batch_test_all_scenarios():
    """ëª¨ë“  ì£¼ìš” ì‹œë‚˜ë¦¬ì˜¤ ë°°ì¹˜ í…ŒìŠ¤íŠ¸"""
    print_header("ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ëª¨ë“œ - ëª¨ë“  ì£¼ìš” ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
    
    # ì£¼ìš” í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë“¤
    test_scenarios = [
        ("naver", "ë„¤ì´ë²„", "backend", "ë°±ì—”ë“œ", "TECH", "ê¸°ìˆ "),
        ("kakao", "ì¹´ì¹´ì˜¤", "frontend", "í”„ë¡ íŠ¸ì—”ë“œ", "HR", "ì¸ì‚¬"),
        ("toss", "í† ìŠ¤", "ai", "AI/ML", "TECH", "ê¸°ìˆ "),
        ("coupang", "ì¿ íŒ¡", "data", "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤", "COLLABORATION", "í˜‘ì—…"),
    ]
    
    all_results = []
    
    for i, (company_id, company_name, position, position_name, interviewer_role, role_name) in enumerate(test_scenarios, 1):
        print(f"\nğŸ¯ ì‹œë‚˜ë¦¬ì˜¤ {i}/{len(test_scenarios)}: {company_name} {position_name} {role_name}")
        print("â”€" * 60)
        
        try:
            # ëª¨ë¸ ì´ˆê¸°í™”
            question_gen = QuestionGenerator()
            answer_gen = AICandidateModel()
            
            # í˜ë¥´ì†Œë‚˜ ìƒì„±
            persona = answer_gen.create_persona_for_interview(company_id, position)
            if not persona:
                persona = answer_gen._create_default_persona(company_id, position)
            
            print(f"âœ… í˜ë¥´ì†Œë‚˜: {persona.name}")
            
            # ì´ë ¥ì„œ ë°ì´í„° êµ¬ì„±
            resume_data = {
                "name": persona.name,
                "background": persona.background,
                "technical_skills": persona.technical_skills,
                "experiences": persona.experiences,
                "projects": persona.projects,
                "strengths": persona.strengths,
                "career_goal": persona.career_goal
            }
            
            # ì§ˆë¬¸ ìƒì„±
            question_result = question_gen.generate_question_by_role(
                interviewer_role=interviewer_role,
                company_id=company_id, 
                user_resume=resume_data
            )
            question = question_result['question']
            print(f"âœ… ì§ˆë¬¸ ìƒì„± ì™„ë£Œ")
            
            # ë‹µë³€ ìƒì„± (ê°„ë‹¨ ë²„ì „)
            question_type_mapping = {
                "HR": QuestionType.HR,
                "TECH": QuestionType.TECH,
                "COLLABORATION": QuestionType.COLLABORATION
            }
            question_type = question_type_mapping.get(interviewer_role, QuestionType.TECH)
            
            levels = [QualityLevel.INADEQUATE, QualityLevel.AVERAGE, QualityLevel.EXCELLENT]
            answers = []
            
            for quality_level in levels:
                request = AnswerRequest(
                    question_content=question,
                    question_type=question_type,
                    question_intent=f"{role_name} ì—­ëŸ‰ í‰ê°€",
                    company_id=company_id,
                    position=position,
                    quality_level=quality_level,
                    llm_provider=LLMProvider.OPENAI_GPT4O
                )
                
                response = answer_gen.generate_answer(request, persona=persona)
                answers.append(response.answer_content)
            
            print(f"âœ… 3ê°€ì§€ ë ˆë²¨ ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # ê²°ê³¼ ì €ì¥
            result = {
                "scenario": f"{company_name}_{position_name}_{role_name}",
                "company": company_name,
                "position": position_name,
                "interviewer_role": role_name,
                "persona_name": persona.name,
                "persona_skills": ', '.join(persona.technical_skills[:8]),
                "question": question,
                "beginner_answer": answers[0] if len(answers) > 0 else "",
                "intermediate_answer": answers[1] if len(answers) > 1 else "",
                "advanced_answer": answers[2] if len(answers) > 2 else "",
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            all_results.append(result)
            print(f"âœ… ì‹œë‚˜ë¦¬ì˜¤ {i} ì™„ë£Œ\n")
            
        except Exception as e:
            print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ {i} ì‹¤íŒ¨: {e}\n")
            continue
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    if all_results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"model_test/batch_model_flow_results_{timestamp}.xlsx"
        
        try:
            df = pd.DataFrame(all_results)
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"ğŸ‰ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ {len(all_results)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ì €ì¥: {filename}")
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ AI ë©´ì ‘ ì‹œìŠ¤í…œ ëª¨ë¸ í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ê³„íš ë° í‰ê°€ì„œìš© ëª¨ë¸ ë™ì‘ ê²€ì¦")
    print("=" * 80)
    
    mode_choice = input("\nì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:\n1. ë‹¨ì¼ í…ŒìŠ¤íŠ¸ (ìƒì„¸)\n2. ë°°ì¹˜ í…ŒìŠ¤íŠ¸ (ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤)\n\nì„ íƒ (1 or 2): ").strip()
    
    if mode_choice == "2":
        batch_test_all_scenarios()
    else:
        run_model_flow_test()

if __name__ == "__main__":
    main()