# ğŸ‘¨â€ğŸ’» ê°œë°œì ê°€ì´ë“œ

AI ë©´ì ‘ ì‹œìŠ¤í…œì˜ ì½”ë“œ êµ¬ì¡°, í™•ì¥ ë°©ë²•, ê°œë°œ í™˜ê²½ ì„¤ì •ì— ëŒ€í•œ ì¢…í•© ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
- [ì•„í‚¤í…ì²˜ ê°œìš”](#ì•„í‚¤í…ì²˜-ê°œìš”)
- [ì½”ë“œ êµ¬ì¡°](#ì½”ë“œ-êµ¬ì¡°)
- [í•µì‹¬ ì»´í¬ë„ŒíŠ¸](#í•µì‹¬-ì»´í¬ë„ŒíŠ¸)
- [ê°œë°œ í™˜ê²½ ì„¤ì •](#ê°œë°œ-í™˜ê²½-ì„¤ì •)
- [í™•ì¥ ê°€ì´ë“œ](#í™•ì¥-ê°€ì´ë“œ)
- [ë””ë²„ê¹… ë° í…ŒìŠ¤íŠ¸](#ë””ë²„ê¹…-ë°-í…ŒìŠ¤íŠ¸)
- [ë°°í¬ ê°€ì´ë“œ](#ë°°í¬-ê°€ì´ë“œ)

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ê°œìš”

### ì‹œìŠ¤í…œ êµ¬ì¡°
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend       â”‚    â”‚   External      â”‚
â”‚   (HTML/JS)     â”‚â—„â”€â”€â–ºâ”‚   (Flask)       â”‚â—„â”€â”€â–ºâ”‚   (OpenAI API)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   File System   â”‚
                       â”‚   (uploads/)    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ê°„ ìƒí˜¸ì‘ìš©
```
Document Upload â†’ Document Processor â†’ User Profile â†’ Question Generation â†’ Interview Session â†’ Evaluation
```

### ê¸°ìˆ  ìŠ¤íƒ
- **Backend**: Python 3.8+, Flask 3.0.3
- **AI Engine**: OpenAI GPT-4o-mini
- **Document Processing**: PyPDF2, python-docx
- **Frontend**: Vanilla JavaScript, HTML5, CSS3
- **Data Storage**: JSON files, In-memory sessions

## ğŸ“ ì½”ë“œ êµ¬ì¡°

### ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„¸
```
final_Q_test/
â”œâ”€â”€ backend/                   # FastAPI ì„œë²„ (ê³„ì¸µí™”)
â”œâ”€â”€ llm/                       # ğŸ†• ëª¨ë“ˆí˜• AI/LLM êµ¬ì¡°
â”‚   â”œâ”€â”€ session/               # ğŸ†• ì„¸ì…˜ ê´€ë¦¬ ëª¨ë“ˆ (í•µì‹¬!)
â”‚   â”‚   â”œâ”€â”€ manager.py         # í†µí•© ì„¸ì…˜ ê´€ë¦¬ì (ì¼ë°˜/ë¹„êµ ë©´ì ‘)
â”‚   â”‚   â”œâ”€â”€ base_session.py    # ê¸°ë³¸ ë©´ì ‘ ì„¸ì…˜ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ comparison_session.py # ë¹„êµ ë©´ì ‘ ì„¸ì…˜ ë¡œì§
â”‚   â”‚   â””â”€â”€ models.py          # ì„¸ì…˜ ê´€ë ¨ ë°ì´í„° ëª¨ë¸
â”‚   â”œâ”€â”€ interviewer/           # ë©´ì ‘ê´€ ëª¨ë“ˆ (ì§ˆë¬¸ ìƒì„±)
â”‚   â”œâ”€â”€ candidate/             # ì§€ì›ì ëª¨ë“ˆ (AI ë‹µë³€)
â”‚   â”œâ”€â”€ feedback/              # í”¼ë“œë°± ëª¨ë“ˆ (ë‹µë³€ í‰ê°€)
â”‚   â”œâ”€â”€ shared/                # ê³µìš© ëª¨ë“ˆ
â”‚   â””â”€â”€ core/                  # LLM ê´€ë¦¬ ë° ê¸°íƒ€ ê³µí†µ ê¸°ëŠ¥
â”‚       â””â”€â”€ llm_manager.py     # LLM ê´€ë¦¬ì
â”œâ”€â”€ database/                  # ë°ì´í„°ë² ì´ìŠ¤ ë ˆì´ì–´
â”œâ”€â”€ scripts/                   # ì‹¤í–‰ ë° ë„êµ¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ uploads/                   # íŒŒì¼ ì—…ë¡œë“œ ì €ì¥ì†Œ
â”œâ”€â”€ logs/                      # ë¡œê·¸ íŒŒì¼
â”œâ”€â”€ requirements.txt           # Python ì˜ì¡´ì„±
â”œâ”€â”€ .env.example               # í™˜ê²½ë³€ìˆ˜ ì˜ˆì‹œ
â””â”€â”€ run.py                     # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
```

### ëª¨ë“ˆ ì˜ì¡´ì„± ë‹¤ì´ì–´ê·¸ë¨
```
backend/main.py
    â†“
backend/services/interview_service.py
    â†“
llm/session/manager.py
    â†“
llm/session/base_session.py
llm/session/comparison_session.py
    â†“
llm/interviewer/service.py
llm/candidate/model.py
llm/feedback/service.py
    â†“
llm/shared/models.py
llm/shared/constants.py
llm/shared/utils.py
llm/shared/config.py
llm/shared/company_data_loader.py
llm/core/llm_manager.py
```

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 1. Document Processor (`core/document_processor.py`)

**ì±…ì„**: ë¬¸ì„œ ì—…ë¡œë“œ, í…ìŠ¤íŠ¸ ì¶”ì¶œ, ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±

```python
class DocumentProcessor:
    def __init__(self):
        self.supported_formats = ALLOWED_FILE_EXTENSIONS
        
    def extract_text_from_file(self, file_content: bytes, file_type: str) -> str:
        """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        
    def create_user_profile(self, documents: Dict[str, str]) -> UserProfile:
        """ë¬¸ì„œ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±"""
        
    def analyze_document_content(self, text: str) -> Dict[str, Any]:
        """ë¬¸ì„œ ë‚´ìš© ë¶„ì„"""
```

**ì£¼ìš” ê¸°ëŠ¥**:
- PDF, DOCX, DOC, TXT íŒŒì¼ ì²˜ë¦¬
- í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
- AI ê¸°ë°˜ í”„ë¡œí•„ ìƒì„±
- ê¸°ìˆ  ìŠ¤í‚¬, í”„ë¡œì íŠ¸, ê²½í—˜ ë¶„ë¥˜

### 2. Session Manager (`llm/session/manager.py`)

**ì±…ì„**: ëª¨ë“  ë©´ì ‘ ì„¸ì…˜(ì¼ë°˜ ë° ë¹„êµ) ê´€ë¦¬, ì§ˆë¬¸ ìƒì„± ë° ë‹µë³€ ì²˜ë¦¬ ìœ„ì„

```python
class SessionManager:
    def __init__(self):
        self.base_session_manager = BaseInterviewSession()
        self.comparison_session_manager = ComparisonSessionManager()
        
    def start_interview(self, company_id: str, position: str, candidate_name: str) -> str:
        """ì¼ë°˜ ë©´ì ‘ ì‹œì‘"""
        
    def get_next_question(self, session_id: str) -> Optional[Dict[str, Any]]:
        """ë‹¤ìŒ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        
    def submit_answer(self, session_id: str, answer_content: str) -> Dict[str, Any]:
        """ë‹µë³€ ì œì¶œ"""
        
    def start_comparison_interview(self, company_id: str, position: str, user_name: str, ai_name: str = "ì¶˜ì‹ì´") -> str:
        """AI ë¹„êµ ë©´ì ‘ ì‹œì‘"""
```

**ì£¼ìš” ê¸°ëŠ¥**: 
- ì¼ë°˜ ë©´ì ‘ ì„¸ì…˜ ë° AI ë¹„êµ ë©´ì ‘ ì„¸ì…˜ í†µí•© ê´€ë¦¬
- ê° ì„¸ì…˜ ìœ í˜•ì— ë§ëŠ” ì§ˆë¬¸ ìƒì„± ë° ë‹µë³€ ì²˜ë¦¬ ë¡œì§ ìœ„ì„
- ì„¸ì…˜ ìƒíƒœ ì¶”ì  ë° ê²°ê³¼ ì œê³µ

### 3. AI Candidate Model (`llm/candidate/model.py`)

**ì±…ì„**: AI ì§€ì›ì ëª¨ë¸ë§, ë‹µë³€ ìƒì„±, ê²½ìŸ ë©´ì ‘ ê´€ë¦¬

```python
class AICandidateModel:
    def __init__(self):
        self.personas = self.load_personas()
        
    def generate_answer(self, answer_request: AnswerRequest) -> AnswerResponse:
        """AI ë‹µë³€ ìƒì„±"""
        
    def get_ai_name(self, llm_provider: LLMProvider) -> str:
        """AI ì´ë¦„ ë°˜í™˜ (ì¶˜ì‹ì´ ê³ ì •)"""
        
    def start_ai_interview(self, company: str, position: str) -> str:
        """AI ë©´ì ‘ ì„¸ì…˜ ì‹œì‘"""
```

**AI ë‹µë³€ íŠ¹ì§•**:
- í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì¼ê´€ëœ ë‹µë³€
- ê¸°ì—…ë³„ ë§ì¶¤í˜• ë‚´ìš©
- ì‹ ë¢°ë„ ì ìˆ˜ ì œê³µ

### 4. LLM Manager (`core/llm_manager.py`)

**ì±…ì„**: OpenAI API ê´€ë¦¬, ì—ëŸ¬ ì²˜ë¦¬, ì¬ì‹œë„ ë¡œì§

```python
class LLMManager:
    def __init__(self):
        self.rate_limiter = RateLimiter()
        
    def generate_response(self, prompt: str, **kwargs) -> str:
        """LLM ì‘ë‹µ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        
    def handle_api_error(self, error: Exception) -> str:
        """API ì—ëŸ¬ ì²˜ë¦¬"""
```

**ê¸°ëŠ¥**:
- Rate limiting (ë¶„ë‹¹ 60íšŒ)
- ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„
- í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›

### 5. Conversation Context (`llm/interviewer/conversation_context.py`)

**ì±…ì„**: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬, ì¤‘ë³µ ì§ˆë¬¸ ë°©ì§€

```python
class ConversationContext:
    def __init__(self):
        self.similarity_threshold = 0.5
        
    def is_duplicate_question(self, new_question: str, existing_questions: List[str]) -> bool:
        """ì¤‘ë³µ ì§ˆë¬¸ ê²€ì‚¬"""
        
    def calculate_semantic_similarity(self, q1: str, q2: str) -> float:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°"""
```

**ì¤‘ë³µ ë°©ì§€ ì•Œê³ ë¦¬ì¦˜**:
- ì˜ë„ ìœ ì‚¬ë„ (60%) + í‚¤ì›Œë“œ ìœ ì‚¬ë„ (30%) + êµ¬ì¡° ìœ ì‚¬ë„ (10%)
- ì„ê³„ê°’ 50% ì´ìƒ ì‹œ ì¤‘ë³µ íŒì •

## ğŸš€ ê°œë°œ í™˜ê²½ ì„¤ì •

### 1. ë¡œì»¬ ê°œë°œ í™˜ê²½

```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
git clone <repository-url>
cd final_Q_test

# 2. ê°€ìƒí™˜ê²½ ì„¤ì •
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# 4. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ì—ì„œ OPENAI_API_KEY ì„¤ì •

# 5. ê°œë°œ ì„œë²„ ì‹¤í–‰
python backend/main.py
```

### 2. ê°œë°œ ë„êµ¬ ì„¤ì •

#### VS Code í™•ì¥ í”„ë¡œê·¸ë¨
```json
{
  "recommendations": [
    "ms-python.python",
    "ms-python.flake8",
    "ms-python.black-formatter",
    "ms-python.pylint",
    "ms-toolsai.jupyter"
  ]
}
```

#### Pre-commit í›… ì„¤ì •
```bash
# pre-commit ì„¤ì¹˜
pip install pre-commit

# í›… ì„¤ì •
pre-commit install

# .pre-commit-config.yaml ìƒì„±
cat > .pre-commit-config.yaml << EOF
repos:
  - repo: https://github.com/psf/black
    rev: 22.3.0
    hooks:
      - id: black
  - repo: https://github.com/pycqa/flake8
    rev: 4.0.1
    hooks:
      - id: flake8
EOF
```

### 3. ë””ë²„ê¹… ì„¤ì •

#### Flask ë””ë²„ê·¸ ëª¨ë“œ
```python
# web/app.py
if __name__ == "__main__":
    app.run(
        host="0.0.0.0",
        port=8888,
        debug=True,  # ê°œë°œ ì‹œì—ë§Œ True
        threaded=True
    )
```

#### ë¡œê¹… ì„¤ì •
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log'),
        logging.StreamHandler()
    ]
)
```

## ğŸ”§ í™•ì¥ ê°€ì´ë“œ

### 1. ìƒˆë¡œìš´ ê¸°ì—… ì¶”ê°€

#### ë‹¨ê³„ 1: ê¸°ì—… ë°ì´í„° ì¶”ê°€
```json
// data/companies_data.json
{
  "companies": [
    {
      "id": "new_company",
      "name": "ìƒˆë¡œìš´ ê¸°ì—…",
      "talent_profile": "ì¸ì¬ìƒ ì„¤ëª…",
      "core_competencies": ["ì—­ëŸ‰1", "ì—­ëŸ‰2"],
      "tech_focus": ["ê¸°ìˆ 1", "ê¸°ìˆ 2"],
      "interview_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
      "company_culture": {
        "work_style": "ì—…ë¬´ ìŠ¤íƒ€ì¼",
        "decision_making": "ì˜ì‚¬ê²°ì • ë°©ì‹",
        "growth_support": "ì„±ì¥ ì§€ì›",
        "core_values": ["ê°€ì¹˜1", "ê°€ì¹˜2"]
      },
      "interviewer_personas": {
        "tech_lead": {
          "name": "ê¸°ìˆ  ë¦¬ë“œ ì´ë¦„",
          "role": "ì—­í• ",
          "experience": "ê²½í—˜",
          "personality": "ì„±ê²©",
          "speaking_style": "ë§í•˜ëŠ” ìŠ¤íƒ€ì¼"
        }
      }
    }
  ]
}
```

#### ë‹¨ê³„ 2: ìƒìˆ˜ ì—…ë°ì´íŠ¸
```python
# core/constants.py
SUPPORTED_COMPANIES = [
    'naver', 'kakao', 'line', 'coupang', 
    'baemin', 'danggeun', 'toss', 'new_company'  # ì¶”ê°€
]
```

#### ë‹¨ê³„ 3: í”„ë¡ íŠ¸ì—”ë“œ ì—…ë°ì´íŠ¸
```html
<!-- web/app.py HTML í…œí”Œë¦¿ -->
<select id="company">
    <option value="">íšŒì‚¬ ì„ íƒ...</option>
    <option value="new_company">ìƒˆë¡œìš´ ê¸°ì—…</option>
</select>
```

### 2. ìƒˆë¡œìš´ ì§ˆë¬¸ íƒ€ì… ì¶”ê°€

#### ë‹¨ê³„ 1: ì§ˆë¬¸ íƒ€ì… ì •ì˜
```python
# core/interview_system.py
class QuestionType(Enum):
    INTRO = "ìê¸°ì†Œê°œ"
    MOTIVATION = "ì§€ì›ë™ê¸°"
    HR = "ì¸ì‚¬"
    TECH = "ê¸°ìˆ "
    COLLABORATION = "í˜‘ì—…"
    FOLLOWUP = "ì‹¬í™”"
    CREATIVITY = "ì°½ì˜ì„±"  # ìƒˆë¡œìš´ íƒ€ì… ì¶”ê°€
```

#### ë‹¨ê³„ 2: ì§ˆë¬¸ ê³„íš ì—…ë°ì´íŠ¸
```python
# core/interview_system.py
class InterviewSession:
    def __init__(self, company_id: str, position: str, candidate_name: str):
        self.question_plan = [
            # ê¸°ì¡´ ì§ˆë¬¸ë“¤...
            {"type": QuestionType.CREATIVITY, "fixed": False}  # ì¶”ê°€
        ]
```

#### ë‹¨ê³„ 3: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€
```python
# core/prompt_templates.py
class PromptTemplates:
    @staticmethod
    def get_creativity_question_prompt(company_data: dict, **kwargs) -> str:
        return f"""
        {company_data['name']}ì˜ ì°½ì˜ì„± í‰ê°€ë¥¼ ìœ„í•œ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.
        
        í‰ê°€ í¬ì¸íŠ¸:
        - ì°½ì˜ì  ì‚¬ê³  ëŠ¥ë ¥
        - í˜ì‹ ì  ì•„ì´ë””ì–´ ì œì•ˆ
        - ë¬¸ì œ í•´ê²°ì˜ ë…ì°½ì„±
        """
```

### 3. ìƒˆë¡œìš´ í‰ê°€ ì§€í‘œ ì¶”ê°€

#### ë‹¨ê³„ 1: í‰ê°€ ëª¨ë¸ í™•ì¥
```python
# core/personalized_system.py
def evaluate_answer_with_new_metrics(self, qa_pair: QuestionAnswer) -> Dict:
    evaluation = {
        "score": 0,
        "feedback": "",
        "creativity_score": 0,      # ìƒˆë¡œìš´ ì§€í‘œ
        "innovation_level": 0,      # ìƒˆë¡œìš´ ì§€í‘œ
        "originality": 0           # ìƒˆë¡œìš´ ì§€í‘œ
    }
    return evaluation
```

#### ë‹¨ê³„ 2: í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸
```python
def get_evaluation_prompt_with_creativity(self, qa_pair: QuestionAnswer) -> str:
    return f"""
    ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
    1. ê¸°ìˆ ì  ì •í™•ì„± (30%)
    2. ë…¼ë¦¬ì  êµ¬ì¡° (25%)
    3. ì°½ì˜ì„± (25%)           # ìƒˆë¡œìš´ ê¸°ì¤€
    4. ì‹¤ìš©ì„± (20%)
    """
```

### 4. ìƒˆë¡œìš´ AI ëª¨ë¸ í†µí•©

#### ë‹¨ê³„ 1: LLM Provider ì¶”ê°€
```python
# core/llm_manager.py
class LLMProvider(Enum):
    OPENAI_GPT35 = "openai-gpt-3.5-turbo"
    OPENAI_GPT4 = "openai-gpt-4"
    CLAUDE = "claude-3-sonnet"           # ìƒˆë¡œìš´ ëª¨ë¸
    GEMINI = "gemini-pro"               # ìƒˆë¡œìš´ ëª¨ë¸
```

#### ë‹¨ê³„ 2: í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„
```python
class LLMManager:
    def __init__(self):
        self.clients = {
            LLMProvider.OPENAI_GPT35: OpenAIClient(),
            LLMProvider.CLAUDE: ClaudeClient(),      # ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸
            LLMProvider.GEMINI: GeminiClient()       # ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸
        }
    
    def generate_response(self, prompt: str, provider: LLMProvider) -> str:
        client = self.clients[provider]
        return client.generate(prompt)
```

## ğŸ§ª ë””ë²„ê¹… ë° í…ŒìŠ¤íŠ¸

### 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸ êµ¬ì¡°
```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_document_processor.py
â”œâ”€â”€ test_interview_system.py
â”œâ”€â”€ test_ai_candidate_model.py
â”œâ”€â”€ test_llm_manager.py
â””â”€â”€ fixtures/
    â”œâ”€â”€ sample_resume.pdf
    â”œâ”€â”€ sample_cover_letter.docx
    â””â”€â”€ mock_responses.json
```

#### ì˜ˆì‹œ í…ŒìŠ¤íŠ¸
```python
# tests/test_document_processor.py
import unittest
from core.document_processor import DocumentProcessor

class TestDocumentProcessor(unittest.TestCase):
    def setUp(self):
        self.processor = DocumentProcessor()
    
    def test_extract_text_from_pdf(self):
        with open('tests/fixtures/sample_resume.pdf', 'rb') as f:
            content = f.read()
        
        text = self.processor.extract_text_from_file(content, 'pdf')
        self.assertIsInstance(text, str)
        self.assertGreater(len(text), 0)
    
    def test_create_user_profile(self):
        documents = {
            'ìê¸°ì†Œê°œì„œ': 'Python ê°œë°œì í™ê¸¸ë™ì…ë‹ˆë‹¤...',
            'ì´ë ¥ì„œ': 'ê²½ë ¥ 3ë…„, ë°±ì—”ë“œ ê°œë°œ...'
        }
        
        profile = self.processor.create_user_profile(documents)
        self.assertIsNotNone(profile.name)
        self.assertGreater(len(profile.technical_skills), 0)
```

### 2. í†µí•© í…ŒìŠ¤íŠ¸

```python
# tests/test_integration.py
import unittest
import json
from web.app import app

class TestIntegration(unittest.TestCase):
    def setUp(self):
        self.app = app.test_client()
        self.app.testing = True
    
    def test_full_interview_flow(self):
        # 1. ë¬¸ì„œ ì—…ë¡œë“œ
        with open('tests/fixtures/sample_resume.pdf', 'rb') as f:
            response = self.app.post('/upload', data={
                'file': f,
                'document_type': 'ì´ë ¥ì„œ'
            })
        self.assertEqual(response.status_code, 200)
        
        # 2. ë¬¸ì„œ ë¶„ì„
        response = self.app.post('/analyze', json={
            'documents': {'ì´ë ¥ì„œ': 'í…ŒìŠ¤íŠ¸ ë‚´ìš©'}
        })
        data = json.loads(response.data)
        self.assertTrue(data['success'])
        
        # 3. ë©´ì ‘ ì‹œì‘
        response = self.app.post('/start_personalized', json={
            'company': 'naver',
            'position': 'ë°±ì—”ë“œ ê°œë°œì',
            'user_profile': data['profile']
        })
        self.assertEqual(response.status_code, 200)
```

### 3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```python
# tests/test_performance.py
import time
import unittest
from core.llm_manager import LLMManager

class TestPerformance(unittest.TestCase):
    def test_response_time(self):
        manager = LLMManager()
        
        start_time = time.time()
        response = manager.generate_response("í…ŒìŠ¤íŠ¸ ì§ˆë¬¸")
        end_time = time.time()
        
        # ì‘ë‹µ ì‹œê°„ì´ 10ì´ˆ ì´ë‚´ì—¬ì•¼ í•¨
        self.assertLess(end_time - start_time, 10)
    
    def test_concurrent_requests(self):
        import threading
        
        def make_request():
            manager = LLMManager()
            manager.generate_response("í…ŒìŠ¤íŠ¸ ì§ˆë¬¸")
        
        threads = []
        for i in range(5):
            t = threading.Thread(target=make_request)
            threads.append(t)
            t.start()
        
        for t in threads:
            t.join()
```

### 4. ë””ë²„ê¹… ìœ í‹¸ë¦¬í‹°

```python
# core/debug_utils.py
import logging
import json
from functools import wraps

def log_function_call(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        logging.info(f"Calling {func.__name__} with args: {args}, kwargs: {kwargs}")
        result = func(*args, **kwargs)
        logging.info(f"{func.__name__} returned: {result}")
        return result
    return wrapper

def debug_interview_session(session_id: str):
    """ë©´ì ‘ ì„¸ì…˜ ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥"""
    from web.app import app_state
    
    if session_id in app_state.current_sessions:
        session = app_state.current_sessions[session_id]
        print(f"ì„¸ì…˜ ID: {session_id}")
        print(f"ì§ˆë¬¸ ìˆ˜: {len(session.conversation_history)}")
        print(f"í˜„ì¬ ì§„í–‰ë¥ : {session.current_question_count}/{len(session.question_plan)}")
        
        for i, qa in enumerate(session.conversation_history):
            print(f"Q{i+1}: {qa.question_content}")
            print(f"A{i+1}: {qa.answer_content[:100]}...")
```

## ğŸš€ ë°°í¬ ê°€ì´ë“œ

### 1. Docker ë°°í¬

#### Dockerfile
```dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8888

CMD ["python", "run.py"]
```

#### docker-compose.yml
```yaml
version: '3.8'

services:
  ai-interview:
    build: .
    ports:
      - "8888:8888"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - FLASK_ENV=production
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - ai-interview
```

### 2. í´ë¼ìš°ë“œ ë°°í¬

#### AWS EC2 ë°°í¬
```bash
# 1. EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Ubuntu 20.04)
# 2. ë³´ì•ˆ ê·¸ë£¹ ì„¤ì • (í¬íŠ¸ 80, 443, 8888 ì˜¤í”ˆ)

# 3. ì„œë²„ ì„¤ì •
sudo apt update
sudo apt install python3-pip nginx certbot python3-certbot-nginx

# 4. ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬
git clone <repository>
cd final_Q_test
pip3 install -r requirements.txt

# 5. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
echo "OPENAI_API_KEY=your-key" > .env

# 6. ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ ë“±ë¡
sudo tee /etc/systemd/system/ai-interview.service > /dev/null <<EOF
[Unit]
Description=AI Interview System
After=network.target

[Service]
Type=simple
User=ubuntu
WorkingDirectory=/home/ubuntu/final_Q_test
ExecStart=/usr/bin/python3 run.py
Restart=always

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl enable ai-interview
sudo systemctl start ai-interview
```

#### Nginx ì„¤ì •
```nginx
server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://127.0.0.1:8888;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
    
    location /uploads {
        alias /home/ubuntu/final_Q_test/uploads;
    }
    
    client_max_body_size 16M;
}
```

### 3. ëª¨ë‹ˆí„°ë§ ì„¤ì •

#### ë¡œê·¸ ëª¨ë‹ˆí„°ë§
```python
# core/monitoring.py
import logging
from logging.handlers import RotatingFileHandler

def setup_logging():
    formatter = logging.Formatter(
        '%(asctime)s %(levelname)s %(name)s %(message)s'
    )
    
    # ë¡œê·¸ íŒŒì¼ ë¡œí…Œì´ì…˜
    handler = RotatingFileHandler(
        'logs/app.log',
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5
    )
    handler.setFormatter(formatter)
    
    logger = logging.getLogger()
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
```

#### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```python
# core/metrics.py
import time
import psutil
from typing import Dict

class MetricsCollector:
    def __init__(self):
        self.metrics = {
            'requests_count': 0,
            'average_response_time': 0,
            'memory_usage': 0,
            'cpu_usage': 0
        }
    
    def record_request(self, response_time: float):
        self.metrics['requests_count'] += 1
        self.metrics['average_response_time'] = (
            self.metrics['average_response_time'] * (self.metrics['requests_count'] - 1) + 
            response_time
        ) / self.metrics['requests_count']
    
    def get_system_metrics(self) -> Dict:
        return {
            'memory_usage': psutil.virtual_memory().percent,
            'cpu_usage': psutil.cpu_percent(),
            'disk_usage': psutil.disk_usage('/').percent
        }
```

### 4. ë³´ì•ˆ ê°•í™”

#### HTTPS ì„¤ì •
```bash
# Let's Encrypt SSL ì¸ì¦ì„œ ì„¤ì¹˜
sudo certbot --nginx -d your-domain.com
```

#### í™˜ê²½ë³€ìˆ˜ ë³´ì•ˆ
```python
# core/config.py
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    SECRET_KEY = os.environ.get('SECRET_KEY') or 'dev-secret-key'
    OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
    
    # í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë” ê°•ë ¥í•œ ë³´ì•ˆ ì„¤ì •
    if os.environ.get('FLASK_ENV') == 'production':
        SESSION_COOKIE_SECURE = True
        SESSION_COOKIE_HTTPONLY = True
        SESSION_COOKIE_SAMESITE = 'Lax'
```

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### ì°¸ê³  ë¬¸ì„œ
- [Flask ê³µì‹ ë¬¸ì„œ](https://flask.palletsprojects.com/)
- [OpenAI API ë¬¸ì„œ](https://platform.openai.com/docs)
- [Python íƒ€ì… íŒíŠ¸ ê°€ì´ë“œ](https://docs.python.org/3/library/typing.html)

### ìœ ìš©í•œ ë„êµ¬
- **ì½”ë“œ í’ˆì§ˆ**: `black`, `flake8`, `pylint`
- **í…ŒìŠ¤íŠ¸**: `pytest`, `coverage`
- **API í…ŒìŠ¤íŠ¸**: `postman`, `insomnia`
- **ëª¨ë‹ˆí„°ë§**: `prometheus`, `grafana`

### ì»¤ë®¤ë‹ˆí‹°
- **GitHub Issues**: ë²„ê·¸ ë¦¬í¬íŠ¸ ë° ê¸°ëŠ¥ ìš”ì²­
- **Stack Overflow**: ê¸°ìˆ ì  ì§ˆë¬¸
- **Discord/Slack**: ì‹¤ì‹œê°„ ê°œë°œì ì±„íŒ…

---

**ğŸ”§ ê°œë°œ íŒ**: ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œ ì‹œ í•­ìƒ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ë¨¼ì € ì‘ì„±í•˜ê³ (TDD), ì‘ì€ ë‹¨ìœ„ë¡œ ì»¤ë°‹í•˜ë©°, ì½”ë“œ ë¦¬ë·°ë¥¼ í†µí•´ í’ˆì§ˆì„ ìœ ì§€í•˜ì„¸ìš”!