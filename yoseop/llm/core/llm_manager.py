#!/usr/bin/env python3
"""
LLM ëª¨ë¸ ê´€ë¦¬ì
ë‹¤ì–‘í•œ LLM ëª¨ë¸ì„ í†µí•© ê´€ë¦¬í•˜ê³  ëª¨ë¸ë³„ íŠ¹ì„±ì— ë§ê²Œ ìµœì í™”
"""

import openai
import json
import time
import threading
from collections import defaultdict, deque
from typing import Dict, List, Any, Optional, Union
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

class LLMProvider(Enum):
    OPENAI_GPT4 = "openai_gpt4"
    OPENAI_GPT35 = "openai_gpt35"
    OPENAI_GPT4O_MINI = "openai_gpt4o_mini"
    GOOGLE_GEMINI_PRO = "google_gemini_pro"
    GOOGLE_GEMINI_FLASH = "google_gemini_flash"
    KT_BELIEF = "kt_belief"

@dataclass
class LLMConfig:
    """LLM ëª¨ë¸ë³„ ì„¤ì •"""
    provider: LLMProvider
    model_name: str
    max_tokens: int
    temperature: float
    api_key: Optional[str] = None
    additional_params: Dict[str, Any] = None

@dataclass
class LLMResponse:
    """LLM ì‘ë‹µ í‘œì¤€í™”"""
    content: str
    provider: LLMProvider
    model_name: str
    token_count: Optional[int] = None
    response_time: Optional[float] = None
    error: Optional[str] = None

class BaseLLMClient(ABC):
    """LLM í´ë¼ì´ì–¸íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
    
    @abstractmethod
    def generate_response(self, prompt: str, system_prompt: str = "") -> LLMResponse:
        """ì‘ë‹µ ìƒì„±"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        pass

class RateLimiter:
    """API í˜¸ì¶œ ì†ë„ ì œí•œ"""
    def __init__(self, max_requests: int = 10, time_window: float = 60.0):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = deque()
        self.lock = threading.Lock()
    
    def acquire(self):
        """ìš”ì²­ í—ˆìš© ì—¬ë¶€ í™•ì¸ ë° ëŒ€ê¸°"""
        with self.lock:
            now = time.time()
            
            # ì‹œê°„ ìœˆë„ìš° ë°–ì˜ ìš”ì²­ ê¸°ë¡ ì œê±°
            while self.requests and now - self.requests[0] > self.time_window:
                self.requests.popleft()
            
            # ìš”ì²­ í•œë„ ì´ˆê³¼ ì‹œ ëŒ€ê¸°
            if len(self.requests) >= self.max_requests:
                sleep_time = self.time_window - (now - self.requests[0]) + 0.1
                print(f"â³ Rate limit ë„ë‹¬. {sleep_time:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(sleep_time)
                return self.acquire()  # ì¬ê·€ í˜¸ì¶œ
            
            # ìš”ì²­ ê¸°ë¡ ì¶”ê°€
            self.requests.append(now)
            return True

class OpenAIClient(BaseLLMClient):
    """OpenAI í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        if config.api_key:
            self.client = openai.OpenAI(api_key=config.api_key)
        else:
            self.client = openai.OpenAI()
        
        # Rate limiter ì´ˆê¸°í™” (ë¶„ë‹¹ 20íšŒ ì œí•œ)
        self.rate_limiter = RateLimiter(max_requests=20, time_window=60.0)
    
    def generate_response(self, prompt: str, system_prompt: str = "") -> LLMResponse:
        """OpenAI APIë¥¼ í†µí•œ ì‘ë‹µ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        import time
        start_time = time.time()
        
        # Rate limiting ì ìš©
        self.rate_limiter.acquire()
        
        max_retries = 5  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
        base_delay = 2.0  # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        
        for attempt in range(max_retries):
            try:
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                
                # ìš”ì²­ íŒŒë¼ë¯¸í„° ìµœì í™” (ì•ˆì •ì„± í–¥ìƒ)
                request_params = {
                    "model": self.config.model_name,
                    "messages": messages,
                    "max_tokens": min(self.config.max_tokens, 400),  # í† í° ì œí•œ ê°•í™”
                    "temperature": self.config.temperature,
                    "timeout": 60.0  # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                }
                
                # ì¶”ê°€ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ë³‘í•©
                if self.config.additional_params:
                    request_params.update(self.config.additional_params)
                
                response = self.client.chat.completions.create(**request_params)
                
                response_time = time.time() - start_time
                
                return LLMResponse(
                    content=response.choices[0].message.content.strip(),
                    provider=self.config.provider,
                    model_name=self.config.model_name,
                    token_count=response.usage.total_tokens if response.usage else None,
                    response_time=response_time
                )
                
            except Exception as e:
                error_msg = str(e)
                print(f"âš ï¸ API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {error_msg}")
                
                # ì˜¤ë²„ë¡œë“œ ì—ëŸ¬, rate limit, ë˜ëŠ” ì„œë²„ ì—ëŸ¬ì¸ ê²½ìš° ì¬ì‹œë„
                if any(keyword in error_msg.lower() for keyword in ["overloaded", "rate limit", "429", "503", "502", "500", "timeout"]):
                    if attempt < max_retries - 1:
                        # ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„° ì ìš©
                        wait_time = base_delay * (2 ** attempt) + (attempt * 0.5)  # ì§€í„° ì¶”ê°€
                        print(f"ğŸ”„ {wait_time:.1f}ì´ˆ í›„ ì¬ì‹œë„... (ë°±ì˜¤í”„ ì ìš©)")
                        time.sleep(wait_time)
                        continue
                
                # ìµœì¢… ì‹¤íŒ¨ ë˜ëŠ” ì¬ì‹œë„í•˜ì§€ ì•ŠëŠ” ì—ëŸ¬
                return LLMResponse(
                    content="",
                    provider=self.config.provider,
                    model_name=self.config.model_name,
                    error=f"API í˜¸ì¶œ ì‹¤íŒ¨ ({attempt + 1}íšŒ ì‹œë„): {error_msg}"
                )
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        return LLMResponse(
            content="",
            provider=self.config.provider,
            model_name=self.config.model_name,
            error=f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {max_retries}íšŒ"
        )
    
    def is_available(self) -> bool:
        """OpenAI API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )
            return True
        except:
            return False

class GoogleGeminiClient(BaseLLMClient):
    """Google Gemini í´ë¼ì´ì–¸íŠ¸ (í–¥í›„ êµ¬í˜„)"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        # Google AI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í–¥í›„ êµ¬í˜„)
        print(f"Google Gemini {config.model_name} í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì¤‘...")
    
    def generate_response(self, prompt: str, system_prompt: str = "") -> LLMResponse:
        """Gemini APIë¥¼ í†µí•œ ì‘ë‹µ ìƒì„± (í–¥í›„ êµ¬í˜„)"""
        return LLMResponse(
            content=f"[Gemini {self.config.model_name} ì‘ë‹µ ì¤€ë¹„ ì¤‘...]",
            provider=self.config.provider,
            model_name=self.config.model_name,
            error="API ë¯¸êµ¬í˜„"
        )
    
    def is_available(self) -> bool:
        return False

class KTBeliefClient(BaseLLMClient):
    """KT ë¯¿ìŒ ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ (í–¥í›„ êµ¬í˜„)"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        print(f"KT ë¯¿ìŒ ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì¤‘...")
    
    def generate_response(self, prompt: str, system_prompt: str = "") -> LLMResponse:
        """KT ë¯¿ìŒ APIë¥¼ í†µí•œ ì‘ë‹µ ìƒì„± (í–¥í›„ êµ¬í˜„)"""
        return LLMResponse(
            content="[KT ë¯¿ìŒ ëª¨ë¸ ì‘ë‹µ ì¤€ë¹„ ì¤‘...]",
            provider=self.config.provider,
            model_name=self.config.model_name,
            error="API ë¯¸êµ¬í˜„"
        )
    
    def is_available(self) -> bool:
        return False

class LLMManager:
    """LLM ëª¨ë¸ í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self):
        self.clients: Dict[LLMProvider, BaseLLMClient] = {}
        self.default_configs = self._get_default_configs()
        
    def _get_default_configs(self) -> Dict[LLMProvider, LLMConfig]:
        """ê¸°ë³¸ ëª¨ë¸ ì„¤ì •"""
        return {
            LLMProvider.OPENAI_GPT4: LLMConfig(
                provider=LLMProvider.OPENAI_GPT4,
                model_name="gpt-4",
                max_tokens=400,  # í† í° ì œí•œ ê°•í™”
                temperature=0.6  # ì•ˆì •ì„± í–¥ìƒ
            ),
            LLMProvider.OPENAI_GPT35: LLMConfig(
                provider=LLMProvider.OPENAI_GPT35,
                model_name="gpt-3.5-turbo",
                max_tokens=350,  # ë” ì•ˆì •ì ì¸ ëª¨ë¸ ìš°ì„  ì‚¬ìš©
                temperature=0.6
            ),
            LLMProvider.OPENAI_GPT4O_MINI: LLMConfig(
                provider=LLMProvider.OPENAI_GPT4O_MINI,
                model_name="gpt-4o-mini",
                max_tokens=350,  # í† í° ì œí•œ ê°•í™”
                temperature=0.6
            ),
            LLMProvider.GOOGLE_GEMINI_PRO: LLMConfig(
                provider=LLMProvider.GOOGLE_GEMINI_PRO,
                model_name="gemini-pro",
                max_tokens=800,
                temperature=0.7
            ),
            LLMProvider.GOOGLE_GEMINI_FLASH: LLMConfig(
                provider=LLMProvider.GOOGLE_GEMINI_FLASH,
                model_name="gemini-flash",
                max_tokens=600,
                temperature=0.7
            ),
            LLMProvider.KT_BELIEF: LLMConfig(
                provider=LLMProvider.KT_BELIEF,
                model_name="kt-belief-1.0",
                max_tokens=600,
                temperature=0.7
            )
        }
    
    def register_model(self, provider: LLMProvider, config: LLMConfig = None, api_key: str = None):
        """ëª¨ë¸ ë“±ë¡"""
        if config is None:
            config = self.default_configs[provider]
        
        if api_key:
            config.api_key = api_key
        
        # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        if provider in [LLMProvider.OPENAI_GPT4, LLMProvider.OPENAI_GPT35, LLMProvider.OPENAI_GPT4O_MINI]:
            self.clients[provider] = OpenAIClient(config)
        elif provider in [LLMProvider.GOOGLE_GEMINI_PRO, LLMProvider.GOOGLE_GEMINI_FLASH]:
            self.clients[provider] = GoogleGeminiClient(config)
        elif provider == LLMProvider.KT_BELIEF:
            self.clients[provider] = KTBeliefClient(config)
        
        print(f"âœ… {provider.value} ëª¨ë¸ ë“±ë¡ ì™„ë£Œ")
    
    def generate_response(self, provider: LLMProvider, prompt: str, system_prompt: str = "") -> LLMResponse:
        """ì§€ì •ëœ ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
        if provider not in self.clients:
            return LLMResponse(
                content="",
                provider=provider,
                model_name="",
                error=f"ëª¨ë¸ {provider.value}ì´ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )
        
        return self.clients[provider].generate_response(prompt, system_prompt)
    
    def generate_multi_responses(self, providers: List[LLMProvider], prompt: str, system_prompt: str = "") -> Dict[LLMProvider, LLMResponse]:
        """ì—¬ëŸ¬ ëª¨ë¸ë¡œ ë™ì‹œ ì‘ë‹µ ìƒì„±"""
        responses = {}
        for provider in providers:
            responses[provider] = self.generate_response(provider, prompt, system_prompt)
        return responses
    
    def get_available_models(self) -> List[LLMProvider]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡"""
        available = []
        for provider, client in self.clients.items():
            if client.is_available():
                available.append(provider)
        return available
    
    def get_model_info(self, provider: LLMProvider) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        if provider not in self.clients:
            return {"error": "ëª¨ë¸ì´ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        config = self.clients[provider].config
        return {
            "provider": provider.value,
            "model_name": config.model_name,
            "max_tokens": config.max_tokens,
            "temperature": config.temperature,
            "is_available": self.clients[provider].is_available()
        }
    
    def update_model_config(self, provider: LLMProvider, **kwargs):
        """ëª¨ë¸ ì„¤ì • ì—…ë°ì´íŠ¸"""
        if provider not in self.clients:
            print(f"âš ï¸ ëª¨ë¸ {provider.value}ì´ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        config = self.clients[provider].config
        for key, value in kwargs.items():
            if hasattr(config, key):
                setattr(config, key, value)
                print(f"âœ… {provider.value} {key} ì„¤ì •ì„ {value}ë¡œ ë³€ê²½")

if __name__ == "__main__":
    # LLM Manager í…ŒìŠ¤íŠ¸
    print("ğŸ”§ LLM Manager í…ŒìŠ¤íŠ¸")
    
    manager = LLMManager()
    
    # OpenAI ëª¨ë¸ ë“±ë¡ í…ŒìŠ¤íŠ¸
    api_key = input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (í…ŒìŠ¤íŠ¸ìš©): ")
    if api_key:
        manager.register_model(LLMProvider.OPENAI_GPT4O_MINI, api_key=api_key)
        
        # ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„± í…ŒìŠ¤íŠ¸
        response = manager.generate_response(
            LLMProvider.OPENAI_GPT4O_MINI,
            "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨í•œ ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.",
            "ë‹¹ì‹ ì€ ì‹ ì… ê°œë°œì ì§€ì›ìì…ë‹ˆë‹¤."
        )
        
        print(f"\nğŸ“ ì‘ë‹µ í…ŒìŠ¤íŠ¸:")
        print(f"ëª¨ë¸: {response.model_name}")
        print(f"ì‘ë‹µ: {response.content}")
        if response.error:
            print(f"ì˜¤ë¥˜: {response.error}")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
    available_models = manager.get_available_models()
    print(f"\nâœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {[m.value for m in available_models]}")